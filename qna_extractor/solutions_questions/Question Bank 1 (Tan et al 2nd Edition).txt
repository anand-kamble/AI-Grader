
1




Introduction
1. [Fall 2008]
For each data set given below, give specific examples of classification, clustering, association rule mining, and anomaly detection tasks that can be performed on the data. For each task, state how the data matrix should be constructed (i.e., specify the rows and columns of the matrix).
(a) Ambulatory Medical Care data1, which contains the demographic and medical visit information for each patient (e.g., gender, age, duration of visit, physician’s diagnosis, symptoms, medication, etc).
Answer:
Classification
Task: Diagnose whether a patient has a disease.
Row: Patient
Column: Patient’s demographic and hospital visit information (e.g., symptoms), along with a class attribute that indicates whether the patient has the disease.
Clustering
Task: Find groups of patients with similar medical conditions
Row: A patient visit
Column: List of medical conditions of each patient
Association rule mining
Task: Identify the symptoms and medical conditions that co-occur together frequently
Row: A patient visit
Column: List of symptoms and diagnosed medical conditions of the patient
Anomaly detection
Task: Identify healthy looking patients with rare medical disorders
Row: A patient visit
Column: List of demographic attributes, symptoms, and medical test results of the patient

     1See for example, the National Hospital Ambulatory Medical Care Survey http://www. cdc.gov/nchs/about/major/ahcd/ahcd1.htm

2 Chapter 1	Introduction

(b) Stock market data, which include the prices and volumes of various stocks on different trading days.
Answer:
Classification
Task: Predict whether the stock price will go up or down the next trading day
Row: A trading day
Column: Trading volume and closing price of the stock the previous 5 days and a class
attribute that indicates whether the stock went up or down
Clustering
Task: Identify groups of stocks with similar price fluctuations
Row: A company’s stock
Column: Changes in the daily closing price of the stock over the past ten years
Association rule mining
Task: Identify stocks with similar fluctuation patterns(e.g., {Google-Up, Yahoo-Up}) Row: A trading day
Column: List of all stock-up and stock-down events on the given day.
Anomaly detection
Task: Identify unusual trading days for a given stock (e.g., unusually high volume)
Row: A trading day
Column: Trading volume, change in daily stock price (daily high ? low prices), and average price change of its competitor stocks
(c) Database of Major League Baseball (MLB).

Classification
Task: Predict the winner of a game between two MLB teams.
Row: A game.
Column: Statistics of the home and visiting teams over their past 10 games they had played
(e.g., average winning percentage and hitting percentage of their players)
Clustering
Task: Identify groups of players with similar statistics
Row: A player
Column: Statistics of the player
Association rule mining
Task: Identify interesting player statistics (e.g., 40% of right-handed players have a batting
percentage below 20% when facing left-handed pitchers) Row: A player
Column: Discretized statistics of the player
Anomaly detection
Task: Identify players who performed considerably better than expected in a given season
Row: A (player,season) pair e.g, (player1 in 2007)
Column: Ratio statistics of a player (e.g., ratio of average batting percentage in 2007 to
career average batting percentage)


2


2




Data
2.1 Types of Attributes
1. Classify the following attributes as binary, discrete, or continuous. Also classify them as qualitative (nominal or ordinal) or quantitative (interval or ratio). Some cases may have more than one interpretation, so briefly indicate your reasoning if you think there may be some ambiguity.

(a) Number of courses registered by a student in a given semester.
Answer: Discrete, quantitative, ratio.
(b) Speed of a car (in miles per hour).
Answer: Discrete, quantitative, ratio.
(c) Decibel as a measure of sound intensity.
Answer: Continuous, quantitative, interval or ratio. It is actually a logratio type (which is somewhere between interval and ratio).
(d) Hurricane intensity according to the Saffir-Simpson Hurricane Scale.
Answer: Discrete, qualitative, ordinal.
(e) Social security number.
Answer: Discrete, qualitative, nominal.

2. Classify the following attributes as:
• discrete or continuous.
• qualitative or quantitative
• nominal, ordinal, interval, or ratio


Some cases may have more than one interpretation, so briefly indicate your reasoning if you think there may be some ambiguity.

(a) Julian Date, which is the number of days elapsed since 12 noon Greenwich Mean Time of January 1, 4713 BC.
Answer: Continuous, quantitative, interval
(b) Movie ratings provided by users (1-star, 2-star, 3-star, or 4-star).
Answer: Discrete, qualitative, ordinal
(c) Mood level of a blogger (cheerful, calm, relaxed, bored, sad, angry or frustrated).
Answer: Discrete, qualitative, nominal
(d) Average number of hours a user spent on the Internet in a week.
Answer: Continuous, quantitative, ratio
(e) IP address of a machine.
Answer: Discrete, qualitative, nominal
(f) Richter scale (in terms of energy release during an earthquake).
Answer: Continuous, qualitative, ordinal
In terms of energy release, the difference between 0.0 and 1.0 is not the same as between 1.0 and 2.0. Ordinal attributes are qualitative; yet, can be continuous.
(g) Salary above the median salary of all employees in an organization.
Answer: Continuous, quantitative, interval
(h) Undergraduate level (freshman, sophomore, junior, and senior) for measuring years in college.
Answer: Discrete, qualitative, ordinal

3. For each attribute given, classify its type as:
• discrete or continuous AND
• qualitative or quantitative AND
• nominal, ordinal, interval, or ratio
Indicate your reasoning if you think there may be some ambiguity in some cases.
Example: Age in years.
Answer: Discrete, quantitative, ratio.


(a) Daily user traffic volume at YouTube.com (i.e., number of daily visitors who visited the Web site).
Answer: Discrete, quantitative, ratio.
(b) Air pressure of a car/bicycle tire (in psi).
Answer: Continuous, quantitative, ratio.
(c) Homeland Security Advisory System ratings - code red/orange/etc.
Answer: Discrete, qualitative, ordinal.
(d) Amount of seismic energy release, measured in Richter scale.
Answer: Continuous, qualitative, ordinal.
(e) Credit card number.
Answer: Discrete, qualitative, nominal.
(f) The wealth of a nation measured in terms of gross domestic product (GDP) per capita above the world’s average of $10,500.
Answer: Continuous, quantitative, interval.

4. For each attribute given, classify its type as:
• discrete or continuous AND
• qualitative or quantitative AND
• nominal, ordinal, interval, or ratio
Indicate your reasoning if you think there may be some ambiguity in some cases.
Example: Age in years.
Answer: Discrete, quantitative, ratio.

(a) Favorite movie of each person.
Answer: Discrete, qualitative, nominal
(b) Number of days since Jan 1, 2011.
Answer: Discrete, quantitative, interval.
(c) Category of a hurricane (The Saffir-Simpson Hurricane Wind Scale ranges from category 1 to category 5).
Answer: Discrete, qualitative, ordinal.
(d) Number of students enrolled in a class.
Answer: Discrete, quantitative, ratio


5. For each attribute given, classify its type as:
• discrete or continuous AND
• qualitative or quantitative AND
• nominal, ordinal, interval, or ratio
Indicate your reasoning if you think there may be some ambiguity in some cases.
Example: Temperature in Kelvin
Answer: Continuous, quantitative, ratio.

(a) Number of years since 1 BC. For example, 2 BC is year -1, 1 BC is year 0, 1 AD is year 1, and 2013 AD is year 2013 (note, there is no 0 AD in Gregorian calendar).
Answer: Discrete/Continuous, quantitative, interval.
(b) GPA of a student.
Answer: Continuous, qualitative, ordinal.
(c) Mood level of a blogger (cheerful, calm, relaxed, bored, sad, angry or frustrated).
Answer: Discrete, qualitative, nominal.
(d) Sound intensity in decibel scale.
Answer: Continuous, qualitative, ordinal. In terms of sound in- tensity, the difference between 0dB and 1dB is not the same as the difference between 10 dB and 11 dB (decibels are in log scale); thus, it is not an interval attribute.

6. State the type of each attribute given below before and after we have performed the following transformation.
(a) Hair color of a person is mapped to the following values: black = 0, brown = 1, red = 2, blonde = 3, grey = 4, white = 5.
Answer: Nominal (both before and after transformation).
(b) Grade of a student (from 0 to 100) is mapped to the following scale: A = 4.0, A- = 3.5, B = 3.0, B- = 2.5, C = 2.0, C- = 1.5, D = 1.0, D- = 0.5, E = 0.0
Answer: Ratio (before transformation) to ordinal (after transfor- mation).


(c) Age of a person is discretized to the following scale: Age < 12, 12
≤ Age < 21, 21 ≤ Age < 45, 45 ≤ Age < 65, Age > 65.
Answer: Ratio (before transformation) to ordinal (after transfor-
mation)
(d) Annual income of a person is discretized to the following scale: In- come < $20K, $20K ≤ Income < $60K, $60K ≤ Income < $120K,
$120K ≤ Age < $250K, Age ≥ $250K.
Answer: Ratio (before transformation) to ordinal (after transfor- mation).
(e) Height of a person is changed from meters to feet.
Answer: Ratio (both before and after transformation)
(f) Height of a person is changed from meters to {Short, Medium, Tall}. Answer: Ratio (before transformation) to ordinal (after transfor- mation).
(g) Height of a person is changed from feet to number of inches above 4 feet.
Answer: Ratio (before transformation) to interval (after transfor- mation).
(h) Weight of a person is standardized by subtracting it with the mean of the weight for all people and dividing by its standard deviation. Answer: Ratio (before transformation) to interval (after transfor- mation)
7. State whether it is meaningful (based on the properties of the attribute values) to apply the following operations to the data given below

(a) Average amplitude of seismic waves (in Richter scale) for the 10 deadliest earthquakes in Asia.
Answer: No because Richter scale is ordinal.
(b) Average number of characters in a collection of spam messages.
Answer: Yes because number of characters is a ratio attribute.
(c) Pearson’s correlation between shirt size and height of an individual.
Answer: No because shirt size is ordinal.
(d) Median zipcode of households in the United States.
Answer: No because zipcode is nominal.


(e) Entropy of students (based on the GPA they obtained for a given course).
   Answer: Yes because entropy is applicable to nominal attributes. (f) Geometric mean of temperature (in Fahrenheit) for a given city.
Answer: No because temperature (in Fahrenheit) is not a ratio
attribute.

2.2 Data Preprocessing
1. Consider the following dataset that contains the age and gender infor- mation for 9 users who visited a given website.

UserID
1
2
3
4
5
6
7
8
9
Age
Gender
17
Female
24
Male
25
Male
28
Male
32
Female
38
Female
39
Female
49
Male
68
Male

(a) Suppose you apply equal interval width approach to discretize the Age attribute into 3 bins. Show the userIDs assigned to each of the 3 bins.
Answer: Bin width = 68?17 = 51 = 17.
3	3
Bin 1: 1, 2, 3, 4, 5
Bin 2: 6, 7, 8
Bin 3: 9
(b) Repeat the previous question using the equal frequency approach. Answer: Since there are 9 users and 3 bins, every bin must contain 3 users.
Bin 1: 1, 2, 3
Bin 2: 4, 5, 6
Bin 3: 7, 8, 9
(c) Repeat question (a) using a supervised discretization approach (with Gender as class attribute). Specifically, choose the bins in such a way that their members are as “pure” as possible (i.e., belonging to the same class).
Answer:
Bin 1: 1, 2, 3, 4
Bin 2: 5, 6, 7
Bin 3: 8, 9


2. Consider an attribute X of a data set that takes the values {x1, x2, · · · , x9}
(sorted in increasing order of magnitude). We apply two methods (equal
interval width and equal frequency) to discretize the attribute into 3 bins. The bins obtained are shown below:

Equal Width: {x1, x2, x3}, {x4, x5, x6, x7, x8}, {x9}
Equal Frequency: {x1, x2, x3}, {x4, x5, x6}, {x7, x8, x9}
Explain what will be the effect of applying the following transformations on each discretization method, i.e., whether the elements assigned to each bin can change if you discretize the attribute after applying the

transformation function below. Note that X¯

denotes the average value

and ?x denotes standard deviation of attribute X.
(a) X ? X ? X¯ (i.e., if the attribute values are centered).
Answer: No change for equal width because the distance between
xi and xi+1 is unchanged. No change for equal frequency because the relative ordering of data points remain the same (i.e., if xi < xi+1 then xi ? X¯ < xi+1 ? X¯ ).
(b) X ? X?X¯ (i.e., if the attribute values are standardized).
Answer: Since the distances between every pair of points (xi, xi+1)
change uniformly (by a constant factor of ?x, the elements in the bins are unchanged for equal width discretization. No change for equal frequency because the relative ordering of data points remain the same.
(c) X ? exp X?X¯	(i.e., if the values are standardized and exponen-
tiated).
Answer: The bin elements may change for equal width because the distances between xi and xi+1 may not change uniformly. No change for equal frequency because the relative ordering of data points remain the same.
3. Consider a dataset that has 3 attributes (x1, x2, and x3). The distribu- tion of each attribute is as follows and shown in Figure
• x1 has a uniform distribution in the range between 0 and 1.
• x2 is generated from a mixture of 3 Gaussian distributions centered at 0.1, 0.5, and 0.9, respectively. The standard deviation of the


distributions are 0.02, 0.1, and 0.02, respectively. Assume each point is generated from one of the 3 distributions and the number of points associated with each distribution is different.
• x3 is generated from an exponential distribution with mean 0.1.
(a) Which attribute(s) is likely to produce the same bins regardless of whether you use equal width or equal frequency approaches (as- suming the number of bins is not too large).
Answer: x1.
(b) Which attribute(s) is more suitable for equal frequency than equal width discretization approaches.
Answer: x3.
(c) Which attribute(s) is not appropriate for both equal width and equal frequency discretization approaches.
Answer: x2.
(d) If all 3 are initially ratio attributes, what are their attribute types after discretization?
Answer: Ordinal.

4. An e-commerce company is interested in identifying the highest spending customers at its online store using association rule mining. One of the rules identified is:

 21 ≤ Age < 45 AND NumberOfVisits > 50 ? AmountSpent > $500,
where the Age attribute was discretized into 5 bins, NumberOfVisits was discretized into 8 bins, and AmountSpent was discretized into 8 bins. The confidence of an association rule A, B ? C is defined as
P (A, B, C)

Confidence(A, B ? C) = P (C|A, B) =

(2.1)
P (A, B)


where P (C|A, B) is the conditional probability of C given A and B, P (A, B, C) is the joint probability of A, B, and C, and P (A, B) is the
joint probability of A and B. The probabilities are empirically esti- mated based on their relative frequencies in the data. For example, P (AmountSpent > $500) is given by the proportion of online users who visited the store and spent more than $500.


(a) Suppose we increase the number of bins for the Age attribute from 5 to 6 so that the discretized Age in the rule becomes 21 ≤ Age
< 30 instead of 21 ≤ Age < 45, will the confidence of the rule be
non-increasing, non-decreasing, stays the same, or could go either
way (increase/decrease)?
Answer: Can increase/decrease.
(b) Suppose we increase the number of bins for the AmountSpent at- tribute from 8 to 10, so that the right hand side of the rule becomes
$500 < AmountSpent < $1000, will the confidence of the rule be non-increasing, non-decreasing, stays the same, or could go either way (increase/decrease)?
Answer: Non-increasing.
(c) Suppose the values for NumberOfVisits attribute are distributed according to a Poisson distribution with a mean value equals to 4. If we discretize the attribute into 4 bins using the equal frequency approach, what are the bin values after discretization? Hint: you need to refer to the cumulative distribution table for Poisson dis- tribution to answer the question.
Answer: Choose the bin values such that the cumulative distribu- tion is close to 0.25, 0.5, and 0.75. This corresponds to bin values: 0 to 2, 3, 4 to 5, and greater than 5.

5. Null values in data records may refer to missing or inapplicable values. Consider the following table of employees for a hypothetical organization:

Name
Sales commission
Occupation
John
5000
Sales
Mary
1000
Sales
Bob
null
Non-sales
Lisa
null
Non-sales

The null values in the table refer to inapplicable values since sales com- mission are calculated for sales employees only. Suppose we are interested to calculate the similarity between users based on their sales commission.

(a) Explain what is the limitation of the approach to compute similarity if we replace the null values in sales commission by 0.
Answer: Mary will be more similar to Bob and Lisa than to John.


(b) Explain what is the limitation of the approach to compute similarity if we replace the null values in sales commission by the average value of sales commission (i.e., 3000).
Answer: Both Mary and John are less similar to each other than to Bob and Lisa.
(c) Propose a method that can handle null values in the sales commis- sion so that employees that have the same occupation are closer to each other than to employees that have different occupations. Answer: One way is to change the similarity function as follows:



Similarity(a, b) =

0,	if one of a or b is null;
, s(a, b),  otherwise.


where s(a, b) is the original similarity measure used for the sales commission.

6. Consider a data set from an online social media Web site that contains information about the age and number of friends for 5,000 users.

(a) Suppose the number of friends for each user is known. However, only 4000 out of 5000 users provide their age information. The average age of the 4,000 users is 30 years old. If you replace the missing values for age with the value 30, will the average age com- puted for the 5,000 users increases, decreases, or stays the same (as 30)?
Answer: Average age does not change.



xold

4000
=	1 	x
4000	i
i=1


xnew	=

1


5000

5000

i=1

1
xi = 5000

4000

i=1


xi +

5000
xi
i=4001

Since xi = xold for i = 4001, 4002, · · · , 5000 and ?4000 xi = 4000xold,

we have



xnew



=	 1  4000x 5000




old



+ 1000x




old

  = x




old


(b) Suppose the covariance between age and number of friends calcu- lated using the 4,000 users (with no missing values) is 20. If you replace the missing values for age with the average age of the 4,000 users, would the covariance between age and number of friends in- creases, decreases, or stays the same (as 20)? Assume that the average number of followers for all 5,000 users is the same as the average for 4,000 users.
Answer: Covariance will decrease. Let C1 =  4000(xi ? x)(yi ?
y)/3999 be the covariance computed using the 4,000 users without
missing values. If we impute the missing values for age with average age, x remains unchanged according to part (a). Furthermore, y is assumed to be unchanged. Thus, the new covariance is


5000
C	=	1 	(x
2	4999	i
i=1
— 
x)(yi
— 
y)

1
=
4999

4000
(xi ? x)(yi ? y) +

5000
(xi ? x)(yi ? y)


1
=
4999

i=1
4000
(xi ? x)(yi ? y) +
i=1
4000

i=4001
5000
(x ? x)(yi ? y)
i=4001

=	 1 	(x
4999	i
i=1
— 
x)(yi

— y) < C1

(2.2)


7. Consider the following data matrix on the right, in which two of its values are missing (the matrix on the left shows its true values).

?0.2326	0.2270
?0.0847	0.7125

?0.2326	?
?

0.1329	0.1461	?
?	?	?

0.3724	0.1756
?	?

0.3724
?	?

0.6926	0.7834	??	0.6926
?

0.7933	0.7375
0.8229	0.2147
0.8497	0.4980
? 1.0592	0.7600 ?

0.7933
0.8229
0.8497
? 1.0592	?


(a) Impute the missing values for the matrix on the right by their re- spective column averages. Show the imputed values and calculate their root-mean-square-error (RMSE).
s(A4,1 ? A˜ 4,1)2 + (A11,2 ? A˜ 11,2)2

where Ai,j denotes the true value of the (i, j)-th element of the data matrix and A˜ i,j denotes its corresponding imputed value.
Answer: The column averages are [0.5819 0.4962]. The imputed values are
?0.2326
?0.0847
0.5819
0.3724

0.6926
0.7933
0.8229
? 0.8497	?
and the RMSE value is



RMSE =

(0.1329 ? 0.5819)2 + (0.7600 ? 0.4962)2
2

= 0.3683


(b) The Expectation-Maximization (E-M) algorithm is a well-known approach for imputing missing values. Assuming the data is gen- erated from a multivariate Gaussian distribution, E-M iteratively computes the following conditional mean for each attribute and uses it to impute the missing values:

µi|j = µˆi + ?ij??1(xj ? µˆj)
where the indices i, j ? {1, 2} refer to one of the two attributes of the data and ?	denote inverse of the covariance matrix. Repeat
the previous question by applying the E-M algorithm iteratively for


5 times. Assume the covariance matrix of the data is known and given by

? =  0.25	0.1
0.1	0.15
In the first iteration, compute the mean value for each column using only the non-missing values. In subsequent iterations, compute the mean value for each column using both the non-missing and imputed values. Show the imputed values after each iteration and compute the root-mean-square-error. Compare the error against the answer in part (a).
Answer:
The inverse of the covariance matrix is

??1 =	5.4545	?3.6364
?3.6364	9.0909
The results after each iteration are shown below:

Iteration
µˆ1
µˆ2
Imputed x4,1
Imputed x11,2
RMSE
1
0.5819
0.4962
0.2315
0.9301
0.1390
2
0.5527
0.5324
0.1826
0.9928
0.1683
3
0.5486
0.5376
0.1756
1.0018
0.1736
4
0.5480
0.5384
0.1746
1.0030
0.1743
5
0.5479
0.5385
0.1745
1.0032
0.1745
The root-mean-square-error for EM algorithm is considerably lower than that using mean imputation.

8. The purpose of this exercise is to illustrate the relationship between PCA and SVD. Let A be an N ?d rectangular data matrix and C be its d?d covariance matrix.

(a) Suppose IN is an N ?N identity matrix and 1N is an N ?N matrix whose elements are equal to 1, i.e., ?i, j : (1)ij = 1. Show that the covariance matrix C can be expressed into the following form:


C =   1	 AT I
N ? 1
.

 1
— N 1N A


Answer: The covariance between columns i and j in matrix A is given by

Cij


	
=	k (Aki ? Ai)(Akj ? Aj) ,	(2.3)
N ? 1

where Ai and Aj are their corresponding column averages. A matrix
of column averages for A can be computed as follows:



1	1
1N A  =

1	1	· · ·	1
1	1	· · ·	1

?. ,.

A11	A12	· · ·	A1d ? A21	A22	· · ·	A2d .

N	N	· · · · · · · · · · · ·
1	1	· · ·	1

· · ·	· · ·	· · ·	· · ·
AN 1	AN 2	· · · ANd

, 1 ?j Aj1	 1 ?j Aj2	· · ·	 1 ?j Ajd ?

N
1
=  .? N

N
j Aj1	1

N
j Aj2	· · ·	1

j Ajd

.?	(2.4)

N ?j Aj1



N ?j Aj2	· · ·

N ?j Ajd

Thus, each term (Aki ? Ai) in Equation (2.3) can be expressed in matrix notation as Aki?	j Aji = [A?	1N A]ki. The covariance



  1		 1
C  =	(A ?	1

A)T (A ?  1 1 A)

N	1	N N
  1		 1

N  N
 T	 1

=
N ? 1

(IN ? N 1N )A

(IN ? N 1N )A

=	 1	 AT  I
N ? 1

 1
— N 1N

  IN

 1
— N 1N

 A	(2.5)


where we have use the following property of matrix transpose (XY)T =
YT XT on the last line. Furthermore, since the identity matrix and
the matrix of all ones are symmetric, i.e., IT = IN and 1T = 1N ,
therefore (IN ?  1 1N )T = (IN ?  1 1N ). Finally, it can be shown
that the matrix (IN ?  1 1N ) is idempotent, which means it is the


same as the square of the matrix:

1	1	2	1
(IN ? N 1N )(IN ? N 1N )  =  IN ? N 1N + N 2 1N 1N
2	1
=  IN ? N 1N + N 2 N 1N
2	1
=  IN ? N 1N + N 1N
1
=  IN ? N 1N ,	(2.6)
where 1N 1N = N 1N is an N ? N matrix whose elements are equal to N . Substituting (2.6) into (2.5), we obtain:

C =   1	 AT  I
N ? 1


 1
— N 1N

 A	(2.7)

(b) Using singular value decomposition, the matrix A can be factor- ized as follows: A = U?VT , where U is the N ? N left singular matrix, ? is the N ? d matrix containing the singular values, and
V is the d ? d right singular matrix. Similarly, using eigenvalue
decomposition, the covariance matrix can be factorized as follows:
C = X?XT . Show the relationship between SVD and PCA is given by the following equation:

V?2VT ?  1 AT 1

A = (N ? 1)X?XT .


Answer: From the previous question, we can write:
C =   1	 AT  I	?  1 1  A =   1	 AT A ?  1 AT 1 A 

N ? 1

N	N N

N ? 1

N	N
(2.8)

Since A = U?VT and U is an orthogonal matrix,

AT A = [U?VT ]T [U?VT ] = V?T UT U?VT = V?T ?VT .

If N > d, then ? has N d rows of all zeros. If we remove such rows, ? becomes a d d square matrix and ?T ? = ?2. By substituting C = X?XT and AT A = V?2V into Equation (2.8), we have:
X?XT =   1	 V?2VT ?  1 A1 A .
N ? 1	N


(c) Find the relationship between the right singular matrix V and the matrix of principal components X if the data matrix A has been column-centered (i.e., every column of A has been subtracted by the column mean) before applying SVD.
Answer: If the matrix A has been column-centered, then its col- umn mean is zero, which means AT 1N is a matrix of all zeros. Thus, the last equation in the previous question reduces to:


X?XT =	1
N ? 1

V?2VT .

This suggests that the right singular matrix V corresponds to the principal components X, while the square root of the singular values are the same as N ? 1 times the eigenvalues.
9. Principal component analysis (PCA) can be used for image compression by transforming a high-resolution image into its lower rank approxima-
tion. In this exercise, you will be provided with the following three images of size 1080 ? 1920 pixels each (the filenames are img1.jpg, img2.jpg, and img3.jpg).

(a) img1	(b) img2	(c) img3

Figure 2.1. Image data set.

You will use Matlab to apply PCA to each of the following images.

(a) Load each image using the imread command. For example:
matlab> A = imread(’img1.jpg’);
(b) Plot the image in gray scale.
matlab> imagesc(A); matlab> colormap(gray);


Answer: See Figure 2.1.
(c) Apply principal component analysis to obtain a reduced rank ap- proximation of the image.
For example, to obtain a rank-10 approximation (i.e., using the first 10 principal components), use the following commands:
matlab> A = double(A);	% convert A from uint8 to double format matlab> [U,V] = princomp(A);	% apply principal component analysis matlab> rank = 10;	% set rank to be 10
matlab> B = V(:,1:rank)*U(:,1:rank)’;	% B is the compressed image of A matlab> figure;
matlab> imagesc(B); matlab> colormap(gray);
For each image, vary the rank (i.e., number of principal compo- nents) as follows: 10, 30, 50, and 100. Save each image as follows:
matlab>  saveas(gcf,  ’filename.jpg’,  ’jpeg’);
Insert the compressed (reduced rank) images to the solution file of your homework (don’t submit the jpg files individually).
Answer: See Figure 2.2.
(d) Compare the size of matrix A (in bytes) to the total sizes of matrices U and V (in bytes). Compute the compression ratio:



Compression ratio =

Size of matrix A


Size of matrix U + Size of matrix V


for each reduced rank (10, 30, 50, 100) of the images. You can use the whos command to determine the size of the matrices:
matlab> whos A U V
Answer: See Table 2.1.

rank
size of A
size of U
size of V
compression rate
10
16588800
153600
86400
69.12
30
16588800
460800
259200
23.04
50
16588800
768000
432000
13.824
100
16588800
1536000
864000
6.912
Table 2.1. Compression ratio for various images



		

(a) rank 10 img1	(b) rank 10 img2	(c) rank 10 img3


(d) rank 30 img1	(e) rank 30 img2	(f) rank 30 img3


(g) rank 50 img1	(h) rank 50 img2	(i) rank 50 img3


(j) rank 100 img1	(k) rank 100 img2	(l) rank 100 img3

Figure 2.2. Reduced-rank images using PCA


(e) Compute the reconstruction error A ? B F of each reduced rank image, where · F denote the Frobenius norm of a matrix. Note that the higher the reconstruction error, the lower the quality of
the compressed image. Plot a graph of reconstruction error (y-axis) versus compression ratio (x-axis) for each image.
Answer: See Table 2.2 and Figure 2.3.
(f) State the minimum number of principal components (10, 30, 50, 100) needed to (visually) retain most of the salient features of each


image
rank
reconstruction error
img1
10
4.9565 ? 104
img1
30
3.7198 ? 104
img1
50
3.0998 ? 104
img1
100
2.2135 ? 104
img2
10
1.7798 ? 104
img2
30
1.2190 ? 104
img2
50
1.0236 ? 104
img2
100
7.4063 ? 103
img3
10
3.9544 ? 103
img3
30
3.1775 ? 103
img3
50
2.8146 ? 103
img3
100
2.2397 ? 103
Table 2.2. Reconstruction error for various images


(a) img1	(b) img2	(c) img3

Figure 2.3. Reconstruction error versus compression ratio

image (i.e., the city square in img1.jpg, shape of the face in img2. jpg, and shape of the apple in img3.jpg). Which image requires the least number of principal components? Which image requires the most number of principal components?
Answer:
img1.jpg: 50 components
img2.jpg: 30 components
img3.jpg: 10 components


2.3 Measures of Similarity and Dissimilarity
1. Consider the following binary vectors:
x1 = (1, 1, 1, 1, 1)
x2 = (1, 1, 1, 0, 0)
y1 = (0, 0, 0, 0, 0)
y2 = (0, 0, 0, 1, 1)

(a) According to Jaccard coefficient, which pair of vectors—(x1, x2) or (y1, y2)—are more similar to each other?
Answer:
Jaccard(x1, x2) = 3 = 0.6.
Jaccard(y1, y2) = 0 = 0.
Therefore, according to Jaccard coefficient, (x1, x2) are more simi- lar.

(b) According to simple matching coefficient, which pair of vectors— (x1, x2) or (y1, y2)—are more similar to each other?
Answer:
SMC(x1, x2) = 3 = 0.6.
SMC(y1, y2) = 3 = 0.6.
Therefore, according to simple matching coefficient, they are both equally similar.

(c) According to Euclidean distance, which pair of vectors—(x1, x2) or (y1, y2)—are more similar to each other?
Answer:
Euclidean(x1, x2) = √2 = 1.4142. Euclidean(y1, y2) = √2 = 1.4142.
Therefore, according to Euclidean distance, they are both equally similar.

2. Consider a weighted, undirected, graph G (see Figure 2.4 as an example). Let e(u, v) be the weight of the edge between nodes u and v, where e(u, u) = 0 and e(u, v) = ∞ if u and v is disconnected. Assume the


graph is a connected component, i.e., there exists a path between every two nodes. Suppose the path length, d(u, v), is defined as follows:



d(u, v) =

e(u, v),	if there is an edge between u and v;
, minw/=u/=v d(u, w) + d(w, v),  otherwise.


Is d(u, v) a metric? State your reasons clearly. (Check whether the positivity, symmetry, and triangle inequality properties are preserved.).



Figure 2.4. Weighted undirected graph.


Answer:

(a) Positivity property is preserved by definition since d(u, u) = 0 and
d(u, v) > 0 if u /= v.
(b) Symmetry property is preserved since the graph is undirected.
(c) Triangle inequality is not preserved. A counter-example is d(K, J) ≥
d(K, I) + d(I, J).

Therefore d(u, v) is not a metric.
3. For document analysis, numerous measures have been proposed to deter- mine the semantic similarity between two words using a domain ontology such as WordNet. For example, words such as dog and cat have higher semantic similarity than dog and money (since the former refers to two types of carnivores). Figure 2.5 below shows an example for comput- ing the Wu-Palmer similarity between dog and cat based on their path


length in the WordNet hypernym hierarchy. The depth h refers to the length of the shortest path from the root to their lowest common hyper- nym (e.g., carnivore for the word pair dog and cat), whereas k is the minimum path length between the two words.




Figure 2.5. Sample of the hypernym hierarchy in WordNet.

The Wu-Palmer similarity measure is defined as follows:

2h
W =
k + 2h

For example1, for dog and cat, W = 26/(4 + 26) = 0.867, whereas for
dog and money, W = 4/(19 + 4) = 0.174.

(a) What is the maximum and minimum possible value for Wu-Palmer similarity?

     1In this simplified example, we assume each word has exactly 1 sense. In general, a word can have multiple senses. As a result, the Wu-Palmer measure is given by the highest similarity that can be achieved using one of its possible senses.


Answer: Maximum value is 1; minimum value approaches 0.
(b) Let 1 ? W be the Wu-Palmer distance measure.
• Does 1 ? W satisfy the positivity property?
Answer: Yes. Since 1 ? W = k = 0 when k = 0, this implies
that d(u, v) = 0 if and only if u = v.
• Does 1 ? W satisfy the symmetry property?
Answer: Yes because W is a symmetric measure.
• Does 1 ? W satisfy the triangle inequality property?
Answer: No because each node can have more than one path
to the root, some maybe shorter than others. For example, the words (money, statute) are very dissimilar to each other. But (money, bill) and (bill, statute) are very similar, thus violating triangle inequality. The actual path for these words in the WordNet ontology are shown in Figure 2.6.


Figure 2.6. Sample of the hypernym hierarchy in WordNet.


4. Suppose you are given a census data, where every data object corre- sponds to a household and the following continuous attributes are used to characterize each household: total household income, number of house- hold residents, property value, number of bedrooms, and number of ve- hicles owned. Suppose we are interested in clustering the households based on these attributes.


(a) Explain why cosine is not a good measure for clustering the data. Answer: These attributes are all numerical and can have widely varying ranges of values, depending on the scale used to measure them. As a result, cosine measure will be biased by the attributes with largest range of magnitudes (e.g., total household income and property value).
(b) Explain why correlation is not a good measure for clustering the data.
Answer: The same argument as part (a). Because each attribute has different range, correlating the data points is meaningless.
(c) Explain what preprocessing steps and corresponding proximity mea- sure you should use to do the clustering.
Answer: Euclidean distance, applied after standardizing the at- tributes to have a mean of 0 and a standard deviation of 1, would be appropriate
5. Consider the following distance measure:
d(x, y) = 1 ? c(x, y),
where c(x, y) is the cosine similarity between two data objects, x and
y. Does the distance measure satisfy the positivity, symmetry, and tri- angle inequality properties? For each property, show your steps clearly. Assume x and y are non-negative vectors (e.g., term vectors for a pair of documents).
Answer:
(a) Positivity You need to show that ?x, y : d(x, y) = 1 ?  x·y  ≥ 0
and d(x, y) = 0 if and only if x = y.
By definition, x · y =  x	y cos ?, where ? is the angle between x
and y. Since cos ? ≤ 1 (from trigonometry), therefore
d(x, y) = 1 ?  x · y  = 1 ?  x	y cos ? = 1 ? cos ? ≥ 0,
x	y	x	y

which completes the first part of the proof.
If x = y, then
d(x, y) = 1 ?  x · x  = 1 ?  x	x cos 0 = 0.
x	x	x	x


However, if d(x, y) = 0, then

1 ? cos ? = 0 ? cos ? = 1 ? ? = 0
In other words, as long as x and y are co-linear to each other, d(x, y) = 0 (even though x /= y). The distance measure therefore does not satisfy the positivity property.
(b) Symmetry
Because x · y = y · x,
d(x, y) = 1 ?  x · y  = 1 ?  y · x  = d(y, x)
x	y	y	x

Hence, the distance measure satisfies the symmetry property.
(c) Triangle Inequality
First, note that cos ? decreases with increasing ? for 0 ≤ ? ≤ π/2 (we focus only on this range of values for ? because the vectors are non-negative). Since the distance measure d(x, y) = 1 ? cos ? depends on the angle between the two vectors x and y, the larger
the angle, the larger the distance. We can show that the distance measure violates triangle inequality by choosing the angles in such a way that d(x, z) > d(x, y)+d(y, z). Consider the situation shown in Figure 2.7 below.



Figure 2.7. Triangle inequality violation example


In this case, we have: d(x, z) = 1 ? cos ? and d(x, y) = d(y, z) = 1 ? cos(?/2). From trigonometry identities, cos ? = 2 cos2(?/2) ?


1.  Therefore, d(x, z) = 1 ? cos ? = 1 ? 2 cos2(?/2) + 1 = 2 ?
2 cos (?/2). On the other hand, d(x, y) + d(y, z) = 2  2 cos(?/2).
Since cos2(?/2) < cos(?/2) as long as 0 < cos(?/2) < 1, we have found a counter-example where

d(x, z) = 2 ? 2 cos2(?/2) > d(x, y) + d(y, z) = 2 ? 2 cos(?/2).
Here’s a simple example. Suppose we have 3 documents and 2 words data and mining. Document x contains the word data only and document z contains the word mining only. However, document y contains both words. We can represent the documents as follows:

x = (1, 0),  y = (1, 1),  z = (0, 1).
In this case, d(x, z) = 1 and d(x, y) = d(y, z) = 1 ? 1/√2 = 0.2929. Therefore, d(x, z) > d(x, y)+d(y, z), which is a violation of triangle
inequality.

6. Consider a database of web graphs. Each graph is unweighted and con- tains a set of nodes and directed edges. A node corresponds to a web page while an edge is a transition from one page to another when a user clicks on a hyperlink or enters the URL directly into the location bar of the Web browser. Each web graph also represents the Web session of a user. Consider the following approaches for defining the similarity between two Web sessions, s1 and s2.
Approach 1 Node-based similarity


Sim

(s , s ) = ?i I(wi ? s1) ? I(wi ? s2) ,

n  1	2

max(|s1|, |s2|)


where wi is a web page, |si| is the number of web pages visited during session si, max(a, b) is a function that returns the maximum value between a and b, and I(wi ? sj) is an indicator function whose value is 1 if session sj visited web page wi and 0 otherwise.
Approach 2 Link-based similarity

Sim (s , s ) = ?i,j I(wi ? wj ? s1) ? I(wi ? wj ? s2) ,

l  1	2

max(|s1|, |s2|)


where wi ? wj is a transition from page wi to wj, |si| is the number of transitions in session si, max(a, b) is a function that returns the maximum value between a and b, and I(wi ? wj ? sk) is an in- dicator function whose value is 1 if session sk contains a transition
from web page wi to wj and 0 otherwise.

(a) Consider the following two Web sessions: s1 = (A ? B ? C ? B ? D ? E) and s2 = (A ? C ? B ? E). Compute the node- based and link-based similarities for the Web graphs constructed
from the two sessions.
Answer: Simn(s1, s2) = 4/5 and Siml(s1, s2) = 1/5.
(b) Suppose the node-based similarity for s1 and s2 equals to 1. Can the web graphs for s1 and s2 be different? State your reasons clearly. Answer: Yes. As long as both graphs contain the same set of
nodes, the node-based similarity is equal to 1. But the graphs may still be different because the links in the graph could be different.
(c) Suppose Siml(s1, s2) = 1 according to approach 2. Can the web graphs for s1 and s2 be different? State your reasons clearly. Answer: No. The web graphs are the same because all the node transitions in s1 must also be present in s2, and vice-versa.
(d) Which approach do you think is more effective at measuring simi- larity between two web sessions? State your reasons clearly. Answer: Link-based similarity is more effective because its value is 1 only if the web graphs are isomorphic.

7. Consider the following distance measure D between two clusters of data points, X and Y:

D(X, Y) = min{d(x, y) : x ? X, y ? Y},
where d(x, y) is the Euclidean distance between two data points, x and
y. Intuitively, D measures the distance between clusters in terms of the closest two points from each cluster (see Figure 2.8). Does the dis-
tance measure satisfy the positivity, symmetry, and triangle inequality properties? For each property, show your proof clearly or give a counter- example if the property is not satisfied.
Answer:







Figure 2.8. Cluster distance measure


(a) Positivity: Since Euclidean distance between any two data points is always non-negative, therefore D(X, Y ) ≥ 0. D(X, y) can be zero even when X /= Y only if there is a data point is assigned to both
clusters X and Y (i.e., if overlapping clusters are allowed). So,
the distance measure satisfies the positivity property for disjoint clusters but not for overlapping clusters.
(b) Symmetry: Since Euclidean distance is a symmetric measure, ?(X, Y) = min{d(x, y) : x ? X, y ? Y} = min{d(y, x) : x ? X, y ? Y} = ?(Y, X). Thus, the measure is symmetric.
(c) Triangle Inequality: Triangle inequality property can be vio- lated. A counter-example is shown in Figure 2.9.



Figure 2.9. Violation of triangle inequality


8. For this question, assume each object is characterized by a set of continuous- valued attributes.
(a) If two objects have a cosine similarity of 1, must their attribute values be identical? Explain.


Answer: No. A cosine similarity of 1 simply implies that the two attribute vectors are parallel to each other. For example, when x = (1, 2) and y = (2, 4), then their cosine similarity is 1.
(b) If two objects have a correlation value of 1, must their attribute values be identical? Explain.
Answer: No. A correlation value of 1 simply implies that there is a linear relationship between the two attribute vectors. For example, when x = (1, 2) and y = (3, 5), then their correlation is 1.
(c) If two objects have a Euclidean distance of 0, must their attribute values be identical? Explain.
Answer: Yes. Consider a pair of objects with attribute vectors x and y. Suppose their Euclidean distance is d(x, y) = sqrt	i(xi ? yi)2 = 0, which is true only if xi = yi for all i.
(d) Let x and y be the attribute vectors of two objects. State whether the following proximity measures—cosine, correlation, and Euclidean distance—are invariant (unchanged) under the following transfor- mations. Specifically, if x	xr and y	yr, would cosine(x, y) = cosine(xr, yr), correlation(x, y) = correlation(xr, yr), and Euclidean(x, y)
= Euclidean(xr, yr)?
i. Translation: x ? x + c and y ? y + c, where c is a constant added to each attribute value in x and y.
Answer: Cosine is not invariant because cosine(x + c, y + c) =

i(xi+c)(yi+c)

i(xi+c)2(yi+c)2

i xiyi
x2y2

unless c = 0. Euclidean distance

√? i i	√?

Similarly, correlation measure is also invariant because when x ? x + c, then the mean will also be shifted x ? x + c but the standard deviation remains unchanged since ?x =
(x + c  x  c)2 =    (x  x)2. Thus, correlation(x +
c, y+c) = ?i(xi+c—x—c)(yi+c—y—c)  = ?i(xi—x)(yi—y) = correlation(x,y).
ii. Scaling: x ? cx and y ? cy, where c is a constant multiplied to each attribute value in x and y.

Answer: Cosine is invariant because √

cx cy
(?i cxi)2(?j cyj )2

√(?i xi)2(?j yj )2

Correlation is also invariant because when x ? cx, then both
the mean and standard deviation are re-scaled by the same fac-
tor: xr = ?i cxi = cx and ?x? = √? (cxi ? cx)2 = c?x. Eu-
n	i	√?

clidean distance is not invariant because

	

i(cxi ? cyi)2 =


iii. Standardization: x ? (x ? c)/d and y ? (y ? c)/d, where c
and d are constants.
Answer: Standardization is a combination of translation (by the mean of the vector) and scaling (by the standard deviation). Since correlation is invariant with respect to both operations, it is also invariant with respect to standardization. However, cosine and Euclidean distance are not invariant since they are not preserved by one of the two operations.
9. Consider the following survey data about users who joined an online community. The sample covariance between the user’s height (in mm) and number of years being a member of the community is 5.0.
(a) Suppose the sample covariance between the user’s age and number of years being a member of the community is only 0.5. Does this imply that user’s height is more correlated with number of years in the community than user’s age? Answer yes or no and explain your reasons clearly.
Answer: No. Covariance is not a dimensionless quantity, so its magnitude depends on the scale of measurement.
(b) Suppose the height attribute is re-defined as height above the av- erage for all users who participated in the survey. For example, a user who is 1650 mm tall has a height value of -50 mm (assuming the average height of all users is 1700 mm). Would the covariance between the re-defined height attribute and number of years in the community be greater than, smaller than, or equal to 5.0?
Answer: Equal. Let xh denote the height attribute and xy be the number of years in the community. The sample covariance between the two attributes is given by:



?xh,xy

i=1
=		1		(x N ? 1 N
— 
xh


)(xiy
— 
xy),


where xh and xy are the average height and average number of years, respectively. If we re-define the height attribute as xrh = xh ? xh,



then xrh = 0. Hence, the covariance between xrh and xy becomes


? ?	=	 1	 ?(xr
— 
xr )(x
— 
x )

xh,xy

N ? 1

ih
i=1 N

h	iy	y

=	 1		(x N ? 1 i=1
— 
xh
— 
0)(xiy
— 
xy)

=  ?xh,xy

This result means centering the height attribute has no effect on its covariance to other attributes.
(c) If the measurement unit for height is converted from mm to inches (where 1 inch = 25.4 mm), will the covariance between height (in inches) and number of years in the community be greater than, smaller than, or equal to 5.0?
Answer: Re-scaling the height attribute is equivalent to multi- plying the original attribute by some constant C, i.e., xrh = Cxh. Furthermore, we can show that xrh = Cxh. Thus the covariance between the rescaled height and number of years in the community will be:


? ?	=	 1	 ?(xr
— 
xr )(x
— 
x )

xh,xy

N ? 1

ih
i=1 N

h	iy	y

=	 1		(Cx N ? 1 i=1
N
— 
Cxh

)(xiy
— 
xy)

=	 C		(x N ? 1 i=1
=  C?xh,xy
— 
xh

)(xiy
— 
xy)



In this case, C =  1 

which is smaller than 1. Therefore, the

covariance value will be smaller when you convert the unit from
mm to inches.
(d) Suppose you standardize both the height and number of years in the community attributes (by subtracting their respective means and dividing by their corresponding standard deviations). Would their covariance value be greater than, smaller than, or equal to


5.0? To obtain full credit, you must prove your answer by showing the computations clearly.
Answer: The re-defined attributes after standardization are: xrh =

xh—xh , xr

= xy —xy . Furthermore, we can show that xr

= 0, xr

= 0.

?h	y	?y	h	y
Then,



? ?  ?

i=1
=	1		(xr
— 
xr )(xr
— 
xr )


N				
=	1		( xih ? xh )( xiy ? xy )


 1  ?N




(xih ? xh)(xiy ? xy)

=	?xh,xy
?h?y


(2.9)



Note that  1  ?x ,x

is equivalent to the correlation coefficient be-

?h?y	h  y
tween xh and xy. Since correlation coefficient is always less than or
equal to 1 whereas the original covariance value is +5, this means that the covariance value is smaller after standardization.
Next, we will prove that correlation coefficient is always between
?1 and +1. First, note that


?h =

,u,	1

?(xih ? xh)2,	?y =

,u,	1

?(xiy ? xy)2.

N ? 1 i=1	N ? 1 i=1

Thus, Equation (2.12) can be re-written as follows:

?xh,xy

?x?h,x?y	=

?h?y

?N (xih ? xh)(xiy ? xy)

=	s  ?


N
i=1

(xih ? xh)2

  ?


N
i=1

(xiy ? xy)2

(2.10)


Let hi = xih ? xh and yi = xiy ? xy. Equation (2.13) becomes

?N  hiyi

?h • ?y

?x?h,x?y	=

s  ?


N
i=1

2  ?


N	2
i=1  i

=	(2.11)
|?h||?y|


According to Cauchy-Schwarz inequality, for any vectors ?h and ?y, we have
?h • ?y ≤ |?h||?y|.
Thus the ratio on the right-hand side of Equation (2.14) is less than or equal to 1, which completes the proof.
10. Suppose you are given a database of patient’s demographic information from a healthcare provider. The covariance matrix obtained for three attributes: age, weight, and systolic blood pressure (bp) is shown below:

age ?
weight  ?
bp ?

, 389.75 199.37 135.12 ?
135.12  426.30  359.36

(a) Does this imply that user’s age is more correlated with his/her weight than systolic blood pressure? Answer yes or no and explain your reasons clearly.
Answer: No. Covariance is not a dimensionless quantity, so its magnitude depends on the scale of measurement. Even though covariance between age and weight is higher than that between age and systolic blood pressure, it is possible the correlation is lower.
(b) Suppose the weight attribute is centered by subtracting it with the average weight of all patients in the database. For example, a 200- pound patient has a weight recorded as 50 (if the average weight of the patients is 150 pounds). Would the covariance between the centered weight attribute and age be greater than, smaller than, or equal to 199.37?
Answer: Equal. Let xh denote the weight attribute and xy is the age attribute. The sample covariance between the two attributes is given by:



?xh,xy

i=1
=		1		(x N ? 1 N
— 
xh


)(xiy
— 
xy),


where xh and xy are the average weight and average age, respec- tively. If we re-define the weight attribute as xrh = xh ? xh, then xrh = 0. Hence, the covariance between xrh and xy becomes


? ?	=	 1	 ?(xr
— 
xr )(x
— 
x )

xh,xy

N ? 1

ih
i=1 N

h	iy	y

=	 1		(x N ? 1 i=1
— 
xh
— 
0)(xiy
— 
xy)

=  ?xh,xy

This result means centering the weight attribute has no effect on its covariance to other attributes.
(c) If the measurement unit for weight is converted from pounds to kilograms (where 1 kg = 2.2 pounds), will the covariance between weight (in kilogram) and age be greater than, smaller than, or equal to 199.37?
Answer: Re-scaling the weight attribute is equivalent to multi- plying the original attribute by some constant C, i.e., xrh = Cxh. Furthermore, we can show that xrh = Cxh. Thus the covariance between the rescaled weight and age will be:


? ?	=	 1	 ?(xr
— 
xr )(x
— 
x )

xh,xy

N ? 1

ih
i=1 N

h	iy	y

=	 1		(Cx N ? 1 i=1
N
— 
Cxh

)(xiy
— 
xy)

=	 C		(x N ? 1 i=1
=  C?xh,xy
— 
xh

)(xiy
— 
xy)


In this case, C =  1  which is smaller than 1. Therefore, the covari- ance value will be smaller when you convert the unit from pounds to kilograms.
(d) Suppose you standardize both the age and weight attributes (by subtracting their respective means and dividing by their corre- sponding standard deviations). Would their covariance value be greater than, smaller than, or equal to 199.37?


Answer: The re-defined attributes after standardization are: xrh =

xh—xh , xr

= xy —xy . Furthermore, we can show that xr

= 0, xr

= 0.

?h	y	?y	h	y
Then,



? ?  ?

i=1
=	1		(xr
— 
xr )(xr
— 
xr )


N				
=	1		( xih ? xh )( xiy ? xy )


 1  ?N




(xih ? xh)(xiy ? xy)

=	?xh,xy
?h?y


(2.12)



Note that  1  ?x ,x

is equivalent to the correlation coefficient be-

?h?y	h  y
tween xh and xy. Since correlation coefficient is always less than or
equal to 1 whereas the original covariance value is +5, this means that the covariance value is smaller after standardization.
Next, we will prove that correlation coefficient is always between
?1 and +1. First, note that


?h =

,u,	1

?(xih ? xh)2,	?y =

,u,	1

?(xiy ? xy)2.

N ? 1 i=1	N ? 1 i=1

Thus, Equation (2.12) can be re-written as follows:
?xh,xy

?x?h,x?y	=

?h?y

?N (xih ? xh)(xiy ? xy)

=	s  ?


N
i=1

(xih ? xh)2

  ?


N
i=1

(xiy ? xy)2

(2.13)


Let hi = xih ? xh and yi = xiy ? xy. Equation (2.13) becomes

?N  hiyi

hT y

?x?h,x?y	=

s  ?


N
i=1

2  ?


N
i=1

(2.14)
h	y
2
i


According to Cauchy-Schwarz inequality, for any vectors h and y, we have
hT y ≤ h	y .
Thus the ratio on the right-hand side of Equation (2.14) is less than or equal to 1, which completes the proof.
11. Consider the following distance measure for two sets, X and Y:
(X, Y) = 1	|X ? Y| ,
|X ? Y|
where ? is the intersection between the two sets, ? is the union of the two sets, and | · | denote the cardinality of the set. This measure is equivalent to 1 minus the Jaccard similarity. Does the distance measure
satisfy the positivity, symmetry, and triangle inequality properties? For each property, explain your reason clearly or give a counter-example if the property is not satisfied.
Answer:
(a) Positivity: Since |X ? Y| ≤ |X ? Y|, therefore |X?Y| ≤ 1 and
(X, Y) = 1	|X ? Y|	0.
|X ? Y|
Furthermore, if X = Y, then |X ? Y| = |X ? Y| = |X|. Hence,
(X, Y) = 1	|X ? Y| = 1	1 = 0.
|X ? Y|
Similarly, if ?(X, Y) = 0, then
1	|X ? Y| = 0,
|X ? Y|
which means, |X ? Y| = |X ? Y|, or equivalently, X = Y. Hence, the positivity property holds for the distance measure.
(b) Symmetry:
?(X, Y) = 1 ? |X ? Y| = 1 ? |Y ? X| = ?(Y, X).
|X ? Y|	|Y ? X|
Hence, the symmetry property holds for the distance measure.


(c) Triangle inequality:

?(X, Y) + ?(Y, Z)  =  1 ? |X ? Y| + 1 ? |Y ? Z|
|X ? Y|	|Y ? Z|
=	|X ? Y| ? |X ? Y| + |Y ? Z| ? |Y ? Z|
|X ? Y|	|Y ? Z|
≥	|X ? Y| ? |X ? Y| + |Y ? Z| ? |Y ? Z|


and

|X ? Y ? Z|

|X ? Y ? Z|


?(X, Z) = 1?|X ? Z| ≤ 1?   |X ? Z|	 = |X ? Y ? Z| ? |X ? Z| .

|X ? Z|

|X ? Y ? Z|

|X ? Y ? Z|





Figure 2.10. Illustration of triangle inequality

Figure 2.10 shows the Venn diagram for sets X, Y and Z. The number of data points in each subregion in the Venn Diagram is labeled A through G. From this figure, it can be easily seen that,
|X?Y|?|X?Y|+|Y?Z|?|Y?Z|   =   A+C+D+F+B+C+D+G
whereas
|X ? Y ? Z| ? |X ? Z| = A + B + C + F + G.


The preceding equations suggest that

|X ? Y ? Z| ? |X ? Z| ≤ |X ? Y| ? |X ? Y| + |Y ? Z| ? |Y ? Z|
Putting the inequalities together, we have (X, Z)	|X ? Y ? Z| ? |X ? Z|
|X ? Y ? Z|
|X ? Y| ? |X ? Y| + |Y ? Z| ? |Y ? Z|
|X ? Y ? Z|
≤  ?(X, Y) + ?(Y, Z)
12. Which similarity or distance measure is most effective for each of the domains given below:
(a) Which measure, Jaccard or Simple Matching Coefficient, is most appropriate to compare how similar are the answers provided by students in an exam. Assume that the answers to all the questions in the exam are either True or False.
Answer: Simple matching coefficient. The values of true and false are equally important when computing similarity.
(b) Which measure, Jaccard or Simple Matching Coefficient, is most appropriate to compare how similar are the locations visited by tourists at an amusement park. Assume the location information is stored as binary yes/no attributes (yes means a location was visited by the tourist and no means a location has not been visited).
Answer: Jaccard. Here places visited by the tourists should play a more significant role in computing similarity than places they did not visit.
(c) Which measure, Euclidean distance or correlation coefficient, is most appropriate to compare two flows in a network traffic. For each flow, we record information about the number of packets trans- mitted, number of bytes transferred, number of acknowledgments sent, and duration of the session.
Answer: Euclidean distance (after standardizing each attribute). Correlation coefficient is not meaningful here because it is not mean- ingful to correlate two flows which has different attribute values (i.e., correlating the attributes are meaningful but correlating the flows are not).


(d) Which measure, Euclidean distance or cosine similarity, is most appropriate to compare the coordinates of a moving object in a 2-dimensional space. For example, using GPS data, the object may be located at (31.4? West,12.4? North) at time t1 and (29.4? West,12.5? North) at another time t2. Note: we may use +/- to indicate East/West or North/South directions when computing the similarity or distance measures.
Answer: Euclidean distance. This is because cosine measures the angle of the two locations. Thus, if two locations lie along the same line through the origin, their cosine similarity will be 0 even though they are located far away from each other.
(e) Which measure, Euclidean distance or cosine similarity, is most ap- propriate to compare the similarity of items bought by customers at a grocery store. Assume each customer is represented by a 0/1 bi- nary vector of items (where a 1 means the customer had previously bought the item).
Answer: Cosine similarity because presence of an item in the trans- action plays a more important role in determining similarity than absence of the item.




3




Classification
3.1 Decision Tree
1. Consider a training set sampled uniformly from the the two-dimensional space shown in Figure 3.1.



Figure 3.1. 2D region


Assume that the training set size is large enough so that the probabilities can be calculated accurately based on the areas of the selected regions.


The space is divided into three classes—A, B, and C. In this exercise, you will build a decision tree from the training set.

(a) Compute the entropy for the overall data.
Answer: For overall data, p(A) = 0.32 + 0.09 = 0.41, p(B) = 0.42 + 0.04 = 0.46, and p(C) = 0.04 + 0.09 = 0.13. Therefore the overall entropy is

?0.41 log2 0.41 ? 0.46 log2 0.46 ? 0.13 log2 0.13 = 1.4254.
(b) Compare the entropy when the data is split at x <= 0.2, x <= 0.7, and y <= 0.6.
Answer:
i. Split at x = 0.2:
For the child node x ≤ 0.2, p(A) = 0, p(B) = 0.8, and p(C) = 0.2. Its entropy is ?0.8 log2 0.8 ? 0.2 log2 0.2 = 0.7219. For the child node x > 0.2, p(A) = 0.41/0.80 = 0.5125, p(B) =
0.3/0.8 = 0.3750, and p(C) = 0.09/0.80 = 0.1125. Its entropy is ?0.5125 log2 0.5125 ? 0.375 log2 0.375 ? 0.1125 log2 0.1125 =
1.3795. Therefore, the average entropy for the children is 0.2 ?
0.7219 + 0.8 ? 1.3795 = 1.2480.
ii. Split at x = 0.7:
For the child node x ≤ 0.7, p(A) = 0.2/0.7 = 0.2857, p(B) = 0.46/0.7 = 0.6571, and p(C) = 0.04/0.7 = 0.0571. Its entropy is ?0.2857 log2 0.2857?0.6571 log2 0.6571?0.0571 log2 0.0571 =
1.1503. For the child node x > 0.7, p(A) = 0.7, p(B) = 0, and
p(C) = 0.3. Its entropy is ?0.7 log2 0.7 ? 0.3 log2 0.3 = 0.8813. Therefore the average entropy for the children is 0.7 ? 1.1503 + 0.3 ? 0.8813 = 1.0696.
iii. Split at y = 0.6.
For the child node y ≤ 0.6, p(A) = 0.09/0.6 = 0.15, p(B) = 0.42/0.6 = 0.7, and p(C) = 0.09/0.6 = 0.15.  Its entropy is
?0.15 log2 0.15 ? 0.7 log2 0.7 ? 0.15 log2 0.15 = 1.1813. For the
child node y > 0.6, p(A) = 0.32/0.4 = 0.8 and p(B) = p(C) =
0.04/0.4  =  0.1.	Its entropy is ?0.8 log2 0.8 ? 0.1 log2 0.1 ?
0.1 log2 0.1 = 0.9219. Therefore, the average entropy for the children is 0.6 ? 1.1813 + 0.4 ? 0.9219 = 1.0776.
(c) Based on your answer in part (b), which attribute split condition do you think should be used as the root of the decision tree.


Answer: Comparing their entropy values, the split at x = 0.7 has the highest gain.
(d) Draw the full decision tree for the data set.
Answer: Full decision tree:



Figure 3.2. Decision tree for 2D region.


2. Draw the full decision tree that perfectly classifies each of the data sets given below. There could be more than one answer to each question (you only need to draw one). You do not have to consider the impurity measure used by the decision tree algorithm. Ignore pre-pruning and post-pruning. Assume there are no noise and missing attribute values.

(a) Consider a data set with three Boolean attributes, A, B, and C, and a binary class label y whose value is True if the number of at- tributes with True values is even and False otherwise. For example, if A=True, B=True, C=False, then y=True (because there are two attributes with True values).


Answer: This corresponds to a parity function for 3 Boolean at- tributes.



Figure 3.3. Decision tree for parity function


(b) Consider the 2-dimensional data set shown in Figure 3.4, where A, B, and C are the class labels for the respective regions.


Figure 3.4. 2D region


Answer: The decision tree is shown in Figure 3.5.

3. Show that the gini index of a node never increases after it has been split into smaller successor nodes. To simplify the problem, you can assume




Figure 3.5. Decision tree for 2D region

that both the splitting attribute associated with the node and the class label are binary valued.
Answer:
Suppose there are N training examples, divided into 2 classes, with n+
positive and n— negative examples. The gini index before splitting is:

g1 = 1 ?

 n+  2



 n— 2



(3.1)


After splitting on a binary attribute X, let n1+ and n1— be the number of positive and negative examples associated with the left child whereas n2+ and n2— be the number of positive and negative examples associated with the right child. Furthermore,

n1
n2 n+
=
=
=
n1+ + n1—
n2+ + n2— n1+ + n2+
n—
=
n1— + n2—
n1 + n2
=
N


The gini index after splitting is given as follows:


g2	=


n1
N  1 ?

n1+  2
n1

 n1— 2 



+ n2  1
N

n2+  2
n2

 n2— 2 



n1 + n2	n2

n2—	n2	n2

=	 1+

 1  	 2+	 2—
?

N
 n2
=  1
n1N

n1N
 n2
n1N

n1N
 n2
— n2N

n2N
 n2
n2N

n2N


(3.2)


Comparing Equations (3.1) and (3.2), we can prove that gini index never increases after splitting (i.e., g1 ≥ g2) by showing that:




or,

 n+  2




 n— 2



2
1+

n1N

2
+	2+
n2N

n2
+	1—
n1N

n2
+	2—
n2N



 n2
N	≤
n2

2	2
1+ +	2+
n1	n2
n2	n2


and	(3.3)

 —	 1— +  2—

(3.4)

N	n1	n2

To prove the inequality given in (3.3):


n2	n2	n2

n2	n2

(n1+ + n2+)2

 1+ +  2+ ?  +	=	 1+ +  2+ ?

n1	n2	N

n1	n2

n1 + n2

2
=	1+

n2(n1 + n2) + n2

n1(n1 + n2) ? n1n2(n1+ + n2+)2

n1n2(n1 + n2)
n2 n2 + n2 n2 ? 2n1n2n1+n2+
=	1+  2	2+  1
n1n2(n1 + n2)
(n1+n2 ? n2+n1)2
n1n2(n1 + n2)
≥ 0
The last step follows from the fact that (n1+n2 ? n2+n1)2 ≥ 0 (i.e., the square of any real numbers must be non-negative) and n1n2(n1+n2) ≥ 0. Thus, the inequality in (3.3) holds. A similar proof can be given for the
inequality in (3.4) by replacing n1+, n2+, and n+ with n1—, n2—, and
n—, respectively.


4. Consider a data set that contains 4 Boolean attributes A, B, C, and D, and a Boolean class y. For each Boolean expression below (between the class y and the rest of the attributes), state whether it is possible to construct a smaller decision tree that perfectly classifies the data without generating the complete tree (i.e., the number of leave nodes is less than 16). If possible, draw the tree.
(a) y = A ? B ? C ? D.
Answer: Yes, it is possible to construct a smaller tree.



Figure 3.6. Decision tree for y = A ? B ? C ? D.


(b) y = A ? B ? C ? D.
Answer: Yes, it is possible to construct a smaller tree.

Figure 3.7. Decision tree for y = A ? B ? C ? D.

(c) y = (A ? B) ? (C ? D).
Answer: Yes, it is possible to construct a smaller tree.

Figure 3.8. Decision tree for y = (A ? B) ? (C ? D).


5. Consider the training set given below for predicting lung cancer in pa- tients based on their symptoms (chronic cough and weight loss) and other lifestyle and environmental attributes (tobacco smoking and expo- sure to radon). Draw a two-level decision tree obtained using entropy as the impurity measure. Show your steps clearly (i.e., the computation of information gain for every candidate attribute at the first and second levels of the decision tree must be shown). Compute the training error of the decision tree.

Tobacco
Smoking
Radon
Exposure
Chronic
Cough
Weight
Loss
Lung
Cancer
Yes
Yes
Yes
No
Yes
Yes
No
Yes
No
Yes
Yes
No
Yes
Yes
Yes
Yes
No
Yes
Yes
Yes
No
Yes
No
Yes
Yes
Yes
No
No
No
No
No
No
Yes
No
No
No
No
Yes
Yes
No
No
No
Yes
No
No
No
No
No
Yes
No

Answer:
Before splitting: p(+) = p(?) = 0.5. Therefore, the overall entropy is
?0.5 log(0.5) ? 0.5 log(0.5) = 1. The contingency tables and entropies after splitting on the attributes are:


Lung Cancer
Entropy
(Child)
Entropy
Total
Information
Gain

Yes
No



Tobacco
Smoking
Yes
4
1
0.7219
0.7219
0.2781

No
1
4
0.7219


Radon
Exposure
Yes
2
0
0
0.7635
0.2365

No
3
5
0.9544


Chronic
Cough
Yes
4
3
0.9852
0.9651
0.0349

No
1
2
0.9183


Weight
Loss
Yes
3
2
0.9710
0.9710
0.0290

No
2
3
0.9710



So, the attribute with highest information gain is tobacco smoking. Next, for tobacco smoking = yes, the contingency tables and entropies after splitting on the remaining attributes are:


Tobacco smoking
= yes
Lung Cancer
Entropy
(Child)
Entropy
Total
Information
Gain

Yes
No



Radon
Exposure
Yes
1
0
0
0.6490
0.0729

No
3
1
0.8113


Chronic
Cough
Yes
4
0
0
0
0.7219

No
0
1
0


Weight
Loss
Yes
2
0
0
0.5510
0.1709

No
2
1
0.9183



Therefore, the best attribute to split the data (at level 2) for tobacco smoking = yes is Chronic cough. If chronic cough = yes, the leaf node is labeled as lung cancer = yes. If chronic cough = no, the leaf node is labeled as lung cancer = no.
For tobacco smoking = no, the contingency tables and entropies after splitting on the remaining attributes are:

Tobacco smoking
= no
Lung Cancer
Entropy
(Child)
Entropy
Total
Information
Gain

Yes
No



Radon
Exposure
Yes
1
0
0
0
0.7219

No
0
4
0


Chronic
Cough
Yes
0
3
0
0.4000
0.3219

No
1
1
1


Weight
Loss
Yes
1
2
0.9183
0.5510
0.1709

No
0
2
0



Therefore, the best attribute to split the data (at level 2) for tobacco smoking = no is radon exposure. If radon exposure = yes, the leaf node is labeled as lung cancer = yes. If radon exposure = no, the leaf node is labeled as lung cancer = no. The 2-level decision tree is shown below and its training error is 0.
6. Show that the error rate of a decision tree never increases if one of the nodes is split into smaller successor nodes. To simplify the problem, you can assume that both the splitting attribute associated with the node and the class label are binary valued. It is sufficient to assume that the tree originally has only 1 node. After splitting, the new decision tree has 3 nodes (1 root node and 2 leave nodes). Show that the error rate of the new decision tree cannot be larger than the error rate of the initial tree.




Figure 3.9. Decision tree for lung cancer prediction problem.

Answer: Consider a node (before splitting) with the following class distribution: n+ + n— = N , where n+ and n— are the number of training examples that belong to the positive and negative classes, respectively. The error rate of the node is:


Error rate (before splitting), E

=  1 ? max n+ , n—	(3.5)


Suppose the node is split into two children. The class distribution for the first child is n1+ and n1—, while the class distribution for the second child is n2+ and n2—, respectively. The error rates of the children are
E	= 1 ? max n1+ , n1— ,	E	= 1 ? max n2+ , n2— 

Therefore, the error rate after splitting is:


Error rate (after splitting), Ea

= n1 E
N	c1

+ n2 E
N	c2

=	n1 1 ? max  n1+ , n1—  + n2 1 ? max  n2+ , n2— 
=	n1 + n2 ? max  n1+ , n1—  ? max  n2+ , n2— 
=  1 ?  max  n1+ , n1—  + max  n2+ , n2—	(3.6)


To complete the proof, we need to show that
max  n1+ , n1—  + max  n2+ , n2—  ≥ max n+ , n— 

Note that
max  n1+ , n1—  + max  n2+ , n2— 
=  max  n1+ + n2+ , n1— + n2— , n1+ + n2— , n1— + n2+ 
= max  n+ , n— , n1+ + n2— , n1— + n2+ 
≤ max  n+ , n—	(3.7)
where, on the last line, we have used the following property of the max function, that adding any number to a list can only make the maximum value larger. Thus, Ea ≤ Eb.
7. Consider the two-dimensional data shown in Figure 3.10(a). The data consists of two classes: A and B.

Figure 3.10. (a) A 2-d data set, (b) a 2-level decision tree.


(a) Draw a 2-level decision tree for the data (see Figure 3.10(b)). Use gini index as the splitting criterion. Assume the classifier uses a


binary split, i.e., the splitting criterion at each internal node must be specified either as x ≤ c or y ≤ c, where c is some constant. In other words, do not specify the splitting criteria as 0.5 ≤ x ≤ 1.0
or x + y ≤ 1.
Answer: There are two important points to note here. First, the
probabilities associated with each class is proportional to its area in the diagram. Second, the best split position is always located at the boundary between the two classes. Thus, the candidate split positions you need to consider at level 1 of the decision tree are
x ≤ 0.2, x ≤ 0.5, y ≤ 0.7, and y ≤ 0.4.


Class
Gini
(Child)
Gini
Total

A
B


x ≤ 0.2
Yes
0.14
0.06
0.4200
0.3630

No
0.62
0.18
0.3488

x ≤ 0.50
Yes
0.26
0.24
0.4992
0.2496

No
0.5
0
0

y ≤ 0.4
Yes
0.40
0
0
0.2880

No
0.36
0.24
0.4800

y ≤ 0.7
Yes
0.55
0.15
0.3367
0.3617

No
0.21
0.09
0.4200

Thus, we should split at x = 0.5 because it has the lowest gini. Furthermore, for x > 0.5, notice that the entire region is classified as A, so it does not have to be split any further. For x ≤ 0.5, we
need to consider the following candidate split positions: x ≤ 0.2,
y ≤ 0.4, and y ≤ 0.7.


Class
Gini
(Child)
Gini
Total

A
B


x ≤ 0.2
Yes
0.14
0.06
0.4200
0.4560

No
0.12
0.18
0.4800

y ≤ 0.4
Yes
0.20
0
0
0.1920

No
0.06
0.24
0.3200

y ≤ 0.7
Yes
0.20
0.15
0.4898
0.4869

No
0.06
0.09
0.4800

Clearly, the best split position is y ≤ 0.4. If true, then the node is classified as A. Otherwise, it is classified as B. The resulting decision
tree is shown in Figure 3.11.




Figure 3.11. Decision tree for 2D problem.

(b) Compute the expected error rate of your decision tree when it is applied to a test set randomly sampled from the same 2-d space. Answer: The expected error rate of the tree is equal to the area of the upper left-hand corner box labeled as A, which is equal to 0.06 or 6%.

8. Consider the decision trees shown in Figures 3.12(a) and (b). For each approach described below, you need to compute the generalization er- rors for both trees and decide which tree is better. The training and validation data sets are shown in Figures 3.12(c) and (d), respectively.

(a) Optimistic approach (assumes generalization error is given by the training error).
Answer: Error rates for trees A and B and 10% and 20%, respec- tively. So tree A is better.
(b) Pessimistic approach using the upper bound on generalization error with ? = 0.25 (or Z1—?/2 = 1.15).
Answer: For this approach, you need to compute the expected error of each node and then add them up. Given a leaf node with N training examples that reach the node and an error rate of e, the upper bound on generalization error is:


z2 ?/2

r e(1—e)


z2 ?/2

er(e, N ) ≤

	2N	1—?/2	N	4N 2 
z 2
N


For tree A, there are five leaf nodes. The upper bound on error rates of the nodes (going from left to right) are: e1(0, 2) = e2(0, 2) =



Figure 3.12. Question 8.

0.3980, e3(0, 1) = 0.5694, e4(0, 2) = 0.3980, and e5(1/3, 3) = 0.6500.
So, the total expected error for tree A is 2 ? 0.3980 + 2 ? 0.3980 + 1 ? 0.5694 + 2 ? 0.3980 + 3 ? 0.6500 = 4.9077 and its expected error rate is 4.9077/10 = 0.4908.
For tree B, there are only three leaf nodes. The upper bound on error rates of the nodes (going from left to right) are: e1(0, 2) = e2(0, 2) = 0.3980 and e3(1/3, 3) = e4(1/3, 3) = 0.6500. So, the total
expected error for tree B is 2 ? 0.3980 + 2 ? 0.3980 + 3 ? 0.6500 + 3 ? 0.6500 = 5.4923 and its expected error rate is 4.9077/10 = 0.5492.
So, tree A is better.
(c) Reduced error pruning approach (generalization error is computed using the validation set shown in Figure 3.12(d)).
Answer: Error rates for trees A and B and 50% and 30%, respec- tively. So tree B is better.


(d) minimum description length (MDL) approach. The total descrip- tion length of a tree is given by:

Cost(tree, data) = Cost(tree) + Cost(data|tree),
• Each internal node of the tree is encoded by the ID of the split- ting attribute. If there are m attributes, the cost of encoding
each attribute is log2 m bits.
• Each leaf node is encoded using the ID of the class it is associ- ated with. If there are k classes, the cost of encoding a class is
log2 k bits.
• Cost(tree) is the cost of encoding all the nodes in the tree. To simplify the computation, you can assume that the total cost
of the tree is obtained by adding up the costs of encoding each internal node and each leaf node.
• Cost(data|tree) is encoded using the classification errors the tree commits on the training set. Each error is encoded by
log2 n bits, where n is the total number of training examples.
Answer:
Total description length for tree A is

       4 ? [log2 3| + 5 ? [log2 2| + 1 ? [log2 10| = 17 bits Total description length for tree B is
       3 ? [log2 3| + 4 ? [log2 2| + 2 ? [log2 10| = 18 bits So tree A is better.
9. Draw a decision tree that perfectly classifies each of the data sets de- scribed below. There could be more than one answer to each question (you only need to draw one tree). You do not have to create a sample of the data to answer this question. Assume there are no noise nor missing values.

(a) Consider a data set with three Boolean attributes, A, B, and C, with a binary class label y whose value is positive if the number of attributes with True values is exceeds those with False values, and


negative otherwise. For example, if A=True, B=True, C=False, then y=+ (because there are more attributes with True values). Draw the full decision tree (with 8 leaf nodes) for the data. State whether it is possible to construct a smaller tree (with number of leaf nodes less than 8) that perfectly classifies the data. If possible, show the tree.
Answer: Full decision tree is shown in Figure 3.13


Figure 3.13. Full decision tree.

It is possible to construct a smaller tree, which is shown in Figure 3.14.



Figure 3.14. Pruned decision tree.


(b) Consider the diagram shown in Figure 3.15, where A, B, and C are the class labels associated with each region. Assuming a sufficiently


large number of training examples are sampled from each region (enough to learn the correct decision boundaries), draw a decision tree that would perfectly classify the data. You may assume the decision tree algorithm uses only binary splits (instead of multiway splits).


Figure 3.15. 2-D data set.

Answer: The decision tree is shown in Figure 3.16



Figure 3.16. Decision tree for 2-D data set.


10. Consider the following data set that contains 100 training examples (50 labeled as positive class while the remainder labeled as negative class).


X
Y
Z
No. of + Examples
No. of ? Examples
1
1
1
5
0
1
1
0
0
20
1
0
1
20
0
1
0
0
0
5
0
1
1
10
0
0
1
0
15
0
0
0
1
0
10
0
0
0
0
15

(a) Build a two-level decision tree using gini index as the criterion for splitting. You need to show your computations for each candidate splitting attribute at each level clearly to obtain full credit. What is the overall training error rate of the induced tree? Note: we consider a tree with only 1 internal node and two leaf nodes as a one-level decision tree.
Answer:
For level 1,
Before splitting: Gini = 1?( 	5+20+10+15	 )2+( 	20+5+10+15	 )2 =

0.5

5+20+10+15+20+5+10+15

5+20+10+15+20+5+10+15

If we split the node on attribute X,


N1
N2
+
25
25
?
25
25
Gini(N ) = 1 ? ( 25 )2 ? ( 25 )2 = 0.5
1	50	50
Gini(N2) = 0.5
50	50
Gini(childern) = 100 ? 0.5 + 100 ? 0.5 = 0.5
If we split the node on attribute Y ,


N1
N2
+
30
20
?
20
30
Gini(N ) = 1 ? ( 30 )2 ? ( 20 )2 = 0.48
Gini(N ) = 1 ? ( 20 )2 ? ( 30 )2 = 0.48


50	50
Gini(childern) = 100 ? 0.48 + 100 ? 0.48 = 0.48
If we split the node on attribute Z,


N1
N2
+
35
15
?
10
40
Gini(N ) = 1 ? ( 35 )2 ? ( 10 )2 = 0.3457
Gini(N ) = 1 ? ( 15 )2 ? ( 40 )2 = 0.3967
2	55	55
45	55
Gini(childern) = 100 ? 0.3457 + 100 ? 0.3967 = 0.3737
The Gini(children) of Z is the smallest, hence we should split the node on Z. The first level tree is shown in Figure 3.17.



Figure 3.17. First split for 2(a).

For level 2:
For node N1:
If we split the node on attribute X,


N1
N2
+
25
10
?
0
10
Gini(N ) = 1 ? ( 25 )2 ? (  0 )2 = 0
Gini(N ) = 1 ? ( 10 )2 ? ( 10 )2 = 0.5
2	20	20
25	20
Gini(childern) = 45 ? 0 + 45 ? 0.5 = 0.222
If we split the node on attribute Y ,



N1
N2
+
15
20
?
0
10
Gini(N ) = 1 ? ( 15 )2 ? (  0 )2 = 0
Gini(N ) = 1 ? ( 10 )2 ? ( 10 )2 = 0.444
2	30	30
15	30
Gini(childern) = 45 ? 0 + 45 ? 0.444 = 0.296
The Gini(children) of X is the smaller, hence we should split the node on X.
For node N2:
If we split the node on attribute X,


N1
N2
+
0
15
?
25
15
Gini(N ) = 1 ? (  0 )2 ? ( 25 )2 = 0
Gini(N ) = 1 ? ( 15 )2 ? ( 15 )2 = 0.5
2	30	30
25	30
Gini(childern) = 55 ? 0 + 55 ? 0.5 = 0.273
If we split the node on attribute Y ,


N1
N2
+
15
0
?
20
20
Gini(N ) = 1 ? ( 15 )2 ? ( 20 )2 = 0.4898
Gini(N ) = 1 ? (  0 )2 ? ( 20 )2 = 0
2	20	20
35	20
Gini(childern) = 55 ? 0.4898 + 55 ? 0 = 0.3117
The Gini(children) of X is the smaller, hence we should split the node on X.
The two level decision tree is shown in Figure 3.18 The error rate is: 10+15 = 0.25.




Figure 3.18. Two level tree for 2(a).

(b) Use variable X as the first splitting attribute, then choose the best available splitting attribute at each of the two successor nodes. What is the training error rate of the induced tree?
Answer:
If we split on X first, For node N1:
If we split the node on attribute Y ,


N1
N2
+
5
20
?
20
5
Gini(N ) = 1 ? (  5 )2 ? ( 20 )2 = 0.32
Gini(N ) = 1 ? ( 20 )2 ? (  5 )2 = 0.32
2	25	25
25	25
Gini(childern) = 50 ? 0.32 + 50 ? 0.32 = 0.32
If we split the node on attribute Z,


N1
N2
+
25
0
?
0
25
Gini(N ) = 1 ? ( 25 )2 ? (  0 )2 = 0
Gini(N ) = 1 ? (  0 )2 ? ( 25 )2 = 0


25	25
Gini(childern) = 50 ? 0 + 50 ? 0 = 0
The Gini(children) of Z is the smaller, hence we should split the node on Z.
For node N2:
If we split the node on attribute Y ,


N1
N2
+
25
0
?
0
25
Gini(N ) = 1 ? ( 25 )2 ? (  0 )2 = 0
Gini(N ) = 1 ? (  0 )2 ? ( 25 )2 = 0
2	25	25
25	25
Gini(childern) = 50 ? 0 + 50 ? 0 = 0
If we split the node on attribute Y ,


N1
N2
+
10
15
?
10
15
Gini(N ) = 1 ? ( 10 )2 ? ( 10 )2 = 0.5
Gini(N ) = 1 ? ( 15 )2 ? ( 15 )2 = 0.5
2	30	30
20	30
Gini(childern) = 50 ? 0.5 + 50 ? 0.5 = 0.5
The Gini(children) of Y is the smaller, hence we should split the node on Y .
The two level decision tree is shown in Figure 3.19 The error rate is: 10+15 = 0.
(c) Discuss the results obtained in parts (a) and (b) above. Comment on the suitability of the greedy heuristic used as the splitting at- tribute selection.
Answer: Greedy heuristic method cannot guarantee to produce the optimal decision tree.




Figure 3.19. Two level tree for 2(a).

11. Consider the problem of predicting how well a baseball player will bat against a particular pitcher. The training set contains ten positive and ten negative examples. Assume there are two candidate attributes for splitting the data—ID (which is unique for every player) and Handedness (left or right). Among the left-handed players, nine of them are from the positive class and one from the negative class. On the other hand, among the right-handed players, only one of them is from the positive class, while the remaining nine are from the negative class.
(a) Compute the information gain if we use ID as the splitting attribute.
Answer: The entropy for the parent node is:
        Entropyparent = ?(0.5 log 0.5 + 0.5 log 0.5) = 1 If split using ID,


Gain = Entropyparent

20
(	Entropy(i)) = 1  20	1 ( 0 log 0 1 log 1) = 1
20
i=1


(b) Repeat part (a) using Handedness as the splitting attribute.
Answer: If split using ID,
10
Gain = Entropyparent? 20 (?0.9 log 0.9?0.1 log 0.1)?2 = 1?0.469 = 0.531
(c) Based on your answers in parts (a) and (b), which attribute will be chosen according to information gain?
Answer: According to information gain, we should choose ID to split the node.


(d) Repeat part (a) using gain ratio (instead of information gain).
Answer:
GainRATIOsplit =  Gain  = 	1	 =  1  = 0.231
SplitInfo	20?(—0.05 log 0.05)	4.322
(e) Repeat part (b) using gain ratio (instead of information gain).
Answer:
GainRATIOsplit =   Gain  = 	0.531	 = 0.531 = 0.531
SplitInfo	2?(—0.5 log 0.5)	1
(f) Based on your answers in parts (d) and (e), which attribute will be chosen according to gain ratio?
Answer: According to the gain ratio, we should choose Handedness to split the node.

12. Consider the training set given below for determining whether a loan application should be approved or rejected. Draw the full decision tree obtained using entropy as the impurity measure. Show your steps clearly (i.e., the computation of information gain for every candidate attribute must be shown). Compute the training error of the decision tree.

Long-Term
Debt
Unemployed
Credit
Rating
Down Payment
< 20%
Class
No
No
Good
Yes
Approve
No
No
Bad
No
Approve
No
No
Bad
Yes
Approve
No
No
Bad
No
Approve
Yes
No
Good
No
Approve
No
Yes
Good
Yes
Reject
Yes
No
Bad
No
Reject
Yes
No
Bad
Yes
Reject
Yes
No
Bad
Yes
Reject
Yes
Yes
Bad
No
Reject

Answer:
Before splitting: p(+) = p(?) = 0.5. Therefore, the overall entropy is
?0.5 log(0.5) ? 0.5 log(0.5) = 1. The contingency tables and entropies after splitting on the attributes are:




Class
Entropy
(Child)
Entropy
Total
Info
Gain

Reject
Approve



Long-Term
Debt
Yes
4
1
0.7219
0.7219
0.2781

No
1
4
0.7219


Unemployed
Yes
2
0
0
0.7635
0.2365

No
3
5
0.9544


Credit
Rating
Bad
4
3
0.9852
0.9651
0.0349

Good
1
2
0.9183


Down Payment
< 20%
Yes
3
2
0.9710
0.9710
0.0290

No
2
3
0.9710



So, the attribute with highest information gain is long-term debt. Next, for long-term debt = yes, the contingency tables and entropies after splitting on the remaining attributes are:


Long-term debt
= yes
Class
Entropy
(Child)
Entropy
Total
Info
Gain

Reject
Approve



Unemployed
Yes
1
0
0
0.6490
0.0729

No
3
1
0.8113


Credit
Rating
Bad
4
0
0
0
0.7219

Good
0
1
0


Down Payment
< 20%
Yes
2
0
0
0.5510
0.1709

No
2
1
0.9183



Therefore, the best attribute to split the data (at level 2) for long-term debt = yes is credit rating. If credit rating = bad, the leaf node is labeled as class = reject. If credit rating = good, the leaf node is labeled as class
= approve.
For long-term debt = no, the contingency tables and entropies after splitting on the remaining attributes are:



Long-term debt
= no
Class
Entropy
(Child)
Entropy
Total
Info
Gain

Reject
Approve



Unemployed
Yes
1
0
0
0
0.7219

No
0
4
0


Credit
Rating
Bad
0
3
0
0.4000
0.3219

Good
1
1
1


Down Payment
< 20%
Yes
1
2
0.9183
0.5510
0.1709

No
0
2
0



Therefore, the best attribute to split the data (at level 2) for long-term debt = no is unemployed. If unemployed = yes, the leaf node is labeled as class = reject. If unemployed = no, the leaf node is labeled as class
= approve. The decision tree is shown below and its training error is 0.

Figure 3.20. Decision tree for loan approval problem.


13. This question examines property of the entropy measure.
(a) Show that the entropy measure ?p(x) log p(x) is non-negative.
Answer: Since 0 ≤ p(x) ≤ 1, hence log p(x) ≤ 0. Hence, ?p(x) log p(x) ≥
0.
(b) Consider a pair of binary variables, X and Y . Suppose we need to estimate their joint probability distribution, P (X, Y ) as shown in the following 2 ? 2 table:


Y = 1
Y = 0
X = 1
P (X = 1, Y = 1) = a
P (X = 1, Y = 0) = b
X = 0
P (X = 0, Y = 1) = c
P (X = 0, Y = 0) = d


Find the joint probabilities (i.e., values of a, b, c, and d) that will maximize the entropy of the distribution assuming we know that P (X = 1) = a + b = 0.7 and   X,Y P (X, Y ) = a + b + c + d = 1.
Hint: solve the constraint optimization problem where the objective
function corresponds to the total entropy of the distribution (refer to lecture 3 on how to solve a constraint optimization problem with the Lagrange multiplier method).
Answer:
The entropy of the distribution is:
Entropy = ?(a log a + b log b + c log c + d log d).
L = ?(a log a+b log b+c log c+d log d)??1(a+b+c+d?1)??2(a+b?0.7) Take derivative with respect to a,b,c,d,?1,?2, we get:
∂L	1
∂a = log a + ln2 ? ?1 ? ?2 = 0
∂L	1
∂b = log b + ln2 ? ?1 ? ?2 = 0
∂L	1
∂c = log c + ln2 ? ?1 = 0
∂L	1
∂d = log d + ln2 ? ?1 = 0
∂L

∂?1

= ?(a + b + c + d ? 1) = 0

∂L
∂?2

= ?(a + b ? 0.7) = 0

The solutions are: a = b = 0.35, c = d = 0.15.
(c) Consider a nominal attribute X that has three possible values, x1, x2, and x3. Suppose you have a decision tree classifier that can produce either a multi-way split or a binary split on attribute X. Show that the average entropy of the successors for node X in a multi-way split is always smaller than or equal to the average entropy of the successors of node X in a binary split. Hint: you can apply the following Gibbs inequality for a given pair of probability distributions, p and q, for the proof:
— ? pi log pi ≤ ? pi log qi.


Answer:
Assume this is a binary classifier, and the classes are labeled as ”+” and ”?”. The numbers of samples in the nodes split by xi is denoted by ni in the multi-way split and in the binary split the
numbers of samples are denoted as nr1 and nr2,3. N is the total number of samples. It is obvious that nr1 = n1 and n2,3 = n2 + n3.
”n+” and ”n—” are represented as the number of samples belonging
i	i
to class ”+” and class ”+” respectively in node ni.
For multi-way split, the entropy is:


n1 n+

n+	n—	n—

 1 	 1 	 1 	 1 
Entropy1 =  ?	(	log	+	log	)

N
n2
— N (
n3
— N (

n1
 n+
n2
 n+
n3


log log

n1
 n+
n2
 n+
n3

n1
+  n—2 
n2
+  n—3 
n3


log log

n1
 n—2  )
n2
 n—3  )
n3

For binary split, the entropy is:


n1 n+

n+	n—	n—

Entropy2 =  ?

(  1  log  1  +  1  log  1  )

N  n1	n1	n1	n1

n2 + n3  n+ + n+

 n+ + n+

 n—2 + n—3 

 n—2 + n—3 

(
N	n2

+ n3

log

n2 + n3

+
n2 + n3

log

)
n2 + n3


If we want to compare the above two equations, we only need to compare these two equations:


n2 n+

n+	n—

n—	n3 n+

n+	n—	n—

E1 = ?

(  2  log  2  +  2  log  2  ) ?	(  3  log  3  +  3  log  3  )

N  n2	n2	n2	n2	N n3

n3	n3	n3



and

E2 = ?


n2 + n3


 n+ + n+	 n+ + n+	 n—2 + n—3 	 n—2 + n—3 

N	n2 + n3

n2 + n3

n2 + n3

n2 + n3


Using Gibbs’ inequality, we can show that,





n2 n+

n+	n—


n—	n3 n+

n+	n—	n—

E1	=  ?

(  2  log  2  +  2  log  2  ) ?	(  3  log  3  +  3  log  3  )

N  n2	n2	n2	n2	N n3

n3	n3	n3

n2 n+
≤ ? N ( n
n3 n+
— N ( n

log

log

 n+ + n+ n2 + n3
 n+ + n+ n2 + n3

+  n—2 
n2
+  n—3 
n3

log

log

 n—2 + n—3  )
n2 + n3
 n—2 + n—3  )
n2 + n3

n+ + n+	n+ + n+	n— + n—	n— + n—
=  ?(  2	3  log  2	3  +   2	3  log  2	3  )
N	n2 + n3	N	n2 + n3
n2 + n3 n+ + n+	n+ + n+	n— + n—	n— + n—
=  ?	(  2	3  log  2	3  +   2	3  log  2	3  )

N
=  E2

n2 + n3

n2 + n3

n2 + n3

n2 + n3


Hence, Entropy1 ≤ Entropy2.
14. This question examines the relationship between entropy (H) and mutual information (I).


(a) Based on the following definitions:

I(X, Y )  =
x?X,y?Y



 p(x, y) 
p(x, y) log
p(x)p(y)

H(X)  =  ?	p(x) log p(x)
x?X

H(X, Y )  =  ?
x?X,y?Y

p(x, y) log p(x, y)


Prove that I(X, Y ) = H(X) + H(Y ) ? H(X, Y ).



Answer:

I(X, Y )  =
x?X,y?Y
=
x?X,y?Y



 p(x, y) 
p(x, y) log
p(x)p(y)
p(x, y) log p(x, y) ?
x?X,y?Y






p(x, y) log p(x)

— x?X?,y?Y

p(x, y) log p(y)

=	?	p(x, y) log p(x, y) ? ?  ? p(x, y)  log p(x)
— ?  ? p(x, y)  log p(y)

y?Y
=
x?X,y?Y

x?X
p(x, y) log p(x, y) ?	p(x) log p(x)
x?X

— p(y) log p(y)
y?Y
=  ?H(X, Y ) + H(X) + H(Y )
(b) Consider a data source that generates a letter ? from a set of al- phabets {a, b, c, · · · , z}. If a vowel (a, e, i, o, u) is four times more likely to be generated than a consonant (b, c, d, f, · · · , z), calculate
the entropy for ?.
Answer: Let V be the set of vowels and C be the set of consonants. Based on the given information:


P (? = ? ) =	4p, if ?i ? V;
p,	if ?i ? C.
Since there are 5 vowels and 21 consonants and ?26

(3.8)

P (? = ?i) =

1:
5 ? 4p + 21 ? p = 1 =? p = 1/41.


Therefore, entropy for the random variable ? is

26
Entropy(?)  =  ?	P (?i) log2 P (?i)
i=1
4	4	1	1
=  ?5 ? 41 log2 41 ? 21 ? 41 log2 41
=  4.3819 bits.

(c) Consider a pair of binary variables, X and Y . Suppose we need to estimate their joint probability distribution, P (X, Y ) as shown in the following 2 ? 2 table:


Y = 1
Y = 0
X = 1
P (X = 1, Y = 1) = a
P (X = 1, Y = 0) = b
X = 0
P (X = 0, Y = 1) = c
P (X = 0, Y = 0) = d
Find the joint probabilities (i.e., values of a, b, c, and d) that will maximize the entropy of the distribution assuming we know that P (X = 1) = a + b = 0.6 and   X,Y P (X, Y ) = a + b + c + d = 1.
Hint: solve the constraint optimization problem where the objective
function corresponds to the total entropy of the distribution.
Answer: We can pose this as the following optimization problem:


max
a,b,c,d

?a log a ? b log b ? c log c ? d log d

s.t.	a + b = 0.6
a + b + c + d = 1

By using the Lagrange multiplier method, define the Lagrangian as:

L = ?a log a ? b log b ? c log c ? d log d ? ?(a + b ? 0.6) ? µ(a + b + c + d ? 1)


Take the partial derivatives with respect to a, b, c, and d, and set them to zero:

— ? ? µ = 0
— ? ? µ = 0
— µ = 0
— µ = 0


a = b = 2—(1/ ln 2+?+µ)
c = d = 2—(1/ ln 2+µ)

Since a + b = 0.6, this reduces to a = b = 0.3. Plugging this into
a + b + c + d = 1 and using c = d, this gives c = d = 0.2.
(d) Based on your answer in part (c), calculate the mutual information between X and Y .
Answer:

P (X = 1) = a + b = 0.6, P (X = 0) = c + d = 0.4 P (Y = 1) = a + c = 0.5, P (Y = 1) = b + d = 0.5 H(X) = ?0.6 log 0.6 ? 0.4 log 0.4 = 0.971
H(Y ) = ?0.5 log 0.5 ? 0.5 log 0.5 = 1.000
H(X, Y ) = ?2 ? 0.3 log 0.3 ? 2 ? 0.2 log 0.2 = 1.971
Therefore, mutual information is H(X) + H(Y ) ? H(X, Y ) = 0.
15. This questions compares entropy against Gini index as impurity mea- sures for decision trees.
(a) Consider a two-class problem. Show that the entropy of a node in the decision tree is always greater than or equal to its gini index (use log2 for entropy).


Answer: Let p and (1 ? p) be the distribution of the two classes at a given node. The entropy of the node is E = ?p log2 p ? (1 ? p) log2(1 ? p) whereas its gini is G = 1 ? p2 ? (1 ? p)2. We need to show that

E ? G = ?p log2 p ? (1 ? p) log2(1 ? p) ? 1 + p2 + (1 ? p)2 ≥ 0
First, note that the negative logarithm is a convex function. Ac- cording to Jensen inequality, for any convex function f (x):
? ?if (xi) ≥ f (? ?ixi),
i	i
where ?i ?i = 1. Thus:

E ? G =  ?p log2 p ? (1 ? p) log2(1 ? p) ? 1 + p2 + (1 ? p)2
=  ?p log2 p ? (1 ? p) log2(1 ? p) ? 2p(1 ? p)
≥ ? log2	p2 + (1 ? p)2	? 2p(1 ? p)
          = ? log2 1 ? 2p(1 ? p)  ? 2p(1 ? p)	(3.9)
where Jensen inequality was applied on the third line. Based on the Taylor series expansion of the logarithm function:

log(1 ? x) = ? ? x


Thus



n
?	2p(1 ? p)

E ? G  ≥

=

n=1 n?=2

n	? 2p(1 ? p)
n
2p(1 ? p)

n

which is always non-negative since 2p(1 ? p) ≥ 0 when 0 ≤ p ≤ 1.
The latter can be proved as follows. By contradiction, suppose


2p(1 ? p) < 0 and 0 ≤ p ≤ 1. The second inequality implies p ≥ 0 and (1?p) ≥ 0. But, since 2p(1?p) < 0, either p < 0 or (1?p) < 0, which is a contradiction. Thus, 2p(1 ? p) must be non-negative.
(b) Consider a decision tree classifier for two-class problem. Suppose we have the option of choosing either the binary attribute A or B as our splitting condition (see Figure 3.21). If attribute A is preferred over attribute B as the splitting condition according to the entropy measure, is it possible for attribute B to be preferred over attribute A according to gini index? If so, give an example; otherwise, prove that it is impossible.

Figure 3.21. Two options for decision tree

Answer: Yes, it is possible for the different measures to select dif- ferent attributes as the splitting condition. Consider the following class distribution of the child nodes for nodes A and B. Assume there are 4 positives and 6 negative examples in the training data (before splitting). The contingency tables after splitting on at- tributes A and B are:

A = a1	A = a2
+
?

B = b1	B = b2
+
?

The weighted entropy of the children after splitting on A is:

4	4	3	3
EA=a1	= ? 7 log 7 ? 7 log 7 = 0.9852
3	3	0	0
EA=a2	= ? 3 log 3 ? 3 log 3 = 0
EA	=  7/10EA=a1 + 3/10EA=a2 = 0.6897


The weighted entropy of the children after splitting on B is:

3	3	1	1
EB=b1	= ? 4 log 4 ? 4 log 4 = 0.8113
1	1	5	5
EB=b2	= ? 6 log 6 ? 6 log 6 = 0.6500
E + B	=  4/10EB=b1 + 6/10EB=b2 = 0.7145

Since EA < EB, attribute A will be chosen to split the node. The gini after splitting on A is:

GA=a1	=  1 ?
GA=a2	=  1 =

  4  2
  3  2


  3  2
  0  2



= 0.4898

= 0

GA	=  7/10GA=a2 + 3/10GA=a2 = 0.3429


The gini after splitting on B is:
  1  2

  3  2

GB=b1	=  1 ?
GB=b2	=  1 =

4	?
  1  2



4
  5  2


= 0.3750

= 0.2778

GB	=  4/10GB=b1 + 6/10GB=b2 = 0.3167

Since GB < GA, attribute B will be chosen to split the node.
16. Consider a node in a decision tree with n+ positive and n— negative training examples. If the node is split to its k children, The average weighted entropy of the children is given by

Entropy(children) =	nk Entropy(t ),
n	k
k

where nk is the number of training examples associated with the child node tk and n = k nk = n+ + n—. Apply the formula to calculate the average weighted entropy for each of the candidate test conditions shown below. Based on their entropy values, which attribute, A or B, should be chosen to split the parent node?




Answer: For node A, the entropies of its children are

10	10	5	5
Entropy(left)  =  ? 15 log2 15 ? 15 log2 15 = 0.9183
5	5	20	20
Entropy(middle)  =  ? 25 log2 25 ? 25 log2 25 = 0.7219
10	10	0	0
Entropy(right)  =  ? 10 log2 10 ? 10 log2 10 = 0
The average weighted entropy for node A is

15	25	10
50 ? 0.9183 + 50 ? 0.7219 + 50 ? 0 = 0.6365
Similarly, for node B, the entropies of its children are

20	20	10	10
Entropy(left)  =  ? 30 log2 30 ? 30 log2 30 = 0.9183
5	5	15	15
Entropy(right)  =  ? 20 log2 20 ? 20 log2 20 = 0.8113
The average weighted entropy for node B is

30	20
50 ? 0.9183 + 50 ? 0.8113 = 0.8755
Based on their average weighted entropy values, node A should be chosen because its value is smaller.
17. Consider a node in a decision tree with n+ positive and n— negative training examples. If the node is split to its k children, The average weighted gini index of the children is given by

Gini(children) =	nk Gini(t ),
n	k
k


where nk is the number of training examples associated with the child node tk and n = k nk = n+ + n—. Apply the formula to calculate the average weighted gini for each of the candidate test conditions shown below. Based on their gini values, which attribute, A or B, should be chosen to split the parent node?



Answer:
After splitting on attribute A, the gini index for each child is as follows:
Gini(left)  =  1 ? ( 15 )2 ? (  5 )2 = 0.3750
Gini(middle) =  1 ? (  5 )2 ? ( 10 )2 = 0.4444
Gini(right)  =  1 ? ( 10 )2 ? ( 15 )2 = 0.4800
Thus, the average weighted gini for the children is
20	15	25
Gini(children) = 60 ? 0.3750 + 60 ? 0.4444 + 60 ? 0.4800 = 0.4361.
After splitting on attribute B, the gini index for each child is as follows: Gini(left) = 1 ? ( 20 )2 ? ( 10 )2 = 0.4444
Gini(right)  =  1 ? ( 10 )2 ? ( 20 )2 = 0.4444

Thus, the average weighted gini for the children is
30	30
Gini(children) = 60 ? 0.4444 + 60 ? 0.4444 = 0.4444.
Based on the results, attribute A should be chosen to split the data since it has a lower gini.


18. Consider the decision tree shown in Figure 3.22 for a binary classification problem. Assume the classes are denoted as + and ?, respectively. Suppose there are four binary attributes in the data, A, B, C, and D.
The counts shown in the leaf nodes of the tree correspond to the number of training examples assigned to the nodes. Assume that the decision tree classifier assigns the majority class of training examples as the class label of each leaf node.


Figure 3.22. Unpruned decision tree


(a) Calculate the training error rate of the decision tree.
Answer: There are 100 examples in the training set. Assuming the leaf nodes are assigned to the majority class of training examples,

the training error rate of the tree is  20

= 0.2.

(b) Calculate the generalization error rate of the decision tree using the validation set given below. Note that the wildcard ? shown in the table means the value could be either 0 or 1.


A
B
C
D
Number of + examples
Number of ? examples
0
0
0
*
20
10
0
0
1
*
0
5
0
1
*
0
10
5
0
1
*
1
5
5
1
*
0
*
5
25
1
*
1
*
10
0


Answer: There are 100 examples in the validation set. The esti-

mated generalization error of the tree is  25

= 0.25.

(c) Calculate the training and generalization error estimate of the pruned decision tree shown in Figure 3.23. Based on your estimate of gen-


Figure 3.23. Pruned decision tree

eralization error, which tree should be preferred (the unpruned tree given in Figure 3.22 or the pruned tree given in Figure 3.23)?

Answer: The training error of the pruned tree is  25

= 0.25 while

its generalization error is  25

= 0.25. According to estimated gener-

alization errors using the validation set, both trees are equivalent.
However, since the pruned tree is simpler, it should be preferred over the unpruned tree.
(d) Apply the minimum description length principle to determine which tree should be preferred (the unpruned tree given in Figure 3.22 or the pruned tree given in Figure 3.23). Assume the tree requires log2 c bits to encode each leaf node (where c is number of classes), log2 d bits to encode each internal node (where d is number of at- tributes), and log2 N bits to encode each misclassified training ex- ample (where N is the number of training examples).
Answer: The unpruned tree has 5 internal and 6 leaf nodes. It also made 20 mistakes on a training set of size 100. Thus, the total


description length for the unpruned tree is

6 ? log2 2 + 5 ? log2 4 + 20 ? [log2 100| = 156 bits
The pruned tree has 4 internal and 5 leaf nodes. It made 25 mistakes on the training set. So, the total description length for the pruned tree is

5 ? log2 2 + 4 ? log2 4 + 25 ? [log2 100| = 188 bits
According to the MDL principle, the unpruned tree should be pre- ferred over the pruned one.

3.2 Model Evaluation
1. You have been asked to develop a classification model for diagnosing whether a patient is infected with a certain disease. To help you con- struct the models, your collaborator has provided you with a small train- ing set (N = 10) with equal number of positive and negative examples. You tried several approaches and found two most promising models, C1 and C2. The outputs of the models in terms of predicting whether each of the training examples belong to the “positive” class are summarized in the table below. The first row shows the probability a training example belongs to the positive class according to classifier C1, while the second row shows the same information for classifier C2. The last row indicates the true class label of the 10 training examples.


P (y = +|C1)
0.1
0.15
0.2
0.3
0.31
0.4
0.62
0.77
0.81
0.95
P (y = +|C2)
0.25
0.49
0.05
0.35
0.66
0.6
0.7
0.65
0.55
0.99
y
-
+
-
-
+
-
+
+
-
+

(a) Draw the corresponding ROC curves for both classifiers on the same plot.
Answer: See Figure 3.24.
(b) Compute the area under ROC curve for each classifier.	Which classifier has a larger area under the ROC curve?
Answer:
AUC(C1) = 0.68



1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0	0.2	0.4	0.6	0.8	1
False Positive

Figure 3.24. ROC curves for both classes.

AUC(C2) = 0.92
Hence, C2 has larger area under the ROC curve.
(c) Compute the Wilcoxon Mann Whitney statistic for both classifiers. The statistic can be computed as follows:

m—1	n—1
WMW =	i=0	j=0	 ,	(3.10)
mn


where

I(x, y) =	1,  xi > yj ;
0,  otherwise.

Note that {x0, x1, · · · , xm—1} correspond to the classifier outputs for the m positive examples while {y0, y1, · · · , yn—1} correspond to the classifier outputs for the n negative examples (in this exercise,
m = n = 5). Which classifier has a larger WMW value? Based on your answers, state the relationship between WMW and the ROC curve.



Answer:



WMW (C1) =

WMW (C2) =


1 + 3 + 4 + 4 + 5


25
3 + 5 + 5 + 5 + 5


25



= 0.68

= 0.92

Hence, C2 has larger WMW. WMW is equivalent to the area under
ROC curve.
2. You have been asked to develop a classification model for diagnosing whether a patient is infected with a certain disease. To help you evaluate the models, your collaborator has provided you with a small test set (N = 10) with equal number of positive and negative examples. You applied two classifiers, C1 and C2. The outputs of the classifiers in terms of predicting whether each of the test examples belong to the “positive” class are summarized in the table below. The first row shows the probability a test example belongs to the positive class according to classifier C1, while the second row shows the same information for classifier C2. The last row indicates the true class label of the 10 test examples.


P (y = +|C1)
0.15
0.2
0.25
0.37
0.41
0.55
0.65
0.8
0.92
0.99
P (y = +|C2)
0.33
0.22
0.1
0.41
0.68
0.59
0.72
0.75
0.64
0.95
y
-
-
+
-
+
-
-
+
+
+
(a) Draw the corresponding ROC curves for both classifiers on the same plot. Answer: See Figure 3.25.
(b) For each classifier Ci, what is the optimal threshold we should use for P (y = +|Ci) to obtain a high true positive rate and low false positive rate? Draw the confusion matrix associated with each clas-
sifier (using the selected optimal threshold for predicting positive class). Based on your results, which classifier is better in terms of
(a) accuracy, and (b) F-measure?
Answer: For classifier C1, the optimal threshold is when P (y =
+|C1) ≥ 0.7. The confusion matrix for the classifier is

P (y = +|C1) ≥ 0.7
Predicted

+
-
Actual
+
3
2

-
0
5





Figure 3.25. ROC curve for classifiers C1 and C2

F -measure =   2?3	 = 6 = 0.75.

Accuracy =  8

2?3+2+0	8
= 0.8.

10
For classifier C2, the optimal threshold is when P (y = +|C1) ≥
0.625. The confusion matrix for the classifier is

P (y = +|C2) ≥ 0.625
Predicted

+
-
Actual
+
4
1

-
1
4


F -measure =   2?4	 =  8

= 0.8.

Accuracy =  8

2?4+1+1	10
= 0.8.

10
C2 is better than C1 in terms of F-measure but they both have the same accuracy.
(c) Compute the area under ROC curve for each classifier.	Which classifier has a larger area under the ROC curve?
Answer:
For C1: Area under ROC curve = 0.4?0.6+0.2?0.8+0.4?1 = 0.8.
For C2: Area under ROC curve = 0.2 ? 0.4 + 0.8 ? 0.8 = 0.72. Classifier C1 has a larger area under ROC curve.
(d) Suppose a binary classifier produces a true positive rate of 40% and false positive rate of 60% (i.e., worse than random guessing).


Explain a simple approach you can use to improve the performance of the classifier so that it performs better than random guessing. What is the expected true positive rate and false positive rate of the classifier using your proposed approach?
Answer: A simple approach would be to predict the opposite of what the classifier says. For example, if the classifier predicts it to be a positive class, we should predict it as negative class instead. Similarly, if the classifier predicts it to be a negative class, we should declare it as positive. By reversing the prediction, the true positive rate of the classifier becomes 60% and its false positive rate becomes 40%.

3. Suppose we are given a pair of “independent” classifiers, C1and C2 (being independent means their errors are uncorrelated).




Assume the classifiers have been trained on a two-class problem (denoted as positive and negative class, respectively). The class distribution is skewed, i.e., the proportion of negative class outnumbers the positive class by 9:1. The precision and recall for classifier C1 (with respect to the positive class) are 0.5 and 0.8, respectively. On the other hand, the precision and recall for classifier C2 (with respect to the positive class) are both 0.6. Consider the hybrid classifier obtained by combining C1 and C2. Assume the precision and recall of C2 remain unchanged when used in the hybrid setting (even though the class proportion has changed). Compare the F-measure of the hybrid classifier against that for C1 and C2. Which of them is the best? Show your steps clearly to receive full credit.
Answer: Suppose there are N total examples. Based on the information given, the confusion matrix for classifiers C1 and C2 are given below:


Classifier
C1
Predicted class


+
?

Actual
class
+
0.08N
0.02N
0.1N

?
0.08N
0.82N
0.9N

0.16N
0.84N
N

Precision = 0.5, Recall = 0.8, F-measure =  2rp

Classifier
C2
Predicted class


+
?

Actual
class
+
0.06N
0.04N
0.1N

?
0.04N
0.86N
0.9N

0.1N
0.9N
N
Precision = 0.6, Recall = 0.6, F-measure =  2rp

= 0.6154






= 0.6


For the hybrid classifier, after applying C1, 84% of the training exam- ples will be propagated to C2, out of which there are 0.02N positive and 0.82N negative examples. Based on this information, the confusion ma- trix for C2 in the hybrid setting is given below (assuming it maintains the same precision and recall values as before):

Classifier
C2
Predicted class


+
?

Actual
class
+
0.012N
0.008N
0.02N

?
0.008N
0.812N
0.82N

0.02N
0.82N
0.84N
A summary of the class distribution at each leaf node in the hybrid classifier is shown in the Figure below.




The precision, recall, and F-measure for the hybrid classifier are:
	0.08N +0.012N	
0.08N +0.08N +0.012N +0.008N

Recall = 0.08N +0.012N

= 0.92

F-measure = 2?0.5111?0.92 = 0.6571.
This is better than the F-measure for both C1 and C2.
4. Consider the two decision trees shown in Figure 3.26 for a binary classifi- cation problem. Assume the classes are denoted as + and ?, respectively. Suppose there are four binary attributes in the data, A, B, C, and D.
The counts shown in the leaf nodes of the tree correspond to the number of training examples assigned to the nodes. Assume that the decision tree classifier assigns the majority class of training examples as the class label of each leaf node.


Figure 3.26. Two candidate decision trees


(a) Draw the confusion matrix for both trees on the training data. A confusion matrix is a table that summarizes the number of examples correctly or incorrectly predicted by the model. For example:


Predicted

+
?
Actual
+
n++
n+—

?
n—+
n——

In the above table, n+— is the number of positive examples incor- rectly predicted as negative class.


Answer: For the left tree, we may assign the class labels of the leaf nodes as follows (from left to right): +, -, -, +. In this case, the confusion matrix for the left tree is


C1
Predicted

+
?
Actual
+
20
10

?
10
20

If the right-most leaf node was assigned to the negative class, then the confusion matrix for the left tree is

C2
Predicted

+
?
Actual
+
10
20

?
0
30

For the right tree, we may assign the class labels of the leaf nodes as follows (from left to right): +, -, +. In this case, the confusion matrix for the left tree is

C3
Predicted

+
?
Actual
+
25
5

?
20
10

If the right-most leaf node was assigned to the negative class, then the confusion matrix for the right tree is


C4
Predicted

+
?
Actual
+
15
15

?
10
20

(b) Calculate the training error rate of both decision trees. Which tree has a lower training error?
Answer: The training error rate for the left tree is: Training error = 20 = 0.33.
The training error rate for the right tree is:


Training error = 25 = 0.42.
Thus, the left tree has a lower error rate.
(c) Apply the minimum description length principle to determine which tree should be preferred.
Answer: To apply MDL, we first compute the following:
The cost for encoding each internal node = log2 d = log2 4 = 2 bits. The cost for encoding each leaf node = log2 c = log2 2 = 1 bit.
The cost for encoding each error = log2 N = [log2 60| = 6 bits. Since the left tree has 3 internal nodes, 4 leaf nodes, and misclassifies
20 examples, its total description length is

3 ? 2 + 4 ? 1 + 20 ? 6 = 130 bits.
Since the right tree has 2 internal nodes, 3 leaf nodes, and misclas- sifies 25 examples, its total description length is

2 ? 2 + 3 ? 1 + 25 ? 6 = 157 bits.
Thus, according to the MDL principle, the left tree should be pre- ferred.




4




Alternative Classification Techniques

4.1 Linear Classifier and SVM
1. Consider the 2-dimensional data shown in Figure 4.1. There are three data points, two of them are classified as positive (red circles) and one is negative (blue square). Let the decision boundary of the linear classifier be wT x + b = 0 (shown as a red line in the diagram).

(a) The geometric margin of each data point is the perpendicular dis- tance between each data point to the decision boundary (shown as d1, d2, and d3, respectively). Derive an expression for the total ge- ometric margin, M = d1 + d2 + d3, as a function of w, c1, c2, and c3.
Answer:
The three data points x1, x2, and x3 satisfy the following equations:

wT x1 + b  =  ?c1
wT x2 + b  =  c2
wT x3 + b  =  c3




Figure 4.1. Linear classifier

Similarly, the corresponding three points on the decision boundary satisfy the following equations:

wT z1 + b
=
0
wT z2 + b
=
0
wT z3 + b
=
0

Subtracting the equations, we obtain:

wT (z1 ? x1) = c1 =? w d1 = c1 wT (x2 ? z2) = c2 =? w d2 = c2 wT (x3 ? z3) = c3 =? w d3 = c3
Putting them together, we obtain

M = c1 + c2 + c3 .
 w 

(b) If the decision boundary is shifted from wT x+b = 0 to wT x+br = 0, where b /= br, how does it affect the total geometric margin, M ? Answer:




wT (zr1 ? x1) + (br ? b) = c1 =? w d1 = c1 + (b ? br) wT (xr2 ? z2) + (b ? br) = c2 =? w d2 = c2 + (br ? b) wT (xr3 ? z3) + (b ? br) = c3 =? w d3 = c3 + (br ? b)
The new total geometric margin is

M = c1 + c2 + c3 + (br ? b) .
w

2. A linear classifier can be mathematically expressed as f (x) =  i wi?i(x), where each ?i(x) is a (possibly nonlinear) feature function of the original feature set x. The predicted class for a test instance x is determined as follows:
yˆ =	+1, if f (x) ≥ 0;
?1, otherwise.
For each binary classification data set described below, state whether it can be perfectly classified by a linear classifier by choosing appropriate feature functions. If the answer is yes, write the mathematical expression for the linear classifier f (x). Identify the feature functions ?i(x) as well as the parameters w in your expression.

(a) A data set with 4 continuous-valued features x1, x2, x3, and x4. The class label is +1 if the product of the x1 and x2 is greater than or equal to the product of x3 and x4; otherwise, it is ?1. Answer: Yes. The data can be perfectly classified as follows
f (x) = x1x2 ? x3x4 =  1  ?1   x1x2

By choosing w=[1 -1], ?1(x) = x1x2 and ?2(x) = x3x4, we can con- struct a linear classifier using the two-dimensional feature function ? = [?1 ?2].
(b) A data set with 4 continuous-valued features x1, x2, x3, and x4. The class label is +1 if at least one of the features is greater than 10; otherwise, it is ?1.


Answer: Yes. The data can be perfectly classified as follows

f (x) = sign(x1?10)+sign(x2?10)+sign(x3?10)+sign(x4?10)?0.5,
where sign(z) = 1 if z > 0 and 0 otherwise. By choosing w=[-0.5 1 1 1 1] and ?0 = 1, ?i(x) = sign(xi ? 10), we can construct a linear classifier to perfectly classify the data.
(c) A data set with 2 binary features, x1 and x2, whose class label y is determined as follows (this is similar to the exclusive OR binary operator using -1 instead of 0):

x1
x2
y
1
1
-1
1
-1
1
-1
1
1
-1
-1
-1
Answer: Yes. The data can be perfectly classified as follows

f (x) = ?x1x2
By choosing w=-1 and the feature function ?(x) = x1x2, we can construct a linear classifier to perfectly classify the data.

3. A linear classifier can be mathematically expressed as f (x) =  i wi?i(x), where each ?i(x) is a (possibly nonlinear) feature function of the original feature set x. The predicted class for a test instance x is determined as follows:
yˆ =	+1, if f (x) ≥ 0;
?1, otherwise.
For each binary classification data set described below, state whether it can be perfectly classified by a linear classifier by choosing appropriate feature functions. If the answer is yes, write the mathematical expression for the linear classifier f (x). Identify the feature functions ?i(x) as well as the parameters w in your expression.

(a) A data set with 4 continuous-valued features x1, x2, x3, and x4. The class label is +1 if the average of the first two features is greater than or equal to the average of the last two features; otherwise, it is ?1.


Answer: Yes. We can choose ?i(x) = xi, i.e., use the original features as feature function. The linear classifier can be written as
f (x) = x1 + x2 ? x3 ? x4
2

(b) A data set with 8 Boolean features, where each xi can take the value 0 or 1. The class label is +1 if there are more 1s than 0s; otherwise it is ?1.
Answer: Yes. We can choose ?i(x) = xi (?i ? {1, 8}) and ?9(x) =
1. The linear classifier can be written as

8
f (x) =	xi ? 4.5
i=1

The weights are equal to 1 for the first 8 feature functions and -4.5 for the last one.
(c) A data set with 8 Boolean features, where each xi can take the value 0 or 1. The class label is +1 if there are even number of 1s; otherwise it is ?1. (0 is considered an even number as well).
Answer: Yes. First, convert the Boolean features into {?1, +1} by setting 2xi ? 1. If there are even number of 1s, then the product 2xi ? 1 of all the features should be non-negative. Thus, we can
define the linear classifier as follows:

8
f (x) =	(2xi ? 1) = 256?0(x) ? 128?1(x) ? 128?2(x) + · · · + 1
i=1

We can define the following feature functions, ?i(x) = xI1 xI2 · · · xI8 ,
where each Ij ? {0, 1}. For example, ?0(x) = x1x2x3x4x5x6x7x8,
?1(x) = x1x2x3x4x5x6x7, and ?2(x) = x1x2x3x4x5x6x8.  The
weights correspond to the polynomial expansion of the product of (2xi ? 1).
(d) A data set with 2 continuous-valued features x1 and x2, where the range of possible values for each xi is between 0 and 1. The class labels are shown in the diagram below.
Answer: Yes, the linear classifier is given by

f (x) = ?(x1 ? 0.5)(x2 ? 0.4) = ?x1x2 + 0.4x1 + 0.5x2 ? 0.2,





where we choose ?1(x) = x1x2, ?2(x) = x1, ?3(x) = x2, and ?0(x) = 1 and w1 = ?1, w2 = 0.4, w3 = 0.5, and w0 = ?0.2. This is because for the data points that belong to the class y = ?1, then (x1 ? 0.5) and (x2 ? 0.4) must be both positive or both negative.
4. Consider the following loss function for a binary classification problem:
E(w) = ?(1 ? yiwT xi) s.t.  w 2 = 1,
i

where yi ? {?1, +1}. This is quite similar to the perceptron loss func- tion, except the sum is taken over all the training examples (instead of
summing over the misclassified training examples only).
Derive a closed-form solution for w that minimizes the constrained op- timization problem. Hint: see lecture 5 on how to solve a constrained optimization problem with the Lagrange multiplier method. Use the

constraint	w 2

= 1 to eliminate the Lagrange multiplier from your

solution for w.
Answer: First, the Lagrangian for the problem is

L =	(1 ? yiwT xi) + ?(wT w ? 1)
i


After taking its partial derivative with respect to w, we have


∂
∂w = ?	yixi
i


+ 2?w	=  0

=	w	=	 1  XT y	(4.1)
2?


where we have used a matrix notation on the second line. Furthermore


wT w =  1
4?2

yT XXT y = 1,


where we have applied the equality constraint wT w = 1. Thus, the Lagrange parameter is found to be

? = 1	yT XXT y
2

Replacing this back into (4.1), we have

XT y
w = √yT XXT y
5. A linear classifier in SVM can be mathematically expressed as f (x) =
   i wi?i(x), where each ?i(x) is a feature function of the original feature set x. Note that the transformed feature set {?i(x)} could be infinite- dimensional. The predicted class for a test instance x is determined as
follows:
yˆ =	+1, if f (x) ≥ 0;
?1, otherwise.
For each binary classification data set described below, state whether it can be perfectly classified by a linear classifier by choosing appropriate feature functions. Restrict the feature functions to polynomial expan- sions of the original attributes, e.g., ?1(x) = x1x2 or ?2(x) = x2x3x4, where x1, x2, x3, and x4 are part of the original attributes. If the answer is yes, write the mathematical expression for the linear classifier f (x). Identify the feature functions ?i(x) as well as the parameters w in your expression.


(a) A data set with 4 continuous-valued features x1, x2, x3, and x4. The class label is +1 if the product of the x1 and x2 is greater than or equal to the product of x3 and x4; otherwise, it is ?1. Answer: Yes, the data can be perfectly classified by choosing the following feature functions: ?1(x) = x1x2 and ?2(x) = x3x4.

f (x) = w1?1(x) + w2?2(x).
The weights of the feature functions are w1 = 1 and w2 = ?1.
(b) A data set with 4 continuous-valued features x1, x2, x3, and x4. The class label is +1 if at least one of the features is greater than 10; otherwise, it is ?1.
Answer:
Let I(·) be an indicator function, whose value is defined as follows:


I(x) =	1,  if x > 0;
0,  otherwise.

(4.2)


The classification problem can be solved by defining

f (x) = I(x1 ? 10) + I(x2 ? 10) + I(x3 ? 10) + I(x4 ? 10) ? 0.5,
where x = (x1, x2, x3, x4). However, our objective is to express f (x) as a linear combination of polynomial feature functions. To do this, we can approximate the indicator functions by sigmoid function:

1
I(x) ? ?(x) = 1 + e—?x
Figure 4.2 illustrates an example of the sigmoid function for differ- ent values of ?. As ? goes to ∞, the sigmoid function approaches the indicator function.
Furthermore, the sigmoid function can be expressed as a polynomial expansion using Maclaurin series, which is a special case of Tay- lor series centered at zero (see, for example, http://mathworld. wolfram.com/SigmoidFunction.html):

∞	n	∞
?(x) = 	1	 = ? (?1) En(0)(?x)n = 0.5 + ? J(n)xn,






Figure 4.2. Sigmoid function

where En(x) is an Euler polynomial. Thus, in principle, given a finite training set, we could find an infinite number of feature func- tions that perfectly classify the data by choosing the appropriate sigmoid function and its Maclaurin series to mimic the indicator functions. The feature functions are simply

2	2	2	2
{?i} = {1, x1, x2, x3, x4, x1, x2, x3, x4, · · · }
and their corresponding weights are

(1.5, J(1), J(1), J(1), J(1), J(2), J(2), J(2), J(2), · · · )
(c) A data set with 2 continuous-valued features x1 and x2. The class label is +1 if the exponential value of the difference between x1 ?x2 is greater than 100 (i.e., exp[x1 ? x2]); otherwise, it is ?1.
Answer: The classification problem can be solved by defining

                  f (x) = exp[x1 ? x2] ? 100 Since the Taylor series expansion for exp(·) is
exp(x) = ? x ,



we can define the following polynomial feature functions:

∞	n

f (x)  =

(x1 ? x2) 	100
n!

n=0
∞	∞	∞	∞
=	? J1(n)xn + ? J2(n)xn + ? ? J3(n, m)xnxm ? 99

Thus, we can perfectly classify the data by choosing the following feature functions

2	2 2
{1, x1, x2, x1x2, x1x2, x1x2, · · · }
with their corresponding weights given by

(?99, J1(1), J2(1), J3(1, 1), J3(1, 2), J3(2, 1), · · · )
(d) A data set with 2 binary features, x1 and x2, whose class label y is determined as follows (this is similar to the exclusive OR binary operator using -1 instead of 0):

x1
x2
y
1
1
-1
1
-1
1
-1
1
1
-1
-1
-1
Answer:
Yes. The data can be perfectly classified as follows

f (x) = ?x1x2
By choosing w = ?1 and the feature function ?(x) = x1x2, we can construct a linear classifier to perfectly classify the data.

4.2 Bayesian classifiers
1. Consider a training set with 3 features, X1, X2, and X3, for a binary classification problem. The class distribution is shown in the table below.


X1
X2
X3
Number of positive
examples
Number of negative
examples
1
1
1
20
8
1
0
0
20
17
0
1
0
5
8
0
0
0
5
17
(a) Based on the information above, determine whether X1 and X2 are independent of each other.
Answer: If X1 and X2 are independent, then P (X1, X2) = P (X1)P (X2) for ALL possible values of X1 and X2. From the training data:
20 + 8


whereas

p(X1 = 1, X2 = 1)  =


28 + 37

= 0.28
100

28 + 13

p(X1 = 1)p(X2 = 1) =

100	?

= 0.2665,
100

Since p(X1 = 1)p(X2 = 1) /= p(X1 = 1, X2 = 1), X1 and X2
are not independent of each other. Note: you only need to find a
counter-example to disprove the independence relationship.
(b) Determine whether X1 and X2 are conditionally independent of each other given the class.
Answer:
We can estimate the class conditional probabilities from the training data to obtain the following:

p(X1 = 1|+) = 0.8, p(X2 = 1|+) = 0.5, p(X1 = 0|+) = 0.2, p(X2 = 0|+) = 0.5
p(X1 = 1, X2 = 1|+) = 0.4, p(X1 = 1, X2 = 0|+) = 0.4, p(X1 = 0, X2 = 1|+) = 0.1, p(X1 = 0, X2 = 0|+) = 0.1 p(X1 = 1|?) = 0.5, p(X2 = 1|?) = 0.32,
p(X1 = 0|?) = 0.5, p(X2 = 0|?) = 0.68
p(X1 = 1, X2 = 1|?) = 0.16, p(X1 = 1, X2 = 0|?) = 0.34, p(X1 = 0, X2 = 1|+) = 0.16, p(X1 = 0, X2 = 0|+) = 0.34.
Since p(X1, X2|+) = p(X1|+)p(X2|+) and p(X1, X2|?) = p(X1|+)p(X2|?) for all X1 and X2, X1 and X2 are therefore conditionally indepen-
dent given the class. Note: it is insufficient to show it only for
X1 = 1 and X2 = 1 but not other values.


(c) Compute the class conditional probabilities P (X1 = 1|+), P (X1 = 1|?), P (X2 = 1|+), P (X2 = 1|?), P (X3 = 1|+), and P (X3 = 1|?).
Answer:
P (X1 = 1|+) = 0.8, P (X1 = 1|?) = 0.5, P (X2 = 1|+) = 0.5,
P (X2 = 1|?) = 0.32, P (X3 = 1|+) = 0.4, and P (X3 = 1|?) = 0.16
(d) Use the class conditional probabilities given in the previous question to predict the class label of each example with the feature set given in the training set above. Use your results to compute the training error rate of the na¨ıve Bayes classifier.
Answer: The na¨ıve Bayes classifier makes a prediction based on the following equation:

yˆ = arg max p(y X1, X2, X3)
y
=  arg max p(X1|y)p(X2|y)p(X3|y)p(y)

You need to determine the predicted class for all 4 cases of the training examples:
i. For X1 = X2 = X3 = 1,

p(X1 = 1|+)p(X2 = 1|+)p(X3 = 1|+)p(+) = 0.8?0.5?0.4?0.5 = 0.08
p(X1 = 1|?)p(X2 = 1|?)p(X3 = 1|?)p(?) = 0.5?0.32?0.16?0.5 = 0.0128
Hence, X1 = X2 = X3 = 1 will be labeled as +.
ii. For X1 = 1, X2 = X3 = 0,

p(X1 = 1|+)p(X2 = 0|+)p(X3 = 0|+)p(+) = 0.8?0.5?0.6?0.5 = 0.12
p(X1 = 1|?)p(X2 = 0|?)p(X3 = 0|?)p(?) = 0.5?0.68?0.84?0.5 = 0.1428
Hence, X1 = 1, X2 = X3 = 0 will be labeled as ?.
iii. For X1 = 0, X2 = 1, X3 = 0,

p(X1 = 0|+)p(X2 = 1|+)p(X3 = 0|+)p(+) = 0.2?0.5?0.6?0.5 = 0.03
p(X1 = 0|?)p(X2 = 1|?)p(X3 = 0|?)p(?) = 0.5?0.32?0.84?0.5 = 0.0672
Hence, X1 = 0, X2 = 1, X3 = 0 will be labeled as ?.


iv. For X1 = X2 = X3 = 0,

p(X1 = 0|+)p(X2 = 0|+)p(X3 = 0|+)p(+) = 0.2?0.5?0.6?0.5 = 0.03
p(X1 = 0|?)p(X2 = 0|?)p(X3 = 0|?)p(?) = 0.5?0.68?0.84?0.5 = 0.1428
Hence, X1 = X2 = X3 = 0 will be labeled as ?.
If you apply the classifier to the training examples given, 38 out of 100 examples will be misclassified. So the training error rate is 0.38.

2. Consider the directed acyclic graph shown in Figure 4.3. Determine whether each of the following independence or conditional independence assumptions are valid according to the constraints given by the graph. To receive full credit, make sure you show your steps clearly (to prove/disprove the assumptions).




Figure 4.3. A directed acyclic graph.



(a) D ? E
Answer: First you need to establish the relationship between
P (D, E) and P (D)P (E) in order to determine whether they are independent.


P (D, E)  =
A,B,C
=
A,B,C

P (A, B, C, D, E)

P (D|C)P (C|A)P (E|A, B)P (A)P (B)(4.3)


Furthermore, we can show that


P (D)  =
A,B,C,E
=
A,B,C,E

P (A, B, C, D, E)

P (D|C)P (C|A)P (E|A, B)P (A)P (B)

=
A,B,C

P (D|C)P (C|A)P (A)P (B)

=	P (D|C)P (C|A)P (A)
A,C
= ? P (D|C)P (C) where ? P (C|A)P (A) = P (C)


and



P (E)  =
A,B,C,D
=
A,B,C,D

P   (A,   B,   C,   D,   E) P (D|C)P (C|A)P (E|A, B)P (A)P (B)

=
A,B,C

P (C|A)P (E|A, B)P (A)P (B)

=	P (E|A, B)P (A)P (B)
A,B

Therefore
P (D)P (E)  =  ? P (D|C)P (C)  ? P (E|A, B)P (A)P (B) 
	
=	P (D|C)P (C)P (E|A, B)P (A)P (B)	(4.4)
ABC

If we compare against P (D, E) given in Equation (4.3), P (C|A) must be equal to P (C) in order for D and E to be independent.
Clearly this contradicts with the directed acyclic graph since there is a direct link from node A to C. Thus, P (D, E) /= P (D)P (E), which means D and E are not independent of each other.
(b) C ? E | A, B


Answer:


P (C, E|A, B)  =
=

=

P (A, B, C, E) P (A, B)
D P (A, B, C, D, E)

C,D,E P (A, B, C, D, E)
D P (D|C)P (C|A)P (E|A, B)P (A)P (B)
C,D,E P (D|C)P (C|A)P (E|A, B)P (A)P (B)
	P (C|A)P (E|A, B)P (A)P (B)	

=
C,E

P (C|A)P (E|A, B)P (A)P (B)

=	P (C|A)P (E|A, B)P (A)P (B)
P (A)P (B)
=  P (C|A)P (E|A, B)
Furthermore it can be shown that


P (C|A, B)  =
=

=

=

P (A, B, C)


P (A, B)
D,E P (A, B, C, D, E)


C,D,E P (A, B, C, D, E)
D,E P (D|C)P (C|A)P (E|A, B)P (A)P (B)
C,D,E P (D|C)P (C|A)P (E|A, B)P (A)P (B)
E P (C|A)P (E|A, B)P (A)P (B)
C,E P (C|A)P (E|A, B)P (A)P (B)

=	P (C|A)P (A)P (B)
P (A)P (B)
=  P (C|A)
Since P (C, E|A, B) = P (C|A, B)P (E|A, B), C and E are condi- tionally independent given A and B.
(c) C ? E | D



Answer:

P (C, E|D)  =


P (C, D, E) P (D)

=	A,B P (A, B, C, D, E)
P (D)
=	A,B P (D|C)P (C|A)P (E|A, B)P (A)P (B)
P (D)
=	P (D|C)	A,B P (C|A)P (E|A, B)P (A)P (B)
P (D)
=	P (C|D)	A,B P (C|A)P (E|A, B)P (A)P (B)
P (C)
=  P (C D)	A,B P (C|A)P (E|A, B)P (A)P (B)
P (C)
=  P (C D)	A,B P (A|C)P (E|A, B)P (A)P (B)
P (A)
=  P (C|D)	P (A|C)P (E|A, B)P (B)	(4.5)
A,B

where we have used the following equations (from Bayes theorem)
P (D|C) = P (C|D) ,	P (C|A) = P (A|C) .

P (D)

P (C)

P (C)

P (A)

If C and E are conditionally independent given D, then the term in parenthesis on the right-hand side of the equation must be equal to P (E|D). However,
P (D, E)

P (E|D)  =



P (D)

=	A,B,C P (A, B, C, D, E)
P (D)
=	A,B,C P (D|C)P (C|A)P (E|A, B)P (A)P (B)
P (D)
=	A,B,C P (C|D)P (A|C)P (E|A, B)P (A)P (B)
P (A)
= ? P (C|D) ? P (A|C)P (E|A, B)P (B)  (4.6)


Comparing Equation (4.6) against Equation (4.5), it is clear that if the parenthesis term on the right side of Equation (4.5) is equal to P (E|D) then

? P (A|C)P (E|A, B)P (B) = ?


P (C|D)P (A|C)P (E|A, B)P (B),


which is not always true. Therefore, since we cannot conclude that P (C, E|D) = P (C|D)P (E|D), C and E are not conditionally inde- pendent given D.

3. Consider the following training set, which contains 3 binary attributes X1, X2, and X3. There are 50 examples in the training set, with equal number of positive and negative examples.

X1
X2
X3
Number of positive
training examples
Number of negative
training examples
1
1
1
5
0
1
0
1
10
10
0
0
1
5
5
0
1
1
0
10
0
0
0
5
0

(a) Compute the class conditional probabilities P (X1 = 1|+), P (X1 = 1|?), P (X2 = 1|+), P (X2 = 1|?), P (X3 = 1|+), and P (X3 = 1|?). Solution:

P (X1 = 1|+) = (5 + 10)/25 = 0.6 P (X1 = 1|?) = (0 + 10)/25 = 0.4 P (X2 = 1|+) = (5 + 0)/25 = 0.2 P (X2 = 1|?) = (0 + 10)/25 = 0.4
P (X3 = 1|+) = (5 + 10 + 5 + 0)/25 = 0.8
P (X3 = 1|?) = (10 + 5 + 10)/25 = 1

(b) Use the class conditional probabilities given in the previous question to predict the class label of each example with the feature set given in the training set above. Use your results to compute the training error rate of the na¨ıve Bayes classifier.


Solution:
We need to compare P (+|X) and P (?|X), and the class with bigger conditional probability is the predicted class.
P (+|X) = P (+|X , X , X ) = P (X1, X2, X3|+) ? P (+)

1	2	3

P (X1, X2, X3)

P (?|X) = P (?|X , X , X ) = P (X1, X2, X3|?) ? P (?)

1	2	3

P (X1, X2, X3)



and,


P (+) = P (?) = 25/50 = 0.5


The problem can be simplified to compare P (X1, X2, X3|+) against
P (X1, X2, X3|?).

P (X1 = 1, X2 = 1, X3 = 1|+)
=
0.6 ? 0.2 ? 0.8 = 0.096
P (X1 = 1, X2 = 1, X3 = 1|?)
=
0.4 ? 0.4 ? 1 = 0.16
P (X1 = 1, X2 = 0, X3 = 1|+)
=
0.6 ? 0.8 ? 0.8 = 0.384
P (X1 = 1, X2 = 0, X3 = 1|?)
=
0.4 ? 0.6 ? 1 = 0.24
P (X1 = 0, X2 = 0, X3 = 1|+)
=
0.4 ? 0.8 ? 0.8 = 0.256
P (X1 = 0, X2 = 0, X3 = 1|?)
=
0.6 ? 0.6 ? 1 = 0.36
P (X1 = 0, X2 = 1, X3 = 1|+)
=
0.4 ? 0.2 ? 0.8 = 0.064
P (X1 = 0, X2 = 1, X3 = 1|?)
=
0.6 ? 0.4 ? 1 = 0.24
P (X1 = 0, X2 = 0, X3 = 0|+)
=
0.4 ? 0.8 ? 0.2 = 0.064
P (X1 = 0, X2 = 0, X3 = 0|?)
=
0.6 ? 0.6 ? 0 = 0
Thus,
X = (1, 1, 1) is predicted as ”-”. 5 errors are made.
X = (1, 0, 1) is predicted as ”+”. 10 errors are made.
X = (0, 0, 1) is predicted as ”-”. 5 errors are made.
X = (0, 1, 1) is predicted as ”-”. 0 errors are made.
X = (0, 0, 0) is predicted as ”+”. 0 errors are made. So, error rate is (5 + 10 + 5)/50 = 0.4.

4. Consider the problem of predicting whether an online visitor will pur- chase a product on a website. A visitor may search for the product they





want to buy or reach the product page through casual browsing start- ing from the homepage. Some product pages have discounts to entice the visitor to make a purchase while others do not. Visitors may also look at the overall rating of the product before deciding whether to buy the product. Visitors who are ready to make a purchase are also given a choice whether to register as a member or remain as a guest to the website. The above figure shows a Bayesian network that captures the relationships among the various attributes. Use the Bayesian network to answer the following questions.

(a) Determine whether each of the following independence or condi- tional independence assumptions are valid according to the con- straints given by the graph shown in Figure ??.
i. Search ? Rating
Answer: Let S: search, R: rating, D: discount, P: purchase,
and G: register.




P (S, R)  =	P (S, R, D, P, G)
DPG
=	P (S)P (R)P (D|S)P (P|D, R)P (G|P )	(from the dag)
=  P (S)P (R) ? P (D|S) ? P (P|D, R) ? P (G|P )
=  P (S)P (R) ? P (D|S) ? P (P|D, R)
D	P
=  P (S)P (R)	P (D|S)
D
=  P (S)P (R),	(4.7)

where	G P (G|P ) =	P P (P|D, R) =	D P (D|S) = 1. Thus
Search is independent of Rating.
ii. Register ? Search | Discount, Rating
Answer:


P (S, G|D, R)  =

P (S, G, D, R) P (D, R)

=	P P (S, R, D, P, G)
P (D, R)
=	P P (S)P (R)P (D|S)P (P|R, D)P (G|P )
P (D, R)
=	P (S)P (R)P (D|S)	P P (P|R, D)P (G|P )
P (D, R)
=	P (S)P (R)P (D|S)	P (P R, D)P (G P ()4.8) P (D, R)
P


Furthermore,


P (S|D, R)  =

P (S, D, R)


P (D, R)

=	P,G P (S, R, D, P, G)
P (D, R)
=	P,G P (S)P (R)P (D|S)P (P|R, D)P (G|P )
P (D, R)
=	P P (S)P (R)P (D|S)P (P|R, D)
P (D, R)

=	P (S)P (R)P (D|S)
P (D, R)

(4.9)


and


P (G|D, R)  =

P (G, D, R)


P (D, R)

=	P,S P (S, R, D, P, G)
P (D, R)
=	P,S P (S)P (R)P (D|S)P (P|R, D)P (G|P )
P (D, R)
=	S P (S)P (R)P (D|S)	P (P R, D)P (G P )
P (D, R)
=  ? P (S|D, R) ? ? P (P|R, D)P (G|P )	(based on Equation (4.9))
=  1 ? ? P (P|R, D)P (G|P )
=	P (P|R, D)P (G|P )	(4.10)
P

Plugging in Equations (4.9) and (4.10) into (4.8), we obtain

P (S, G|D, R) = P (S|D, R) ? P (G|D, R)
Therefore Register is conditionally independent of Search given Rating and Discount.


(b) Use the Bayesian network to predict whether a visitor who search for a product with an overall positive rating is likely to buy the product at the web site.
Answer: For this question, you need to compare P (P = yes|R =
+) against P (P = no|R = +).


P (P = yes|R = +)
P (P = no|R = +)

=	P (P = yes, R = +)
P (P = no, R = +)

S,G,D P (P = yes, R = +, S, G, D)
=
S,G,D P (P = no, R = +, S, G, D)
=	S,G,D P (S)P (R = +)P (D|S)P (P = yes|R = +, D)P (G|P = yes)
S,G,D P (S)P (R = +)P (D|S)P (P = no|R = +, D)P (G|P = no)
=	S,D P (S)P (R = +)P (D|S)P (P = yes|R = +, D)
S,D P (S)P (R = +)P (D|S)P (P = no|R = +, D)
=	S,D P (S)P (D|S)P (P = yes|R = +, D)
S,D P (S)P (D|S)P (P = no|R = +, D)


where,


P (S)P (D|S)P (P = yes|R = +, D)
S,D

= P (S = yes)P (D = yes|S = yes)P (P = yes|R = +, D = yes) + P (S = no)P (D = yes|S = no)P (P = yes|R = +, D = yes) + P (S = yes)P (D = no|S = yes)P (P = yes|R = +, D = no) + P (S = no)P (D = no|S = no)P (P = yes|R = +, D = no)
= 0.3 ? 0.7 ? 0.7 + 0.7 ? 0.2 ? 0.7 + 0.3 ? 0.3 ? 0.3 + 0.7 ? 0.8 ? 0.3
= 0.44
P (S)P (D|S)P (P = no|R = +, D)
S,D
= P (S = yes)P (D = yes|S = yes)P (P = no|R = +, D = yes) + P (S = no)P (D = yes|S = no)P (P = no|R = +, D = yes) + P (S = yes)P (D = no|S = yes)P (P = no|R = +, D = no) + P (S = no)P (D = no|S = no)P (P = no|R = +, D = no)
= 0.3 ? 0.7 ? 0.3 + 0.7 ? 0.2 ? 0.3 + 0.3 ? 0.3 ? 0.7 + 0.7 ? 0.8 ? 0.7
= 0.56


Thus,
P (P = yes|R = +)


=	P (P = yes, R = +) = 0.44 < 1

P (P = no|R = +)
So, the class should be no.

P (P = no, R = +)	0.56


5. An e-commerce company has launched an online marketing campaign using banner advertisements placed on various third-party web sites to drive visitors to the company’s web site. The web site has received 50,000 hits since the campaign was launched, out of which 10,000 of them were the result of click-throughs of the banner advertisement. Furthermore, 20% of the visitors directed to the Web site via the banner advertisement bought an item at the web site (compared to only 6% of the visitors arriving at the web site without clicking on a banner advertisement who ended up buying an item). Based on this information:
(a) Calculate the probability that a visitor will buy an item at the website (regardless of how the visitor arrives at the website).
Answer: Based on the information provided in the question, we have: P(Ad) = 10000/50000 = 0.2, P(Buy|Ad) = 0.2, and P(Buy|No Ad) = 0.06. Therefore,

P(Buy)  =  P(Buy, Ad) + P(Buy, No Ad)
=  P(Buy | Ad)P(Ad) + P(Buy | No Ad)P(No Ad)
=  0.2 ? 0.2 + 0.06 ? 0.8
=  0.088

(b) Determine whether the advertisement campaign is successful. To do this, you need to check the following: among the visitors who made a purchase at the web site, are they more likely to arrive at the web site via the banner advertisement or arrive without clicking on the banner? Show your calculations clearly.
Answer: We need to compare P(Ad|Buy) against P(No Ad|Buy):


 P(Ad|Buy)  P(No Ad|Buy)

=			P(Buy|Ad) P(Ad)	 P(Buy|No Ad) P(No Ad)
=	 0.2 ? 0.2
0.06	0.8
=  0.833


Since the ratio is less than 1, the advertisement is not successful.
6. Consider the following training set for predicting whether there is traffic congestion in the morning on a highway for a particular day. There are 100 examples in the training set, with 40% positive (congestion) and 60% negative (no congestion) examples.



Accident
Weather
Construction
Number of positive
training examples
Number of negative
training examples
no
good
no
5
30
no
good
yes
10
20
yes
good
no
10
5
yes
bad
no
10
5
yes
bad
yes
5
0

(a) Compute the class conditional probabilities P(accident=yes|+), P(accident=yes|?), P(weather=good|+), P(weather=good|?), P(construction=yes|+),
and P(construction=yes|?).
Answer:
P(accident=yes|+) = 25/40, P(accident=yes|?) = 10/60, P(weather=good|+)
= 25/40, P(weather=good|?) = 55/60, P(construction=yes|+) = 15/40, P(construction=yes|?) = 20/60.
(b) Use the class conditional probabilities given in the previous ques- tion to predict the class label of a test example with the following feature set: (Accident = no, weather = bad, construction = yes) by applying the na¨ıve Bayes classifier.
Answer: Let X = (Accident = no, weather = bad, construction
= yes) be the test example. We need to compare P (+|X) against
P (?|X), where the positive class represents congestion.


P (+|X)
P (?|X)

=	P (X|+)P (+)
P (X|?)P (?)
=	P(Accident=no|+)P(weather=bad|+)P(construction=yes|+)P(+) P(Accident=no|-)P(weather=bad|-)P(construction=yes|-)P(-)
=	15/40 ? 15/40 ? 15/40 ? 40/100 50/60 ? 5/60 ? 20/60 ? 60/100
=  1.5188


Since the ratio is greater than 1, P (+|X) > P (?|X). Thus, it is classified as congestion.

7. Consider the problem of predicting whether there will be traffic conges- tion on a specific highway using the Bayesian network shown in Figure
4.4. Use the Bayesian network to answer the following questions.



Figure 4.4. Bayesian network for question 7.

(a) Determine whether each of the following independence or condi- tional independence conditions hold according to the Bayesian net- work. To receive full credit, you must show your steps clearly (to prove/disprove the assumptions).
i. Accident ? Weather
Answer: Let A: Accident, W : weather, R: road condition,
C: construction, y: congestion. We need to check whether
P (A, W ) = P (A)P (W ).


P (A, W )  =	P (A, W, R, C, y)	(from sum rule)
R,C,y
=	P (y|A, C)P (C)P (A|R)P (R|W )P (W )	(from DAG)
R,C,y
=	P (C)P (A|R)P (R|W )P (W )
R,C
=	P (A|R)P (R|W )P (W )
R


This is equal to P (A)P (W ) only if  R P (A|R)P (R|W ) = P (A). Since this is not necessarily true (due to the second term P (R|W )), we cannot conclude that P (A, W ) = P (A)P (W ) based on the directed acyclic graph. Thus, in general, Acci-
dent is not independent of Weather.
ii. Congestion ? Accident | Weather, Construction
Answer: We need to check whether P (y, A|W, C) = P (y|W, C)P (A|W, C).
P (y, A, W, C)

P (y, A|W, C)  =
=

=





P (W, C)
R P (y, A, W, C, R)
y,A,R P (y, A, W, C, R)
R P (y|A, C)P (C)P (A|R)P (R|W )P (W )
y,A,R P (y|A, C)P (C)P (A|R)P (R|W )P (W )
R P (y|A, C)P (C)P (A|R)P (R|W )P (W )

?R P (y|A, C)P (C)P (A|R)P (R|W )P (W )

=	P (y|A, C)P (C)P (W )	R P (A|R)P (R|W )
P (C)P (W )
=  P (y|A, C)	P (A|R)P (R|W )	(4.11)
R
where w?e have used the fact that ?y P (y|A, C) = 1, ?A P (A|R) =


Furthermore, for the right-hand side:


P (y|W, C)  =
=

=

P (y, W, C)


P (W, C)
A,R P (y, A, W, C, R)


y,A,R P (y, A, W, C, R)
A,R P (y|A, C)P (C)P (A|R)P (R|W )P (W )
y,A,R P (y|A, C)P (C)P (A|R)P (R|W )P (W )

=	A,R P (y|A, C)P (C)P (A|R)P (R|W )P (W )
P (C)P (W )
=	P (y|A, C)P (A|R)P (R|W )	(4.12)
A,R
P (A, W, C)

P (A|W, C)  =
=

=



P (W, C)
y,R P (y, A, W, C, R)


y,A,R P (y, A, W, C, R)
y,R P (y|A, C)P (C)P (A|R)P (R|W )P (W )
y,A,R P (y|A, C)P (C)P (A|R)P (R|W )P (W )

=	P (C)P (W )	R P (A|R)P (R|W )
P (C)P (W )
=	P (A|R)P (R|W )	(4.13)
R

Plugging Equation (4.13) into (4.11), we have:

P (y, A|W, C) = P (y|A, C)P (A|W, C)
However, from Equation (4.12), since P (y|W, C) /= P (y|A, C),
P (y, A|W, C) /= P (y|W, C)P (A|W, C),
which means, y and A are not conditionally independent given
W and C.
(b) Suppose the weather is bad on a particular day and there is ongoing construction on the highway. Predict whether the highway will most likely be congested or not.


Answer: We need to compare P (y = yes|W = bad, C = yes) against P (y = no|W = bad, C = yes):
P (y = yes|W = bad, C = yes)
P (y = no|W = bad, C = yes)
=	P (y = yes, W = bad, C = yes)/P (W = bad, C = yes) P (y = no, W = bad, C = yes)/P (W = bad, C = yes)
A,R P (y = yes, W = bad, C = yes, A, R)
=
A,R P (y = no, W = bad, C = yes, A, R)
=	A,R P (y = yes|A, C = yes)P (A|R)P (R|W = bad)P (C = yes)P (W = bad)
A,R P (y = no|A, C = yes)P (A|R)P (R|W = bad)P (C = yes)P (W = bad)
=	A,R P (y = yes|A, C = yes)P (A|R)P (R|W = bad)
A,R P (y = no|A, C = yes)P (A|R)P (R|W = bad)

For the numerator term:

P (y = yes|A, C = yes)P (A|R)P (R|W = bad)
A,R
=  P (y = yes|A = yes, C = yes)P (A = yes|R = bad)P (R = bad|W = bad)
+P (y = yes|A = yes, C = yes)P (A = yes|R = good)P (R = good|W = bad)
+P (y = yes|A = no, C = yes)P (A = no|R = bad)P (R = bad|W = bad)
+P (y = yes|A = no, C = yes)P (A = no|R = good)P (R = good|W = bad)
=  0.9 ? 0.4 ? 0.7 + 0.9 ? 0.05 ? 0.3 + 0.4 ? 0.6 ? 0.7 + 0.4 ? 0.95 ? 0.3
=  0.5475

For the denominator term:

P (y = no|A, C = yes)P (A|R)P (R|W = bad)
A,R
=  P (y = no|A = yes, C = yes)P (A = yes|R = bad)P (R = bad|W = bad)
+P (y = no|A = yes, C = yes)P (A = yes|R = good)P (R = good|W = bad)
+P (y = no|A = no, C = yes)P (A = no|R = bad)P (R = bad|W = bad)
+P (y = no|A = no, C = yes)P (A = no|R = good)P (R = good|W = bad)
=  0.1 ? 0.4 ? 0.7 + 0.1 ? 0.05 ? 0.3 + 0.6 ? 0.6 ? 0.7 + 0.6 ? 0.95 ? 0.3
=  0.4525

Since the numerator is larger than the denominator, we classify the highway as likely to be congested.


4.3 Ensemble Classifier
1. Consider the following loss function for the AdaBoost classifier:


1
LAdaBoost = N

?i=1


exp
— 
yif (xi) ,


where N is the number of training examples, yi ? {?1, +1} is the true class label, and f (xi) ? {?1, +1} is the predicted class label.
(a) The average misclassification error on training data is defined as:


1
Lerror = N

?i=1 I

yi /= f (xi) ,


where I[·] is an indicator function whose value is equal to 1 if its argument is true and 0 otherwise. Compare LAdaBoost against Lerror and find an inequality relation between them, i.e., is it possible to
write one of them as greater than or equal to the other. Make sure you state your reason clearly.
Answer:


Lerror	=



1
N	I
i=1
1 ?


 

yi /= f (xi)




If yif (xi) < 0, then I[yif (xi) < 0] = 1 but exp[yif (xi)] > 1. Con- versely, if yif (xi) > 0, then I[yif (xi) < 0] = 1 but exp[yif (xi)] > 0. Therefore


1
Lerror = N

?i=1 I


yif (xi) < 0

1
<
N
i=1


exp


yif (xi)

= LAdaBoost


(b) In AdaBoost, the partition function (i.e., normalization factor) in each boosting round j is given by



Zj =

?i=1


w(j) exp
— 
yi?jf (xi)


while the error in each round is given by



?j =

?i=1

w(j)I

yi /= ƒ (xi)


Write an expression for Zj in terms of ?j.
Answer:



Zj	=

?i=1


w(j) exp
— 
yi?jƒ (xi)


=
i:yi=f (xi)


(j)
i
i:yi/=f (xi)

w(j) exp ?j 


        = (1 — ?j) exp — ?j + ?j exp ?j	(4.14) where we have used the fact that

?j = ?

w(j)I

yi /= ƒ (xi)  =	?


(j)
i


and

i=1



?


(j)	?







(j)

i:yi



 

f (xi)

1 — ?j	=





i=1
?



wi	—
(j) 


i=1

wi I
 



yi /= ƒ (xi)
  



=
i:yi=f (xi)

(j)
i



Furthermore, it can be easily shown that ?j = 1 log 1—?j

by taking

2	?j
the partial derivative of Zj with respect to ?j and setting it to zero.


Plugging this into Equation (4.14), we have:
Z	= (1 — ? ) exp  — 1 log 1 — ?j  + ? exp 1 log 1 — ?j 

= (1 — ?j)


exp


log

1 — ?j
?j

1

2
+ ?j


exp


log

1 — ?j
?j

  1



= (1 — ?j)

1 — ?j
?j

1

2
+ ?j

1


1 — ?j	2
?j

= (1 — ? )r	?j

+ ? s 1 — ?j

1 — ?j	?j
=  2q?j(1 — ?j)
(c) Consider the following 8 weighted training examples along with their true and predicted class labels after performing j rounds of boosting iterations:
Data point i
1
2
3
4
5
6
7
8
Weight, w(j)
i
yi
ƒ (xi)
0.35
-1
+1
0.2
+1
+1
0.1
+1
+1
0.05
+1
+1
0.05
+1
-1
0.05
-1
-1
0.1
-1
-1
0.1
-1
-1
i. Based on the given information, calculate ?j and ?j. Answer: In the example, only data points 1 and 5 are mis- classified by ƒ (x). Thus


?j = 0.35 + 0.05 = 0.40, ?j

1
=		loge 2

1 — ?j ?j

1
=	log 2

0.6
e 0.4 = 0.2027


ii. Show the new weights for each of the 8 training examples,
w(j+1).
Answer:

i
1
2
3
4
5
6
7
8
w(j)
i
yi
0.35
-1
0.2
+1
0.1
+1
0.05
+1
0.05
+1
0.05
-1
0.1
-1
0.1
-1
ƒ (xi)
w(j+1)
i
+1
0.4375
+1
0.1667
+1
0.0833
+1
0.0417
-1
0.0625
-1
0.0417
-1
0.0833
-1
0.0833




5




Association Analysis

5.1 Association Rules and Frequent Pattern Mining
1. Consider the following set of candidate 3-itemsets:
{a, b, c}, {a, b, d}, {a, b, e}, {a, c, d}, {a, c, e}, {a, c, ƒ}, {a, d, ƒ},
{b, c, d}, {b, c, e}, {b, d, ƒ}, {c, d, e}, {c, d, ƒ}.
(a) Construct a hash tree for storing the above 3-itemsets.

L2
L3	L4

Figure 5.1. Hash tree.


(b) Suppose all the candidate 3-itemsets above are frequent. List all candidate 4-itemsets that can be generated from the frequent 3- itemsets using the candidate generation procedure for Apriori.


Answer: {abcd}, {abce}, {abde}, {acde}, {acdf}, {acef}, {bcde},
{cdef}.
(c) List all candidate 4-itemsets that survive the candidate pruning step of the Apriori algorithm.
Answer: {abcd}, {abce}, {acdf}. The rest of the itemsets are pruned because they contain at least one infrequent 3-itemsets.
(d) Based on the list of frequent 3-itemsets given above, is it possible to generate a frequent 5-itemset? State your reason clearly. Answer: No. A frequent 5-itemset must contain at least 5 frequent 4-itemsets.
2. Consider a transaction dataset that contains five items, {A, B, C, D, E}.
(a) Suppose the support of {A, B} is the same as the support of {A, B, C}, which one of the following statements are true:
i. Support of {A, B} is the same as support of {A, C}.
Answer: False. As a counter-example, let {t1, t2, t3} be trans- actions that contain {A, B} and {A, B, C} but {t1, t2, t3, t4} are transactions that contain {A, C}.
ii. The confidence of the rule {A, B} ? {C} is 100%.
Answer: True because confidence of the rule is given by the
ratio of support between {A, B, C} and {A, B}.
iii. The support of {A, B, D} is the same as the support of {B, C, D}. Answer: False. As a counter-example, let {t1, t2, t3} be trans- actions that contain both {A, B} and {A, B, C}, whereas {t1, t2, t3, t4} be transactions that contain {B, C}.	If D is contained in
{t1, t2, t4, t5}, then support count of {A, B, D} is 2 whereas support count of {B, C, D} is 3.
iv. {A, B, D} is not a closed itemset.
Answer: True because support of {A, B, D} must be the same as support of {A, B, C, D}.
(b) Suppose the support of {A, B} is the same as the support of {B, C}, which one of the following statements are true:
i. All transactions that contain {A, B} also contain {B, C}. Answer: False. As a counter-example, let {t1, t2, t3, t4} be transactions that contain B, {t1, t2} be transactions that con- tain A and {t3, t4} be transactions that contain C.


ii. The confidence of the rule {A, B} ? {C} is 100%.
Answer: False. As a counter-example, let {t1, t2, t3, t4} be transactions that contain B, {t1, t2} be transactions that con- tain A and {t3, t4} be transactions that contain C. In this case, support of {A, B, C} is 0, so confidence of the rule is also 0.
iii. The support of {A, B, D} is the same as the support of {B, C, D}. Answer: False. As a counter-example, let {t1, t2, t3, t4} be transactions that contain B, {t1, t2} be transactions that con- tain A and {t3, t4} be transactions that contain C. Also, let
{t4, t5} be transactions that contain D. In this case, support count of {A, B, D} = 0 whereas support count of {B, C, D} is 1.
iv. {A, B, D} is not a closed itemset.
Answer: False. There is no guarantee that support of {A, B, D}
is identical to at least one of its supersets.
(c) Suppose all the transactions that contain {A, B} also contain {B, C}, which one of the following statements are true:
i. The confidence of the rule {A, B} ? {C} is 100%.
Answer: True because s({A, B, C}) = s({A, B}).
ii. The support of {A, B, D} is the same as the support of {B, C, D}. Answer: False because some transactions that contain {B, C} may not necessarily contain {A, B}. If all the transactions that contain {B, C} also contain D, then the support for {A, B, D} may not be the same as support of {B, C, D}.
iii. {A, B, D} is not a closed itemset.
Answer: True because support of {A, B, D} is the same as the support of {A, B, C, D}.
(d) Suppose the rules {A, B} ? C has the same confidence as {A, B} ?
D, which one of the following statements are true:
i. The confidence of the {A, B} ? {C, D} is the same as the confidence of {A, B} ? {C}.
Answer: False.	Since the rules have identical confidence, therefore support of {A, B, C} is the same as support of {A, B, D} but the transactions that contain {A, B, C} may not be the
same as those that contain {A, B, D}.
ii. All transactions that contain {A,B,C} also contain {A,B,D}. Answer: False.	Since the rules have identical confidence, therefore support of {A, B, C} is the same as support of {A, B, D}


but the transactions that contain {A, B, C} may not be the same as those that contain {A, B, D}.
iii. {A, B, C} is not a closed itemset.
Answer: False. Although support of {A, B, C} is the same as support of {A, B, D}, there is no guarantee it is the same as support of {A, B, C, D} since {A, B, C} and {A, B, D} might be contained in different transactions.
(e) Suppose we are interested to find all the closed itemsets in a given data set. For each of the following scenarios, list all the itemsets that are guaranteed to be not closed when:
i. Support of {B, C} is equal to support of {A, B, C}.
Answer: {B, C}, {B, C, D}, {B, C, E}, and {B, C, D, E}.
ii. All the transactions that contain {A, D} is a subset of transac- tions that contain {C}.
Answer: {A, D}, {A, B, D}, {A, D, E}, and {A, B, D, E}.
3. (a) What is the implication of setting your minimum confidence thresh- old lower than minimum support threshold?
Answer: Note that confidence of a rule X ? Y cannot be less than its support, i.e., P (X, Y )/P (X) ≥ P (X, Y ), since P (X) ? (0, 1]. As a result, all the rules derived from the frequent itemsets (i.e., whose
support is greater than minimum support threshold) will pass the minimum confidence threshold, which means there is no confidence
pruning. Furthermore, many of the rules of the form X ? Y will have confidence values lower than their corresponding support of
Y . Such rules tend to be spurious because they involve negatively correlated itemsets (see the tea-coffee example from the book).
(b) Consider an association rule X ? Y , where X and Y are itemsets. Let P (X) denote the support of itemset X and P (X, Y ) denote the support of X ? Y . Consider the following measure, which is known as the ?-coefficient:

	P (X, Y ) 	P (X)P (Y )	
?(X ? Y ) = √P (X)P (Y )(1 — P (X))(1 — P (Y ))
i. What is the ?-coefficient of association rules whose itemsets X
and Y are independent.
Answer: When X and Y are independent, P (X, Y ) = P (X)P (Y ). Therefore ?(X ? Y ) = 0.


ii. Is the measure monotone, anti-monotone, or non-monotone when the size of itemset X (i.e., left-hand side of the rule) is increased? In other words, when the set X = {a, b} becomes
Xr = {a, b, c}, will the measure be non-decreasing (monotone),
non-increasing (anti-monotone), or neither (non-monotone).
Answer: Non-monotone.
iii. Derive an expression for the upper bound of ?, called ?max, which is a function defined in terms of P (X) and P (Y ) only, i.e.:

?(X ? Y ) ≤ ?max(X, Y ),  where ?max(X, Y ) = ƒ (P (X), P (Y ))
Show that ?max is anti-monotone when the size of X increases (e.g., from X = {a, b} to X’={a, b, c}).
Answer:
Since P (X, Y ) ≤ min(P (X), P (Y )), therefore:
  min(P (X), P (Y )) — P (X)P (Y )	
?  ≤	√P (X)P (Y )(1 — P (X))(1 — P (Y ))

=	P (Y )(1—P (X))
P (Y )(1—P (X)) ,  otherwise.
P (X)(1—P (Y ))
? ?max	(5.1)
If P (X) ≤ P (Y ), then increasing the size of X will only make
P (X) becomes smaller. So, ?max is anti-monotone.
If P (X) > P (Y ), then increasing the size of X may increase
?max as long as P (Xr) > P (Y ).
4. Consider the following set of frequent 2-itemsets:
{p, q}, {p, r}, {p, s}, {p, t}, {q, r}, {q, t}, {r, s}, {s, t}
(a) List all the candidate 3-itemsets produced during the candidate generation step of the Apriori algorithm.
Answer: {p,q,r}, {p,q,s}, {p,q,t}, {p,r,s}, {p,r,t}, {p,s,t}, {q,r,t}
(b) List all the candidate 3-itemsets that survive the pruning step of
the Apriori algorithm.
Answer: {p,q,r}, {p,q,t}, {p,r,s}, {p,s,t}


(c) Based on the list of candidate 3-itemsets given above, is it possi- ble to generate at least one frequent 4-itemset? State your reason clearly.
Answer: No because there are no viable candidate 4-itemset whose subsets of size-3 are all frequent. For example, even if all the can-
didate 3-itemsets in the previous question are frequent, the only candidate 4-itemset generated is {p,q,r,t}. Because {q,r,t} is not frequent, this candidate will be pruned.

5. Consider the transactions shown in the table below:

TID
Items
1
2
3
4
5
6
7
8
9
10
{a,b,c,d}
{a,c}
{b,c,e}
{b,d,e}
{a,c,d}
{a,b,d}
{b,e}
{a,b,c,e}
{a,b,c}
{a,b,d,e}

(a) Using the information given in the table above, draw a lattice struc- ture of the itemsets. Label each node in the lattice with the follow- ing letter(s):
• N: If the itemset is not generated as a candidate itemset by the
Apriori algorithm (using the candidate generation procedure
described in class).
• P: If the itemset is generated as a candidate itemset by the
Apriori algorithm but was subsequently pruned during the can-
didate pruning step.
• I: If the itemset is generated as a candidate itemset by the
Apriori algorithm, survives the candidate pruning step, but
fails the minsup threshold after determining its support from the transaction database.
• F: If the itemset is generated as a frequent itemset.
• C: If the itemset is closed.
• M: If the itemset is maximal frequent.


Assume that minsup = 3, which means an itemset must appear in at least 3 transactions to be frequent. A node can have more than one label. For example, if an itemset is maximal frequent, its corresponding node should have 3 labels, F,C,M because all maximal frequent itemsets are closed. A node can be closed but not frequent if its support is different than the support for all of its parents. You do not have to label the null node.
Answer: See Figure 5.7.


Figure 5.2. Search space for transaction database that contains 5 items.


(b) Calculate the pruning ratio of the Apriori algorithm when applied to the given dataset. Pruning ratio is defined as the percentage of itemsets not considered to be a candidate because (1) they are not generated during candidate generation or (2) they are pruned during the candidate pruning step. In other words, since there are


31 possible itemsets (excluding null) in the lattice, pruning ratio is given by

Pruning ratio =

#N + #P
,
31

where #N and #P are the number of nodes labeled as N and P in your lattice structure.
Answer: Pruning ratio = 14 .
6. Consider the following set of candidate 3-itemsets:
{p, q, r}, {p, q, s}, {p, q, t}, {p, r, s}, {p, r, t}, {q, r, s}, {q, r, t},
{q, s, t}, {r, s, t}.
(a) Construct a binary hash tree for storing the above 3-itemsets. As- sume the hash tree uses a hash function where items p, r, t are hashed to the left child of a node, while items q, s, u are hashed to the right child. A candidate k-itemset is inserted into the tree by hashing on each successive item in the candidate and then fol- lowing the appropriate branch of the tree according to the hash value. Once a leaf node is reached, the candidate is inserted based on one of the following conditions:
Condition 1: If the depth of the leaf node is equal to k (the root node is assumed to be at depth 0), then the candidate is added to the leaf node irrespective of the number of itemsets already stored at the node.
Condition 2: If the depth of the leaf node is less than k, then the candidate is added to the leaf node as long as the number of itemesets already stored at the leaf node is less than maxsize =
2. Otherwise, change the leaf node into an internal node and distribute the candidates (including the new candidate to be added) to its children based on their respective hash values.
Answer: See Figure 5.3.
(b) Suppose all the candidate 3-itemsets above are frequent. List all candidate 4-itemsets that can be generated from the frequent 3- itemsets using the candidate generation procedure for Apriori. Answer: {p,q,r,s}, {p,q,r,t}, {p,q,s,t}, {p,r,s,t}, {q,r,s,t}
(c) List all candidate 4-itemsets that survive the candidate pruning
step of the Apriori algorithm.
Answer: {p,q,r,s}, {p,q,r,t}, {q,r,s,t}


prt	qsu




prt

 p r s  p r t
L1

qsu


prt





qsu



L3

prt


 q r s  q r t
L4

qsu

L5


L2

Figure 5.3. Hash tree for question 6.


(d) Based on the list of frequent 3-itemsets given above, is it possible to generate a frequent 5-itemset? State your reason clearly. Answer: No. Even if all the candidate 4-itemsets given in the previous question are frequent, you still will not be able to gen- erate any viable candidate 5-itemsets (since there are only 3 fre- quent 4-itemsets; any candidate 5-itemset must contain 5 frequent 4-itemsets).
7. Consider a transaction dataset that contains five items, {p, q, r, s, t}.
(a) Suppose the confidence of the rule {p, q} ? {s} is 100%, which of the following statements are true:
?i. All transactions that contain {p, q} also contain {p, s}.
ii. The support counts of {p, q}, {p, s}, and {q, s} are identical. i?ii. {p, q} is not a closed itemset.
iv. {p, q, s} is not a closed itemset.
(b) Suppose the support of {p, q} is identical to the support of {p, q, r}, which of the following statements are true:
i. Support of {p, q} is the same as support of {p, r}.
ii. The confidence of the rule {p, q} ? {r} is the same as the confidence of the rule {p, r} ? {q}.
iii. The support of {p, q, t} is the same as the support of {q, r, t}. i?v. {p, q, t} is not a closed itemset.


(c) Suppose all the transactions that contain {p, q} also contain {p, s}, which of the following statements are true:
?i. The confidence of the rule {p, q} ? {s} is 100%.
ii. The support of {p, q, t} is the same as the support of {p, s, t}.
iii. The confidence of the rule {p, q} ? {s} is the same as the confidence of {p, s} ? {q}
iv. {p, s, t} is not a closed itemset.
(d) Suppose the rules {p, q} ? {r} has the same confidence as {p, r} ?
{q}, which one of the following statements are true:
?i. The support of {p, q} is the same as the support of {p, r}.
ii. All transactions that contain {p,q,t} also contain {r}.
iii. {p, q} is not a closed itemset.
iv. {p, q, t} is not a closed itemset.
8. Consider the lattice structure shown below.



We are interested in finding all closed itemsets in a given data set. For each of the following scenarios, list all the itemsets that are guaranteed to be not closed when:
(a) Support of {A} is equal to support of {A, B, C}.


Answer: Any itemset X that contains {A} but not BOTH B and C are guaranteed to be not closed because the support of X is identical to the support of X ? {B,C}.  The answer to this question is:
{A}, {A,B}, {A,C}, {A,D}, {A,E}, {A,B,D}, {A,B,E}, {A,C,D},
{A,C,E}, {A,D,E}, {A,B,D,E}, {A,C,D,E}.
(b) All the transactions that contain {A} is a subset of transactions that contain {B}.
Answer: Any itemset X that contains {A} but not B are guaran- teed to be not closed because the support of X is identical to the support of X ? {B}. The answer to this question is: {A}, {A,C},
{A,D}, {A,E}, {A,C,D}, {A,C,E}, {A,D,E}, {A,C,D,E}.
9. Consider the following set of candidate 3-itemsets:
{p, q, r}, {p, q, s}, {p, q, t}, {p, r, s}, {p, r, t}, {p, r, u}, {p, s, t},
{q, r, s}, {q, r, t}, {q, r, u}, {q, s, t}, {r, s, t}, {s, t, u}
(a) Construct a hash tree for storing the above 3-itemsets. Assume the hash tree uses a hash function where items a, d are hashed to the left child of a node, items b, e are hashed to the middle child, while items c, ƒ are hashed to the right child. A candidate k-itemset is inserted into the tree by hashing on each successive item in the candidate and then following the appropriate branch of the tree according to the hash value. Once a leaf node is reached, the candidate is inserted based on one of the following conditions:
Condition 1: If the depth of the leaf node is equal to k (the root node is assumed to be at depth 0), then the candidate is added to the leaf node irrespective of the number of itemsets already stored at the node.
Condition 2: If the depth of the leaf node is less than k, then the candidate is added to the leaf node as long as the num- ber of itemsets stored at the leaf node is less than or equal to maxsize = 2. Otherwise, change the leaf node into an internal node and distribute the candidates (including the new candi- date to be added) to its children based on their respective hash values.
Answer:
The hash tree is shown in Figure 5.4





















b,c,d
b,c,e
b,c,f


L2	L3	L4	L±	L6	L7

L9	L10	L11


Figure 5.4. Hash tree for question 9(a).

(b) Consider a transaction that contains items {a,b,d,e,f}. Count the number of leaf nodes in the hash tree to which the transaction will
be hashed into.
Answer:
There are 5 nodes. L1, L2, L3, L4, and L8
(c) Suppose all the candidate 3-itemsets above are frequent. List all candidate 4-itemsets that can be generated from the frequent 3- itemsets using the candidate generation procedure for Apriori.
Answer:
{a,b,c,d}, {a,b,c,e}, {a,b,d,e}, {a,c,d,e}, {a,c,d,f}, {a,c,e,f}, {b,c,d,e},
{b,c,d,f}, {b,c,e,f}
(d) List all candidate 4-itemsets that survive the candidate pruning step of the Apriori algorithm.
Answer:


{a,b,c,d}, {a,b,c,e}, {a,b,d,e}, {a,c,d,e}, {b,c,d,e}
(e) Based on the list of frequent 3-itemsets given above, is it possible to generate a frequent 5-itemset? State your reason clearly. Answer:
The candidate 5-itemset is {a,b,c,d,e}. We can see all the subsets of
{a,b,c,d,e} are frequent. Hence, {a,b,c,d,e} is a frequent 5-itemset.
10. Consider a medical database ? that contains N “transactions” and d
“items”.

(a) Suppose we are interested in rules of the form X ? y, where X is a k-itemset (k ≥ 1) and y is an item (i.e., the right-hand side contains 1-itemsets only). These rules are known as discriminative rules. For
medical diagnosis, such rules can be used to identify segments of the population who are susceptible to certain diseases. An example of such a rule is {male, age>60, alcohol-abuse} ? {prostate-cancer}.
i. Count the maximum number of discriminative rules that can be extracted from the database ? (in terms of d)
Answer:
d(2d—1 — 1)
ii. Count the maximum number of discriminative rules that can
be extracted from a given frequent k-itemset X.
Answer:
k
iii. Suppose we are interested in finding discriminative rules whose support ≥ minsup and confidence ≥ minconf thresholds. Let X be a frequent k-itemset and X—1, X—2, and X—k be the
corresponding set of frequent (k — 1)-itemsets after removing one of the items from X. What minimum support the (k — 1)- itemsets must have in order to guarantee that the itemset X
will generate at least one discriminative rule with confidence
≥ minconƒ . Express your answer in terms of sX (support of
X) and minconƒ .
Answer:
The largest confidence of the discriminative rules generated

from X is


maxconƒ = max
i

 sX
sX?i


, i = 1, ...k


We want to find the minimum support that





Hence,

SX


minsup(Xi)


max
i
 
sX sX?i

≥ minconƒ

minsup		sX	
minconƒ
(b) Suppose we are interested in rules of the form y ? X, where X is a k-itemset (k ≥ 1) and y is an item (i.e., the left-hand side contains 1-itemsets only). These are also known as characteristic rules. Such
rules can be used to identify a disease based on its symptoms. An example of such a rule is {pneumonia} ? {cough, fever, shortness of breath}.
i. Count the maximum number of characteristic rules that can be extracted from the database ? (in terms of d).
Answer:

d(2d—1 — 1)
ii. Count the maximum number of characteristic rules that can be extracted from a given frequent k-itemset X.
Answer: k
iii. Suppose we are interested in finding characteristic rules whose support ≥ minsup and confidence ≥ minconf thresholds. De- sign an Apriori-like algorithm for extracting such rules. In ad-
dition to the regular support and confidence pruning, your al- gorithm should take advantage of the support ordering of the items to further prune the search space of candidate itemsets
and rules. Support ordering means the d items {x1, x2, · · · xd} are ordered in increasing support order, i.e., ?i : support(xi) ≤ support(xi+1).
Hint: Find an expression for the upper bound confidence value for all characteristic rules that can be generated from a k- itemset X. Use the upper bound to prune candidate itemsets and candidate rules that are guaranteed to be of low confidence. Answer:
The upper bound of confidence value for an ordered d itemset
x1, x2, ..., xd is sX . We can design our algorithm as Algorithm
1
5.3:



Algorithm 5.1 Apriori

1: Fk: frequent k-itemsets
2: Lk: candidate k-itemsets
3: U (X): upper bound of confidence value for rules generated from itemset
X
4: R: Characteristic rule set
5: Let k = 1
6: Generate F1=frequent 1-itemsets:
7: repeat
8:	Candidate Generation: Generate Lk+1 from Fk
9:	Candidate Pruning:
10:	for each X in Lk+1 do 11:		if sX < minsup then 12:			Prune X
13:	else if U (X) < minconƒ then
14:	Prune X
15:	else
16:	Generate characteristic rule set Rk+1 from X
17:	Prune off the rules whose confidence is less than minconf
18:	R ? R + Rk+1
19:	end if
20:	end for
21: until Fk is empty
22: return R


11. Consider the lattice structure shown in Figure 5.5.
We are interested in finding all maximal and closed itemsets in a given data set.
(a) What is the minimum and maximum number of maximal frequent itemsets one can generate from such a database?
Answer: Minimum number is 0, maximum number is 10.
(b) What is the minimum and maximum number of closed frequent itemsets one can generate from such a database?
Answer: Minimum number is 0, maximum number is 25 — 1.
(c) If all the transactions that contain itemset {A, B} also contain items
C and E, list all the itemsets that are guaranteed to be not closed.
Answer:




Figure 5.5. Search space for transaction database that contains 5 items.

{A,B}, {A,B,C}, {A,B,E}, {A,B,D}, {A,B,C,D}, {A,B,D,E}
(d) If the support of itemsets {B} and {B, C, D} are identical, list all the itemsets that are guaranteed to be not closed.
Answer:
{B}, {B,C}, {B,D}, {A,B}, {B,E}, {A,B,E}, {A,B,C}, {B,C,E},
{A,B,C,E}, {A,B,D}, {B,D,E}, {A,B,D,E}
12. Consider the contingency tables shown below for 3 pairs of items: (bread, milk), (pepsi, coke), and (caviar, wine).



bread bread

pepsi pepsi

caviar caviar


(a) Rank the following six rules (in increasing magnitude): bread ? milk, milk ? bread, coke ? pepsi, pepsi ? coke, wine ? caviar, and caviar ? wine according to the following measures: support, confidence, interest, and odds ratio.
Answer:
Rank according to support: wine ? caviar, caviar ? wine, coke ?
pepsi, pepsi ? coke, bread ? milk, milk ? bread.
Rank according to confidence: coke ? pepsi, pepsi ? coke, bread
? milk, wine ? caviar, caviar ? wine, milk ? bread.


Rank according to interest: coke ? pepsi, pepsi ? coke, bread ?
milk, milk ? bread, wine ? caviar, caviar ? wine.
Rank according to odds ratio: coke ? pepsi, pepsi ? coke, bread
? milk, milk ? bread, wine ? caviar, caviar ? wine.
(b) One difficulty in comparing the rules is that each item has different support counts (i.e., marginal totals). One way to avoid this diffi- culty is to standardize the tables to have uniform marginal totals. The effect of table standardization is shown below:


A	??	A
A	A

One popular method to standardize a contingency table is to apply the iterative proportional fitting (IPF) algorithm. Given an initial contingency table, the algorithm would iteratively modify all the cell entries in the table until the desired row and column sums are obtained. For equal marginal totals, you can set ƒ1?+ = ƒ0?+ = ƒ+? 1 =
ƒ+? 0 = N/2.

Algorithm 5.2 Iterative Proportional Fitting
Input: f: 2 ? 2 contingency matrix
Output: ƒ : standardized contingency matrix

Set ƒ1?+ = ƒ0?+ = ƒ+? 1 = ƒ+? 0 = N/2
for k=1 to maxiter do
if k is odd then
Rescale each entry in ƒ by row sum: ƒ (k) ? ƒ (k—1) ? fi?+


ij	ij

(k 1)
i+

else
Rescale each entry in ƒ by column sum: ƒ
  end if end for return f


(k)
ij


(k 1)
ij

f+? j

(k 1)
+j



Apply the IPF procedure to all three contingency tables (you can download the Matlab script ipf.m from the class web site or write the code yourself). Show the values of the contingency tables after


standardization. Rank the three tables in increasing order of mag- nitude using support, confidence, interest factor, and odds ratio.
Answer:
The contingency tables after the IPF procedure are:



bread bread

pepsi pepsi

caviar caviar


Using new contingency tables, the orders are shown in the following: Rank according to support: coke ? pepsi, pepsi ? coke, bread ?
milk, milk ? bread, wine ? caviar, caviar ? wine.
Rank according to confidence: coke ? pepsi, pepsi ? coke, bread
? milk, milk ? bread, wine ? caviar, caviar ? wine.
Rank according to interest: coke ? pepsi, pepsi ? coke, bread ?
milk, milk ? bread, wine ? caviar, caviar ? wine.
Rank according to odds ratio: coke ? pepsi, pepsi ? coke, bread
? milk, milk ? bread, wine ? caviar, caviar ? wine.
13. A clique is a pattern that contains highly similar items. If an itemset X = {x1, x2, · · · xk} is a clique, then the presence of any item xi ? X in a transaction t implies the presence of all other items xj ? X — {xi}
in the transaction t with high probability. For example, if the words
{graph, node, link} form a clique pattern in a set of documents, then any document that contains the word graph is highly likely to contain the words node and link as well1. For this exercise, you need to design
an Apriori -like algorithm for efficient mining of such patterns.

(a) Which of the following evaluation measures M for a given itemset X = {x1, x2, · · · xk} is most appropriate to define clique patterns? An itemset X is considered to be a clique only if M (X) ≥ ? , where
? is a user-defined threshold. State your reason clearly.
• M (X) is the maximum support for one of the items in X, i.e.,
M (X) = maxxi?X s(xi), where s(xi) is the support of xi.
• M (X) is the minimum support for one of the items in X, i.e.,
M (X) = minxi?X s(xi), where s(xi) is the support of xi.

     1In this example, an item corresponds to a word and a transaction corresponds to a document.


• M (X) is the maximum confidence of all association rules ex- tracted from X, i.e., M (X) = maxL?X c(L ? X — L). For example, if X = {graph, node, link}, then M (X) is the maxi- mum confidence is computed from the rules {graph} ? {node, link},{node} ? {graph, link}, {node, graph} ? {link}, etc.
  • M (X) is the minimum confidence of all association rules ex- tracted from X, i.e., M (X) = minL?X c(L ? X — L). For example, if X = {graph, node, link}, then M (X) is the mini- mum confidence is computed from the rules {graph} ? {node, link},{node} ? {graph, link}, {node, graph} ? {link}, etc. Answer: M (X) = minL?X c(L ? X — L) is the most appropriate measure for clique patterns. The measure ensures that if a subset
of the itemset is present in an itemset X, the rest of the items in the itemset are likely to be present with a high probability (if ? is chosen appropriately).
(b) Check whether the “clique” measure found in the preceding ques- tion has anti-monotone property. A measure M is anti-monotone
if ?X, z : M (X) ≥ M (X ? {z}), i.e., M is non-increasing when the itemset X is expanded by adding another item z to the set.
Answer: Yes. The previous measure has an anti-monotone prop-
erty. Let X = {x1, x2, · · · , xk} and Xˆ = {x1, x2, · · · , xk, z} be an
extension of X. Based on the definition of the measure M (X):


M (X)  =	min c(L ? X — L) = min

P (X)



L?X
=


P (X)

L?X P (L)

maxL?X P (L)
P (X)
=
maxxj ?X P (xj)


(5.2)


where we have used the fact that maxxj ?X P (xj) ≥ maxL?X P (L) due to the anti-monotone property of the support measure. Thus,

M (Xˆ)  =		P (X ? {z})	
maxxj ?X?{z} P (xj )


(5.3)


Comparing the numerators and denominators of Equations (5.2) and (5.3), it is easy to see that


P (X ? {z}) ≤ P (X),	x max z}

P (xj)	max P (xj)
xj ?X


The first inequality is due to the anti-monotone property of support measure. The second inequality follows since the maximum support of an item in an itemset will either increase or stay the same if we
add another item into the set. Thus, M (Xˆ) ≤ M (X), which means
M (X) has anti-monotone property.
(c) Design an Apriori -like algorithm to efficiently extract the clique patterns. The pseudo-code for the algorithm should look like this:

Algorithm 5.3 Clique Finding Algorithm
1: Let F1 = {i ? I}, i.e., assume every 1-itemset is a clique pattern.
2: Count the support of each itemset in F1.
3: Let k = 2
4: repeat
5:	Candidate Generation
6:	Candidate Pruning
7:	Support Counting
8:	Candidate Elimination
9:	k ? k + 1
10: until Fk is empty
 11: return R = F1 ? F2 ? · · · , i.e. the set of all clique patterns.	

State which of the following steps of the Apriori algorithm (can- didate generation, candidate pruning, support counting, and can- didate elimination) should be modified so that the output consists only of clique patterns. Explain clearly what type of modification you need to make for those steps. For example, if the candidate pruning step has to be modified, how would you do it?
Answer: The candidate generation step should be modified. As- sume all items within an itemset are ordered in increasing support
values, i.e., if X = {a, c, d}, then P (a) ≤ P (c) ≤ P (d). Given a k-clique pattern X = {x1, x2, · · · , xk—1, xk}, we merge it with another k-clique pattern Xr = {xr1, xr2, · · · , xrk—1, xrk } to produce a candidate (k + 1) pattern X ? {xrk } if the following two conditions are satisfied:
i. They share the first k — 1 items in common, i.e., xr1 = x1,
xr2 = x2, · · · , xrk—1 = xk—1 (but xrk	xk).
ii. P (xrk ) ≥ P (xk) and P (X)/P (xrk ) ≥ ? .


The first condition is exactly the same as the candidate generation step for Apriori. The second condition ensures that the itemset
{x1, x2, · · · , xk, xrk } is a viable candidate. If P (X)/P (xrk ) < ? , then P (X ? {xr})/P (xrk ) < ? ; so the itemset is guaranteed to be a non- clique pattern. The candidate pruning and support counting steps
are exactly the same as the Apriori algorithm. The candidate elimi- nation step is modified to prune all itemsets whose measure (M (X)) is below the user-specified threshold ? .
14. Consider the following set of candidate 3-itemsets:
{a, b, c}, {a, b, d}, {a, b, e}, {a, c, d}, {a, c, e}, {a, c, ƒ}, {a, d, e},
{b, c, d}, {b, c, e}, {b, c, ƒ}, {b, d, e}, {b, e, ƒ}, {c, d, e}, {c, e, ƒ}
(a) Construct a hash tree for storing the above 3-itemsets. Assume the hash tree uses a hash function where items a, d are hashed to the left child of a node, items b, e are hashed to the middle child, while items c, ƒ are hashed to the right child. A candidate k-itemset is inserted into the tree by hashing on each successive item in the candidate and then following the appropriate branch of the tree according to the hash value. Once a leaf node is reached, the candidate is inserted based on one of the following conditions:
Condition 1: If the depth of the leaf node is equal to k (the root node is assumed to be at depth 0), then the candidate is added to the leaf
node irrespective of the number of itemsets already stored at the node.
Condition 2: If the depth of the leaf node is less than k, then the candidate is added to the leaf node as long as the number of itemsets currently
stored at the leaf node is less than maxsize = 2. Otherwise, change the leaf node into an internal node and distribute the candidates (including the new candidate to be added) to its children based on their respective hash values. Label the leaf nodes (from left to right) as L1, L2, L3, and so on.
Answer: Shown in Figure 5.6.
(b) Consider a transaction that contains items {a,b,d,e,f}. List all the leaf nodes in the hash tree to which the transaction will be hashed
into.






Figure 5.6. Hash tree.

Answer: Shown in Figure 5.4, the transaction will be hashed into leaf nodes L1, L2, L3, L4, L8 and L9 .
(c) Suppose all the candidate 3-itemsets above are frequent. List all candidate 4-itemsets that can be generated from the frequent 3- itemsets using the candidate generation procedure for Apriori.
Answer: {a,b,c,d}, {a,b,c,e}, {a,b,d,e}, {a,c,d,e}, {a,c,d,f}, {a,c,e,f},
{b,c,d,e}, {b,c,d,f}, {b,c,e,f}.
(d) List all candidate 4-itemsets that survive the candidate pruning step of the Apriori algorithm.
Answer: {a,b,c,d}, {a,b,c,e}, {a,b,d,e}, {a,c,d,e}, {b,c,d,e}, {b,c,e,f}.
(e) If all the candidate 4-itemsets in part (d) are frequent, is it possible to generate a candidate 5-itemset? If yes, what is the candidate 5-itemset?
Answer: The candidate 5-itemset is {a,b,c,d,e}. We can see all the subsets of {a,b,c,d,e} are frequent. Hence, {a,b,c,d,e} is a frequent 5-itemset.

15. Consider the closing prices for five stocks (A, B, C, D, and E) listed in Table 5.1. Suppose you are interested in applying association rule mining to the data.


Table 5.1. Example of stock market data

Day
A
B
C
D
E
1
10.50
11.00
20.00
80.00
95.00
2
12.30
10.40
26.40
76.50
90.20
3
12.00
10.80
26.50
75.50
91.00
4
11.20
10.00
25.50
72.00
87.10
5
11.30
10.20
25.30
73.20
88.90
6
12.50
10.70
27.50
70.00
88.50
7
13.00
10.80
28.80
72.00
90.20
8
13.80
11.00
29.80
71.80
91.00
9
12.95
10.80
27.90
71.00
91.20
10
12.05
10.10
26.10
72.60
92.80
11
11.40
10.05
24.95
70.40
90.10


(a) Convert the stock market prices into transaction data. For each stock X on trading day t, compute the change in its closing price,

∆  (t) = pt(X) — pt—1(X) ,
pt—1(X)

where pt(X) is the price of stock X on day t. Next, create an “item” X-UP for trading day t if ∆X(t) ≥ 0.05 (i.e., if the closing price is up by at least 5%) or X-DOWN if ∆X(t) ≤ —0.05 (i.e., if
the closing price is down by at least 5%). Assuming each trans-
action corresponds to a trading day (starting from Day 2), list all the 10 transactions (including its items) created from the data set. Note that there are 10 possible items: A-UP, A-DOWN, B-UP, B-
DOWN, · · · , E-UP, E-DOWN, that can appear in the transaction data.
Answer1:

Transaction1: {A-UP, B-DOWN, C-UP, E-DOWN};
Transaction2: {};
Transaction3: {A-DOWN, B-DOWN}; Transaction4: {};
Transaction5: {A-UP, C-UP }; Transaction6: {}; Transaction7: {A-UP};


Transaction8: {A-DOWN, C-DOWN }; Transaction9: {A-DOWN, B-DOWN, C-DOWN };
Transaction10: {A-DOWN };
Answer2:(Another acceptable answer)
Transaction1: {A-UP, B-DOWN, C-UP, E-DOWN};
Transaction2: {A-DOWN, B-DOWN}; Transaction3: {A-UP, C-UP }; Transaction4: {A-UP};
Transaction5: {A-DOWN, C-DOWN }; Transaction6: {A-DOWN, B-DOWN, C-DOWN };
Transaction7: {A-DOWN };
(b) Assuming the minimum support threshold is 20%, i.e., an itemset has to appear at least twice in the transaction data to be considered frequent, list all the frequent 1-itemsets, 2-itemsets, and so on (in- cluding their support values), that can be extracted from the data. Answer1:
Frequent 1-itemsets: {A-UP} (0.3), {A-DOWN}(0.4), {B-DOWN}
(0.3), {C-UP} (0.2), {C-DOWN} (0.2);
Frequent 2-itemsets: {A-DOWN, B-DOWN}(0.2), {A-DOWN, C- DOWN}(0.2), {A-UP, C-UP}(0.2).
Answer2:
Frequent 1-itemsets: {A-UP} (3/7), {A-DOWN}(4/7), {B-DOWN}
(3/7), {C-UP} (2/7), {C-DOWN} (2/7);
Frequent 2-itemsets: {A-DOWN, B-DOWN}(2/7), {A-DOWN, C- DOWN}(2/7), {A-UP, C-UP}(2/7).
(c) Based on the frequent itemsets found in part (b), generate all the association rules with minsup = 20% and minconf = 60%. Ignore the rules in which their left or right hand side correspond to an empty set.
Answer1:
A-UP ? C-UP: support = 0.2, confidence = 2/3;
B-DOWN ? A-DOWN: support = 0.2, confidence = 2/3; C-UP ? A-UP: support = 0.2, confidence = 1;
C-DOWN ? A-DOWN: support = 0.2, confidence = 1;
Answer2:
A-UP ? C-UP: support = 2/7, confidence = 2/3;


B-DOWN ? A-DOWN: support = 2/7, confidence = 2/3; C-UP ? A-UP: support = 2/7, confidence = 1;
C-DOWN ? A-DOWN: support = 2/7, confidence = 1;

16. Consider the lattice diagram shown in Figure 5.7. Assume all the item- sets in the lattice are frequent.


Figure 5.7. Search space for transaction database that contains 6 items.

List all the itemsets that are guaranteed to be not closed given each of the conditions below. We are interested in identifying the non-closed itemsets because they can be discarded by the closed itemset generation algorithm. Answer none if no itemsets can be guaranteed to be not closed by the given condition. Note that for some conditions, there may be more than one non-closed frequent itemset.
(a) Support of {A, B} is equal to support of {A, B, D}
Answer: {A,B}, {A,B,C}, {A,B,E}, {A,B,C,E}.
(b) The transactions that contain {A, B} is a subset of the transactions that contain {D}.
Answer: {A,B}, {A,B,C}, {A,B,E}, {A,B,C,E}.


(c) The confidence of the association rule {A, B} ? {D} is 100%.
Answer: {A,B}, {A,B,C}, {A,B,E}, {A,B,C,E}.
(d) All transactions that contain {A,B} also contain {A, D}.
Answer: {A,B}, {A,B,C}, {A,B,E}, {A,B,C,E}.
17. Consider the following set of candidate 3-itemsets:
{p, q, r}, {p, q, s}, {p, r, s}, {p, r, t}, {p, r, u}, {p, s, u}, {p, t, u},
{q, r, s}, {q, r, t}, {q, s, t}, {q, s, u}, {q, t, u}, {r, s, t}, {s, t, u}
(a) Construct a hash tree for storing the above 3-itemsets. Assume the hash tree uses a hash function where items p, s are hashed to the left child of a node, items q, t are hashed to the middle child, while items r, u are hashed to the right child. A candidate k-itemset is inserted into the tree by hashing on each successive item in the candidate and then following the appropriate branch of the tree according to the hash value. Once a leaf node is reached, the candidate is inserted based on one of the following conditions:
Condition 1: If the depth of the leaf node is equal to k (the root node is assumed to be at depth 0), then the candidate is added to the leaf node irrespective of the number of itemsets already stored at the node.
Condition 2: : If the depth of the leaf node is less than k, then the candidate is added to the leaf node as long as the number of itemsets currently stored at the leaf node is less or equal to maxsize = 2. Otherwise, change the leaf node into an internal node and distribute the candidates (including the new candi- date to be added) to its children based on their respective hash values. Label the leaf nodes (from left to right) as L1, L2, L3, and so on.
Answer: As shown in Figure 5.8.
(b) Consider a transaction that contains items {p,r,s,t}. List all the leaf nodes in the hash tree to which the transaction will be hashed
into.
Answer: The transaction will be hased into L1, L4, L5 and L10.
(c) Suppose all the candidate 3-itemsets above are frequent. List all candidate 4-itemsets that can be generated from the frequent 3- itemsets using the candidate generation procedure for Apriori.
Answer: {p, q, r, s}, {p, r, s, t}, {p, r, s, u}, {p, r, t, u},{q, r, s, t} and
{q, s, t, u}.




Figure 5.8. Hash Tree

(d) List all candidate 4-itemsets that survive the candidate pruning step of the Apriori algorithm.
Answer: {p, q, r, s}, {q, r, s, t} and {q, s, t, u}.
(e) If all the candidate 4-itemsets in part (d) are frequent, is it possible
to generate a candidate 5-itemset? If yes, what is the candidate 5-itemset?
Answer: No, since there is no common prefix of length-3 among the 4-itemsets.

18. Consider a medical database that contains N “transactions” (patient records) and d “items”. Suppose we are interested to extract rules of the
form X ? y, where X is a k-itemset (1 ≤ k ≤ d — 1) and y is an item (i.e., the right-hand side of the rule contains only a single item). These
are known as discriminative rules. For medical diagnosis, for example, such rules can be used to identify segments of the population who are vulnerable to certain diseases. An example of such rule might be {male,
age > 60, alcohol-abuse} ? {prostate-cancer}.
(a) Count the maximum number of discriminative rules that can be extracted from the database (state your answer in terms of d). Answer: d ? (2d—1 — 1).
(b) Count the maximum number of discriminative rules that can ex- tracted from a given frequent k-itemset X. For example, if {p, q, r, s, t}


is a frequent 5-itemset, then {p, q, r, s} ? {t} is one possible dis- criminative rule. Note that each extracted rule must include all the
items from the k-itemset (either on the left- or right-hand side of the rule).
Answer: k.
(c) Suppose we are interested to extract all the discriminative rules whose support ≥ minsup and confidence ≥ mincof thresholds. Let X be a frequent k-itemset and X—1,X—2, · · · , and X—k be the cor-
responding set of frequent (k — 1)-itemsets obtained after removing
one of the items from X. What should be the minimum support of
the (k — 1)-itemsets in order to guarantee that the itemset X will generate at least one discriminative rule with confidence ≥ minconf. Express your answer in terms of sX (support of X) and minconƒ .
Answer: Suppose X—t is the itemset with minimum support among all itemsets X—i ? X, 1 ≤ i ≤ k. Let sX be the support of X and s—t be the support of X—t. If X can generate at least one
high confidence discriminative rule, then sX  ≥ minconƒ . Thus,

sX ≤ s—t ≤

  sX
minconf

s?t

(d) Do itemsets that generate discriminative rules have anti-monotone property? In other words, if the itemsets {p,q,r}, {p,q,s}, {p,r,s}, and {q,r,s} do not produce a discriminative rule with confidence
≥ minconf, can we conclude that {p,q,r,s} can never produce a discriminative rule with confidence ≥ minconf?
Answer: No. A counter example is shown in Table 5.2, assuming minconf = 0.5. Consider the itemset {p, q, r}. Based on the given transactions, we obtain

conƒ ({p, q} ? {r}) = conƒ ({p, r} ? {q}) = conƒ ({q, r} ? {p}) = 1/3,
which is less than minconf. Thus, the itemset {p, q, r} does not gen- erate any high confidence discriminative rule. However, conƒ ({p, q, r} ?
{s}) = 1 > minconf. Therefore, the itemset {p, q, r, s} can gener-
ate a high confidence discriminative rule, which violates the anti-
monotone property.
19. Consider a transaction database that contains six items, {p,q,r,s,t,u}.
(a) Suppose the support of {p,q} is the same as the support of {p,q,s}. Which of the following statement(s) is (are) true:


TID
Transaction
1
p,q,r,s
2
p,q
3
p,q
4
p,r
5
p,r
6
q,r
7
q,r
Table 5.2. Example of transactions


i. Support of {p} is the same as support of {p,s}.
Answer: False.
ii. Confidence of the rule {p,q} ? {s} is 100%.
Answer: True.
iii. All transactions that contain item p must also contain item s.
Answer: False.
iv. {p,q,t} is not a closed itemset.
Answer: True. This is because the support of {p,q,t} must be the same support as support of {p,q,s,t} since all transactions that contain {p, q} must also contain item s.
(b) Suppose the support of {p,q} is the same as the support of {p,s}. Which of the following statement(s) is (are) true:
i. All transactions that contain item q must also contain item s.
Answer: False.
ii. The confidence of the rule {p,q} ? {s} is 100%.
Answer: False.
iii. The support of {p,q} is the same as the support of {p,q,s}.
Answer: False.
iv. {p,q,r} is not a closed itemset.
Answer: False.

20. Consider the following set of candidate 3-itemsets:
{p, q, r}, {p, q, s}, {p, q, t}, {p, r, t}, {p, s, t}, {q, r, s}, {q, r, t},
{q, r, u}, {q, s, t}, {q, s, u}, {s, t, u}.
(a) Construct a hash tree for storing the above 3-itemsets. Assume the hash tree uses a hash function where items p, s are hashed to the left


child of a node, items q, t are hashed to the middle child, while items r, u are hashed to the right child. A candidate k-itemset is inserted into the tree by hashing on each successive item in the candidate and then following the appropriate branch of the tree according to the hash value. Once a leaf node is reached, the candidate is inserted based on one of the following conditions:
Condition 1: If the depth of the leaf node is equal to k (the root node is assumed to be at depth 0), then the candidate is added to the leaf node irrespective of the number of itemsets already stored at the node.
Condition 2: If the depth of the leaf node is less than k, then the candidate is added to the leaf node as long as the num- ber of itemsets stored at the leaf node is less than or equal to maxsize = 2. Otherwise, change the leaf node into an internal node and distribute the candidates (including the new candi- date to be added) to its children based on their respective hash values.
Answer: See Figure 5.9.


Figure 5.9. Hash Tree


(b) Consider a transaction that contains items {p,q,r,s,u}. Count the number of leaf nodes in the hash tree to which the transaction will
be hashed into.


Answer: The transaction will be hashed to 7 leaf nodes (L1, L2, L4, L5, L6, L7, and L9).
(c) Suppose all the candidate 3-itemsets above are frequent. List all candidate 4-itemsets that can be generated from the frequent 3- itemsets using the candidate generation procedure for Apriori.
Answer:
The candidate 4-itemsets generated are {p, q, r, s}, {p, q, r, t}, {p, q, s, t},
{q, r, s, t}, {q, r, s, u}, {q, r, t, u}, {q, s, t, u}.
(d) List all candidate 4-itemsets that survive the candidate pruning step of the Apriori algorithm.
Answer:
The candidate 4-itemsets that survive the pruning step are {p, q, r, t}
and {p, q, s, t}.
21. Consider a transaction dataset that contains five items, {A, B, C, D, E}.
(a) Suppose the support of {A, B} is the same as the support of {A, B, C}, which one of the following statements are true:
i. Support of {A} is the same as support of {A, C}. False
ii. The confidence of the rule {A, B} ? {C} is 100%. True
iii. All transactions that contain item A must also contain item C.
False
iv. {A, B, D} is not a closed itemset.	True because its support will be identical to the support of {A, B, C, D}.
(b) Suppose the support of {A, B} is the same as the support of {A, C}, which one of the following statements are true:
i. All transactions that contain item B must contain item C.
False
ii. The confidence of the rule {A, B} ? {C} is 100%. False
iii. The support of {A, B} is the same as the support of {A, B, C}.
False
iv. {A, B, D} is not a closed itemset.  False
(c) Suppose all the transactions that contain {A, B} also contain {B, C}, which one of the following statements are true:
i. The confidence of the rule {B, C} ? {A} is 100%. False
ii. The support of {A} is the same as the support of {C}. False


iii. {A, B, D} is not a closed itemset.	True because its support will be identical to support of {A, B, C, D}.
(d) Suppose the confidence of the rules {A, B} ? C and {A, B} ? D
are identical, which one of the following statements are true:
i. The confidence of the {A, B} ? {C, D} is the same as the confidence of {A, B} ? {C}. False
ii. All transactions that contain {A,B,C} also contain {A,B,D}.
False
iii. {A, B} is not a closed itemset. False
(e) Consider the lattice structure shown in Figure 5.10.


Figure 5.10. Search space for transaction database that contains 6 items.


For each of the following scenarios, list all the itemsets that are guaranteed to be not closed when:
i. Support of {A, B} is equal to support of {A, B, C}.
Answer: {A, B}, {A, B, D}, {A, B, E}, and {A, B, D, E}.
ii. All the transactions that contain {A, B} is a subset of the trans- actions that contain {C}.
Answer: {A, B}, {A, B, D}, {A, B, E}, and {A, B, D, E}.


6




Sequential Patterns and Frequent Subgraphs

6.1 Sequential Pattern Mining
1. (a) List all the 3-subsequences contained in the following data sequence:

< {a, b} {c} {d} {a} >,
assuming there are no timing constraints.
Answer:
< {a, b} {c} >, < {a, b} {d} >, < {a, b} {a} >, < {a} {c} {d} >,
< {b} {c} {d} >, < {a} {c} {a} >, < {b} {c} {d} >, < {a} {d} {a} >,
< {b} {d} {a} >, < {c} {d} {a} >.
(b) List all the 3-element subsequences contained in the data sequence given in part (a).
Answer:
< {a, b} {c} {d} >, < {a, b} {c} {a} >, < {a, b} {d} {a} >,
< {a} {c} {d} >, < {a} {c} {a} >, < {a} {d} {a} >,
< {b} {c} {d} >, < {b} {c} {a} >, < {b} {d} {a} >, < {c} {d} {a} >.
(c) List all the candidate 4-sequences produced by the candidate gen- eration step of the GSP algorithm from the following frequent 3- sequences:
< {a, b, c} >, < {a, b}{c} >, < {a, b}{d} >, < {a, c}{d} >,


< {a, c, d} >,  < {a}{c}{d} >,  < {b, c}{d} >, < {b, c}{d} >,
< {b}{c}{d} >,  < {b}{c, d} >,  < {c, d}{a} > .
Answer:
< {a, b, c}{d} >, < {a, b}{c}{d} >, < {a, b}{c, d} >, < {a, c, d}{a} >
, < {b}{c, d}{a} >.
(d) Based on your answer in part (b), list the candidate 4-sequences that survived the candidate pruning step of the GSP algorithm. Answer: < {a, b, c}{d} >, < {a, b}{c}{d} >.
2. (a) List all the 3-subsequences contained in the following data sequence:

< {p, q} {r} {p, q} >,
assuming no timing constraints.
Answer:
< {p, q}, {r} >, < {p, q}, {p} >, < {p, q}, {q} >, < {p}, {r}, {p} >
, < {p}, {r}, {q} >, < {q}, {r}, {p} >, < {q}, {r}, {q} >, < {p}, {p, q} >
, < {q}, {p, q} >, < {r}, {p, q} >
(b) List all the 3-element subsequences contained in the data sequence given in part (a).
Answer: < {p}, {r}, {p} >, < {p}, {r}, {q} >, < {q}, {r}, {p} >, <
{q}, {r}, {q} >
(c) List all the candidate 4-sequences produced by the candidate gen- eration step of the GSP algorithm from the following frequent 3- sequences:
< {p, q, r} >, < {p, q}{s} >, < {p}{s, p} >, < {p}{p, q} >,
< {p, r, s} >,  < {r}{s}{s} >,  < {q, r}{s} >, < {p, r}{s} >,
< {q}{s}{p} >, < {q}{r, s} >, < {r, s}{s} > .
Answer:
< {p, q, r}, {s} >, < {p, q}, {s}, {p} >, < {p, r, s}, {s} >, < {q, r}, {s}, {s} >
, < {p, r}, {s}, {s} >, < {q}, {r, s}, {s} >, < {p}, {p, q, r} >, < {q}, {p, q}, {s} >
(d) Based on your answer in part (b), list the candidate 4-sequences that survived the candidate pruning step of the GSP algorithm. Answer:
< {p, q, r}, {s} >


3. (a) List all the 3-subsequences contained in the following data sequence:

< {a, b} {a} {a, b} >,
assuming no timing constraints.
Answer:
< {a, b} {a} >, < {a, b} {b} >, < {a} {a} {a} >, < {a} {a} {b} >,
< {a} {a, b} >, < {b} {a} {a} >, < {b} {a} {b} >, and <
{b} {a, b} >
(b) List all the 2-element subsequences contained in the data sequence given in part (a).
Answer:
< {a, b} {a} >, < {a, b} {a, b} >, < {a} {a, b} >, < {a, b} {b} >,
< {b} {a} >, < {b} {b} >, < {a} {a} >, < {a} {b} >, and
< {b} {a, b} >.
(c) List all the candidate 4-sequences produced by the candidate gen- eration step of the GSP algorithm from the following frequent 3- sequences:
< {a, b, c} >, < {a, b}{c} >, < {b}{a, c} >, < {b}{c, d} >,
< {b, c, d} >, < {a, b}{d} >, < {b, c}{d} >, < {a, c}{d} >,
< {a}{c}{d} >,  < {b}{a, d} >,  < {b}{c}{d} > .
Answer:
< {a, b, c, d} >, < {a, b, c}{d} >, < {a, b}{c, d} >, < {a, b}{c}{d} >, and < {b}{a, c}{d} >.
(d) Based on your answer in part (b), list the candidate 4-sequences that survived the candidate pruning step of the GSP algorithm. Answer:
< {a, b, c}{d} > and < {a, b}{c}{d} >
4. (a) List all the 3-subsequences contained in the following data sequence:

< {p, q} {r} {q} >,
assuming no timing constraints.
Answer: < {p, q} {r} >, < {p, q} {q} >, < {p} {r} {q} >, <
{q} {r} {q} >.


(b) List all the 3-element subsequences contained in the data sequence given in part (a).
Answer: < {p, q} {r} {q} >, < {p} {r} {q} >, < {q} {r} {q} >.
(c) List all the candidate 4-sequences produced by the candidate gen- eration step of the GSP algorithm from the following frequent 3- sequences:
< {p, q, r} >, < {p, q}{s} >, < {p}{s, p} >, < {p}{p, q} >,
< {p, r, s} >,  < {r}{s}{s} >,  < {q, r}{s} >, < {p, r}{s} >,
< {q}{s}{p} >, < {q}{r, s} >, < {r, s}{s} > .
Answer:
< {p, q, r} {s} >, < {p, q} {s} {p} >, < {p} {p, q, r} >, < {p} {p, q} {s} >,
< {p, r, s} {s} >, {q, r} {s} {s} >, {p, r} {s} {s} >, and {q} {r, s} {s} >.
(d) Based on your answer in part (b), list the candidate 4-sequences that survived the candidate pruning step of the GSP algorithm. Answer: < {p, q, r} {s} >.
5. (a) List all the 3-subsequences contained in the following data sequence:

< {a, b} {a, b} {c} >,
assuming there are no timing constraints.
Answer: < {a, b}{a} >, < {a, b}{b} >, < {a, b}{c} >, < {a}{a, b} >,
< {a}{a}{c} >, < {a}{b}{c} >, < {b}{a}{c} >, < {b}{b}{c} >,
< {b}{a, b} >.
(b) List all the 3-element subsequences contained in the data sequence given in part (a).
Answer: < {a, b}{a, b}{c} >, < {a, b}{a}{c} >, < {a, b}{b}{c} >,
< {a}{a, b}{c} >, < {a}{a}{c} >, < {a}{b}{c} >, < {b}{a, b}{c} >,
< {b}{a}{c} >, < {b}{b}{c} >.
(c) List all the candidate 4-sequences produced by the candidate gen- eration step of the GSP algorithm from the following frequent 3- sequences:
< {a, b, c} >, < {a, b}{c} >, < {a, b}{d} >, < {a, c}{d} >,
< {a}{c}{d} >, < {b, c}{d} >, < {b, c, d} >, < {b}{a, c} >,
< {b}{a, d} >,  < {b}{c, d} >,  < {b}{c}{d} > .
Answer:	< {a, b, c}{d} >, < {a, b, c, d} >, < {a, b}{c, d} >,
< {a, b}{c}{d} > and < {b}{a, c}{d} >.


(d) Based on your answer in part (b), list the candidate 4-sequences that survived the candidate pruning step of the GSP algorithm. Answer:	< {a, b, c}{d} > and < {a, b}{c}{d} >.
6. Consider a sequence database containing items p, q, r, s, and t.
(a) List all the unique 3-subsequences contained in the following data sequence:
< {p} {p, q} {p} {r} >,
assuming there are no timing constraints.
Answer: < {p} {p, q} >, < {p} {p} {p} >, < {p} {p} {r} >,
< {p} {q} {p} >, < {p} {q} {r} >, < {p, q} {p} >, < {p, q}, {r} >,
< {q}{p}{r} >.
(b) List all the unique 4-element subsequences contained in the data
sequence given in part (a).
Answer: < {p} {p} {p} {r} >, < {p} {q} {p} {r} >, < {p} {p, q} {p} {r} >
(c) List all the candidate 4-sequences produced by the candidate gen-
eration step of the GSP algorithm from the following frequent 3- sequences:
< {p, q, r} >, < {p, q}{s} >,  < {p, q}{t} >,  < {p}{p, q} >,
< {p, r}{s} >,  < {p, r}{t} >,  < {p}{q, r} >,  < {p}{s, t} >,
< {q, r}{s} >,  < {q, r}{t} >,  < {q}{s, t} >,  < {r}{s, t} > .
Answer: < {p, q, r} {s} >, < {p, q, r} {t} >, < {p, q} {s, t} >, <
{p} {p, q, r} >, < {p}{p, q}{s} >, < {p}{p, q}{t} >, < {p, r} {s, t} >
, < {p} {q, r} {s} >, < {p} {q, r} {t} >, < {q, r} {s, t} > .
(d) Based on your answer in part (c), list the candidate 4-sequences
that survived the candidate pruning step of the GSP algorithm.
Answer: < {p, q, r}, {s} >, < {p, q, r}, {t} >, < {p, q}, {s, t} >,
< {p, r}, {s, t} >, < {q, r}, {s, t} > .
7. (a) List all the 3-subsequences contained in the following data sequence:
< {p, q} {r} {p, q} >,
assuming no timing constraints.
Answer:
< {p, q}, {r} >, < {p, q}, {p} >, < {p, q}, {q} >, < {p}, {r}, {p} >,
< {p}, {r}, {q} >, < {q}, {r}, {p} >, < {q}, {r}, {q} >, < {p}, {p, q} >
< {q}, {p, q} >, < {r}, {p, q} >


(b) List all the 3-element subsequences contained in the data sequence given in part (a).
Answer:
< {p}, {r}, {p} >, < {p}, {r}, {q} >, < {q}, {r}, {p} >, < {q}, {r}, {q} >
, < {p, q}{r}{p, q} >, < {q}{r}, {p, q} >, < {p}{r}{p, q} >, < {p, q}{r}{q} >
, < {p, q}{r}{p} >
(c) List all the candidate 4-sequences produced by the candidate gen- eration step of the GSP algorithm from the following frequent 3- sequences:
< {p, q, r} >, < {p, q}{s} >, < {p}{s, p} >, < {p}{p, q} >,
< {p, r, s} >,  < {r}{s}{s} >,  < {q, r}{s} >, < {p, r}{s} >,
< {q}{s}{p} >, < {q, r}{s} >, < {q}{r, s} >, < {r, s}{s} > .
Answer:
< {p, q, r}, {s} >, < {p, q}, {s}, {p} >, < {p, r, s}, {s} >, < {q, r}, {s}, {s} >
, < {p, r}, {s}, {s} >, < {q}, {r, s}, {s} >, < {p}, {p, q, r} >, < {p}, {p, q}, {s} >
(d) Based on your answer in part (c), list the candidate 4-sequences that survived the candidate pruning step of the GSP algorithm.
Answer:
< {p, q, r}, {s} >
8. Consider the following data sequence s:

Timestamp
Element
10
20
40
50
80
{p, q, r, s}
{p, q, t}
{q, r}
{q, s, t}
{r, s, t}

where the timestamp indicates the time in which events associated with the given element were observed.
(a) State whether each following sequential pattern w is a contiguous subsequence of the data sequence s:
• w =< {p}{q}{r}{s}{t} >
Answer: Yes
• w =< {p}{p}{q}{q} >
Answer: Yes


• w =< {p}{s}{t} >
Answer: No
• w =< {p, r}{q, r}{q, s} >
Answer: No
(b) State whether the following sequential pattern is contained in (i.e., supported by) the data sequence above. Use the following time constraints:
mingap = 0, maxgap = 35, window size = 15, maxspan = 65.
• w =< {p, q, r, s, t} >
Answer: Yes
• w =< {q, r, s, t}{q, r}{q, s} >
Answer: Yes
• w =< {q, r, s}{r, s}{r, s} >
Answer: No
• w =< {p, q, r}{q, r, s} >
Answer: No
9. Consider the following data sequence s:

Timestamp
Element
10
20
40
50
80
{a, b, c}
{a, b, d}
{c, d}
{a, b, d}
{b, e}

where the timestamp indicates the time in which events associated with the given element were observed.
(a) State whether each following sequential pattern w is a contiguous subsequence of the data sequence s:
• w =< {a}{b}{c}{d}{e} >
Answer: Yes
• w =< {a, b, c}{a, b}{a} >
Answer: No
• w =< {a}{a}{a} >
Answer: No
• w =< {b}{c}{d} >
Answer: Yes


(b) State whether the following sequential pattern is supported by the data sequence s above given the following time constraints: mingap = 0, maxgap = 35, window size = 15, maxspan = 65.
• w =< {a, b, c, d, e} >
Answer: No; violate window size constraint.
• w =< {a, b, c, d}{e} >
Answer: No; violate maxgap constraint.
• w =< {a, b, c, d}{a, b, d, e} >
Answer: No; violate maxgap constraint.
• w =< {b}{c}{d}{e} >
Answer: Yes.


6.2 Frequent Subgraph Mining
1. Draw all candidate subgraphs obtained from joining the pair of graphs shown in Figure 6.1. Assume the edge-growing method is used to expand the subgraphs.

(a)
+



(b)
+



Figure 6.1. Edge growing of subgraphs.

Answer: The candidate subgraphs are shown in Figure 6.2
2. Consider all the frequent 3-subgraphs shown in Figure 6.3.

(a) Draw all the candidate 4-subgraphs obtained by merging the fre- quent 3-subgraphs in Figure 6.3. Assume the edge-growing method is used to expand the subgraphs.
Answer:
Let F1, F2, and F3 be the three frequent subgraphs shown in Figure
6.3. The candidate subgraphs in Figure 6.4 are obtained as follows:
i. Merging F1 with F1: G1
ii. Merging F2 with F2: G2
iii. Merging F1 with F2: G3
iv. Merging F3 with F3: G4, G5, and G6
v. Merging F1 with F3: G7 and G8
vi. Merging F2 with F3: G9





(a) or






(b)
or	or






or	or





Figure 6.2. Edge growing of subgraphs.



Figure 6.3. Edge growing of subgraphs.

(b) Which of the candidate 4-subgraphs survived the candidate pruning step?
Answer The candidate subgraphs that survive the pruning step are G1, G3, G4, G6, and G7.



Figure 6.4. Candidate 4-subgraphs.

3. Draw all candidate subgraphs obtained from joining the pair of graphs shown in Figure 6.5. Assume the edge-growing method is used to expand the subgraphs. Make sure you remove all the isomorphic graphs.




Figure 6.5. Candidate Generation for Subgraphs

Answer: See Figure 6.6.
4. Draw all the candidate 5-subgraphs obtained by merging the frequent 4-subgraphs shown in Figure 6.7 using the FSG algorithm. Assume the edge-growing method is used to expand the subgraphs. Make sure you remove all the isomorphic subgraphs (i.e., subgraphs that are structurally equivalent). You must also specify which pair of frequent 4-subgraphs,





Figure 6.6. Candidate Generation for Subgraphs

(G1, G2), (G1, G3), or (G2,G3), is merged to generate each of your candidate subgraphs. If a candidate can be generated by more than one pair, you only need to list one of such pairs.


Figure 6.7. Candidate Generation for Subgraphs


Answer:
The solution is shown in Figure 6.8. For each candidate subgraph, the core used to generate the subgraph is shown as shaded nodes in the diagram. Furthermore, some of the subgraphs generated are isomorphic to each other. For example, the subgraphs g1a and g1b are isomorphic, so do (g3a,g3b), (g6a,g6b) and (g8a,g8b). There are altogether 14 distinct candidate 5-subgraphs generated.




5. Draw all the candidate 4-subgraphs obtained by merging the frequent 3-subgraphs shown in Figure 6.9 using the FSG algorithm. Assume the edge-growing method is used to expand the subgraphs. Make sure you remove all the isomorphic subgraphs (i.e., subgraphs that are structurally equivalent). You must also specify which pair of frequent 3-subgraphs, (G1, G2), (G1, G3), or (G2,G3), is merged to generate each of your candidate subgraphs. If a candidate can be generated by more than one pair, you only need to list one of such pairs. Your candidate 4-subgraphs should also include self-joins, i.e., merging each graph with itself.


Figure 6.9. Candidate Generation for Subgraphs

Answer: As shown in Figure 6.10.
6. For each question below, draw all the candidate 5-subgraphs generated from joining a pair of frequent 4-subgraphs shown in Figure 6.11 using the method described in the lecture. Assume the edge-growing method is used to expand the subgraphs. Note that we focus on connected sub- graphs only with no self-loops and no multiple edges (i.e., there cannot be more than one edge incident on the same pair of vertices). Indicate what are the cores (i.e., the common frequent 3-subgraphs) between the two frequent subgraphs that were joined. For example, the vertices that are part of the core can be shaded (colored) while those that are not part of the core can be left unshaded (no color). Answer no candidates if the graphs to be joined have no common core.




Figure 6.10. Candidate 4-subgraphs by FSG algorithm



Figure 6.11. Frequent 4-subgraphs.

(a) Draw all the candidate subgraphs after joining G1 with itself. Answer: See Figure 6.12 (the core is represented by the shaded nodes).
(b) Draw all the candidate subgraphs after joining G2 with itself. Answer: See Figure 6.13 (the core is represented by the shaded nodes).




Figure 6.12. Candidate 5-subgraphs.



Figure 6.13. Candidate 5-subgraphs.

(c) Draw all the candidate subgraphs after joining G3 with itself. Answer: See Figure 6.14 (the core is represented by the shaded nodes).
(d) Draw all the candidate subgraphs after joining G1 with G2. Answer: See Figure 6.15 (the core is represented by the shaded nodes).
(e) Draw all the candidate subgraphs after joining G1 with G3. Answer: See Figure 6.16 (the core is represented by the shaded nodes).
(f) Draw all the candidate subgraphs after joining G2 with G3.




Figure 6.14. Candidate 5-subgraphs.



Figure 6.15. Candidate 5-subgraphs.

Answer: No candidates because the graphs do not share a common core.

7. Consider the frequent 3-subgraphs shown in Figure 6.17. Derive the candidate 4-subgraphs obtained by merging the frequent 3-subgraphs using the FSG algorithm. Assume the edge-growing method is used to expand the subgraphs. Make sure you indicate clearly the vertices and edges that are part of the “core” subgraph used for merging. You can do this by shading the core vertices and using thicker lines for the core edges.

(a) Draw the candidate 4-subgraphs obtained by merging G1 with G2.




Figure 6.16. Candidate 5-subgraphs.



Figure 6.17. Candidate Generation for subgraphs

Answer: See Figure 6.18.
(b) Draw the candidate 4-subgraphs obtained by merging G2 with G3.
Answer: See Figure 6.19.
(c) Draw the candidate 4-subgraphs obtained by merging G1 with G3.
Answer: See Figure 6.20.



Figure 6.18. Candidates subgraphs obtained by merging G1 with G2.



Figure 6.19. Candidate subgraphs obtained by merging G2 with G3



Figure 6.20. Candidate subgraphs obtained by merging G1 with G3

(d) Based on your answers in parts (a), (b), and (c), draw all the can- didate 4-subgraphs that survive the candidate pruning step of FSG algorithm.
Answer: All the candidates generated in part (b) by merging G2 with G3 are pruned. The surviving candidate subgraphs are shown in Figure 6.21.





















Figure 6.21. Candidate subgraphs generated.




7




Cluster Analysis

7.1 K-means
1. For this question, you need to show that sum-of-squared errors (SSE) is non-increasing when the number of clusters increases. Consider a data set ? = {x1, x2, · · · , xN } that contains N observations. Each observa-
tion xi is a p-dimensional vector of continuous-valued attributes.
(a) Suppose all the N observations are grouped into a single cluster. Let µ be the centroid of the cluster. Express the total sum-of-squared errors, SSET , in terms of xi, µ and N .
Answer:

N	N	p

SSET =
i=1

xi — µ 2 =	(xij — µj)2
i=1 j=1


(b) Show that SSET can be decomposed into p separate terms, one for
each attribute, i.e., SSET = ?p	SSEi.
Answer:

N	p	p	N	p

SSET	=

? ?(xij — µj)2 = ? ?(xij — µj)2 = ? SSEj


where SSEj is the total SSE for dimension j. Therefore, to mini- mize the total SSE, we can minimize the SSE for each dimension independently.


(c) Based on the results in part (b), it is sufficient to show that SSE is non-increasing with increasing number of clusters assuming an observation xi has one-dimensional attribute. Now, suppose all the N observations are grouped into two clusters, C1 and C2. Let µ1 and µ2 be their corresponding cluster centroids while n1 and n2 are their respective cluster sizes. Express the sum-of-squared errors for each cluster, SSE(i) (i = 1 or 2), in terms of xi, ni, and µi. You
need to expand the quadratic term, (a — b)2 = a2 — 2ab + b2, and simplify the expression.
Answer: For a given cluster Cj:



xi Cj
=	?


xi?Cj
x2 — 2njµ2 + njµ2

i
(since ?

j


xi = µjnj)


xi Cj
=	?


i	j	j

x2 — njµ2

xi?Cj


(d) Rewrite your expression for SSET in part (a) in terms of xi, µ1, µ2, n1, n2 and N .
Answer: Since µj = ?xi?Cj xi/nj and n1 + n2 = N , therefore

µ  =	?i xi = ?xi?C1 xi + ?xi?C2 xi = n1 µ

+ n2 µ


Replace the expression into SSET :



i

=	x2 — N
i


i
n1
N µ1 +

i	i
i
n2	2
N µ2

? 2	n2  2
	

n2  2


2n1n2




(e) Based on your answers in parts (c) and (d), show that SSET SSE(1) + SSE(2).
Answer:




SSE1 + SSE2	=	? x2 — n1µ2 + ?

x2 — n2µ2

= ? x2 — n1µ2 — n2µ2


(7.2)


Subtracting (7.2) from (7.1)

∆(SSE)  =  SSET — SSE1 — SSE2

2	n2  2

2	n2  2

2n1n2

=  n1µ1 —  1 µ1 + n2µ2 —  2 µ2 —

µ1µ2

=  n µ2(1 — n1 ) + n µ2(1 — n2 ) — 2n1n2 µ µ
=	n1n2 µ2 + n1n2 µ2 — 2n1n2 µ µ

=	n1n2 (µ N	1

— µ2)

≥ 0	(7.3)
2. Consider the following set of one-dimensional points: {5, 7, 16, 18, 24, 26, 34, 38}.
(a) Suppose we apply k-means clustering to obtain three clusters, A, B, and C. If the three initial centroids are located at (15, 25, 31), respectively, show the clustering results after assigning each point to their closest centroid.
Answer: {5,7,16,18}, {24,26}, {34,38}
(b) Based on your answer in part (a), recompute the new locations of the centroids for A, B, and C. Compute also their overall SSE. Answer: New centroids are 11.5, 25, 36. SSE is 135.
(c) What are the locations of the cluster centroids when the algorithm converges? Compute also their overall SSE.
Answer: Same as part (b).
(d) Explain how well you think bisecting kmeans will perform on the same data set (compared to regular kmeans). State your reason clearly.
Answer: Same as part (b).


(e) Suppose we add 199 equally spaced data points between 5 and 7 (i.e., 5.01, 5.02, · · · , 6.99) to the previous data set. How do the clusters look like after applying kmeans with k = 3?
Answer: {5,· · · ,7}, {16,18,24,26}, {34,38}
3. For each situation shown below, explain whether it is a feasible solution of k-means clustering (for the given value of k). The locations of the centroids are marked as ?.

		

(a) k=3

(b) k=3	(c) k=3

x


		
(d) k = 3	(e) k = 2	(f) k = 3

Figure 7.1. K-means clustering.


Answer:

(a) No. The middle centroid is stable; but the two outer centroids are not.
(b) No. The centroids on the ring will be pulled toward the center of the ring.
(c) No. The centroids on the ring will be pulled toward the center of the ring.
(d) No. Points located on the northern border of the larger circle are closer to the centroids of the smaller circle than to the centroid of the larger circle. As a result, the centroids of the two smaller circles will be pulled downward.
(e) No. Argument is the same as part (d).
(f) Yes. This solution should be feasible because it is a stable (though not globally optimal) solution.


4. For each situation shown in Figure 7.2, explain whether it is a feasible solution of k-means clustering (for the given value of k). We consider a solution to be feasible if k-means converges to the centroids shown in the diagram (with the proper choice of initial centroid). The locations
of the centroids are marked as ?.







4

s/2	s/2







s/2  s/2	s/2  s/2

s/3


s



s/2









s/2


s/3


s/3




s/2

(a) k=3	(b) k=3	(c) k=2




d

w/2 w/2


d

(d) k=3	(e) k=3

Figure 7.2. K-means clustering.


Answer:

(a) Not feasible.
(b) Not feasible.
(c) Not feasible.
(d) Feasible.
(e) Not feasible.

5. Consider the following set of one-dimensional points: {0.1, 0.2, 0.45, 0.55, 0.8, 0.9}. All the points are located in the range between [0,1].


(a) Suppose we apply kmeans clustering to obtain three clusters, A, B, and C. If the initial centroids are located at {0, 0.4, 1}, respectively, show the cluster assignments and locations of the centroids after the
first three iterations by filling out the following table.


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.20
0.45
0.55
0.80
0.90
A
B
C
0
-
-
-
-
-
-
0.00
0.40
1.00
1









2









3










Answer:


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.20
0.45
0.55
0.80
0.90
A
B
C
0
-
-
-
-
-
-
0.00
0.40
1.00
1
A
B
B
B
C
C
0.10
0.40
0.85
2
A
A
B
B
C
C
0.15
0.50
0.85
3
A
A
B
B
C
C
0.15
0.50
0.85

SSE = (0.05)2 ? 6 = 0.0015
(b) For the dataset given in part (a), is it possible to obtain empty clusters? If possible, what are the values of the initial centroids? If not, state why.
Answer: Yes. For example, if the centroids are located at 0, 0.90, and 1.0. There won’t be any points assigned to cluster C.
(c) Show the clustering results obtained using bisecting kmeans (with k=3). Comparing against the result for k-means, which method is better to cluster this dataset?
Answer:
There are three possible solutions (depending on the choice of initial centroids).
The first solution is when bisecting k-means initially partitions the data into the following two equal sized clusters:
Cluster 1 contains 0.10, 0.20, and 0.45
Cluster 2 contains 0.55, 0.80 and 0.90
Next, it will select one of the two clusters and partition it into two smaller clusters. Since both clusters have the same SSE, it


randomly chooses one of them and split it into two. For example, if cluster 1 is selected for splitting, the results would be:
Cluster 1 contains 0.10 and 0.20
Cluster 2 contains 0.45
Cluster 3 contains 0.55, 0.80, and 0.90.
In this case, the k-means results are better than bisecting k-means on this dataset.
The second and third solutions occur when bisecting k-means ini- tially partitions the data into two unbalanced clusters, either: Cluster 1 contains 0.10, 0.20, 0.45, and 0.55
Cluster 2 contains 0.80 and 0.90
or
Cluster 1 contains 0.10, 0.20
Cluster 2 contains 0.45, 0.55, 0.80 and 0.90
In both cases, the clusters obtained by bisecting k-means will be identical to the clusters found by regular k-means.

6. K-means does not always converge to the optimal solution as it is sensi- tive to the choice of initial centroids. The centroids are often randomly initialized to a subset of the data points to be clustered.

(a) If the data set contains 100 data points, how many times do you need to repeat the k-means algorithm (each time with a different initialization of centroids) to ensure there is a 50% chance an op- timal k-means solution would be found? Assume the number of clusters is 10 and that each configuration of initial centroids leads to a distinct clustering solution (i.e., yields a different SSE).
Answer:
First, given 100 data points, there are M = 100 possible configura- tions of the initial centroids. If each configuration yields a distinct solution, in which only one of them gives an optimal solution, the probability you achieve the optimal solution by randomly choos- ing one of the initial centroid configuration is 1/M . If you repeat k-means twice by randomly choosing two distinct initial centroids configurations (i.e., sampling without replacement), then the prob- ability one of the configuration yields an optimal solution is 2/M . Thus, if you had randomly chosen p distinct configurations, the


probability one of the configuration is optimal is p/M . We want
p/M = 0.5, so the number of times we should repeat k-means is

p = 0.5 ? M = 0.5 ? 100  = 8.66 ? 1012
(b) If the number of data points increases from 100 to 200 (but number of clusters is still 10), will it improve or diminish your chance of finding the optimal k-means solution by randomly choosing a subset of the data points to be the initial centroids?
Answer:
Since  200 > 100  and the probability that a random initial con-
figuration yields an optimal solution is 1/M , it will diminish the chance.
(c) If the number of data points is fixed but number of clusters (and number of initial centroids) increases, will it improve or diminish your chance of finding the optimal k-means solution by randomly choosing a subset of the data points to be the initial centroids?
Answer:
Since 100 > 100 only if k is between 10 and 89, it will generally
diminish your chance unless the number of clusters is at least 90.

7. Consider the following set of one-dimensional data points: {0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 0.9}.
(a) Suppose we apply kmeans clustering to obtain three clusters, A, B, and C. If the initial centroids are located at {0, 0.25, 0.6}, respec- tively, show the cluster assignments and locations of the centroids
after the first three iterations.
Answer:


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.20
0.40
0.50
0.60
0.80
0.90
A
B
C
0
-
-
-
-
-
-
-
0.00
0.25
0.60
1
A
B
B
C
C
C
C
0.1
0.3
0.7
2
A
A
B
B
C
C
C
0.15
0.45
0.77
3
A
A
B
B
B
C
C
0.15
0.5
0.85


(b) Compute the SSE of the k-means solution (after 3 iterations).
Answer:

SSE = 0.03

(c) Apply bisecting k-means (with k=3) on the data. First, apply k- means on the data with k=2 using initial centroids located at {0.1, 0.9}.


Iter
Cluster assignment of data points
Centroid

0.10
0.20
0.40
0.50
0.60
0.80
0.90
A
B
0
-
-
-
-
-
-
-
0.10
0.90
1









2









Next, compute the SSE for each cluster (make sure you indicate the SSE values in your answer). Choose the cluster with larger SSE value and split it further into 2 sub-clusters. You can choose the two data points with the smallest and largest values as your initial centroids. For example, if the cluster to be split contains data points (0.20, 0.40, 0.60, and 0.80), then the centroids should be initialized to 0.20 and 0.80. Show the clustering solution produced obtained applying bisecting k-means.
Answer:


Iter
Cluster assignment of data points
Centroid

0.10
0.20
0.40
0.50
0.60
0.80
0.90
A
B
0
-
-
-
-
-
-
-
0.10
0.90
1
A
A
A
B
B
B
B
0.50
0.90
2
A
A
A
B
B
C
C
-
-

SSE = 0.056
(d) Compare the results of k-means clustering against bisecting k-means. Which clustering method is more effective for the given data set? Answer:
According to SSE, k-meas has smaller SSE value, so k-means is better in this case.

8. Consider a data set ? = {x1, x2, · · · , xN }, where each xi is a d-dimensional feature vector. The goal of k-means clustering algorithm is to minimize


the following objective function:

N	k

min

? ? Wij xi — cj 2,	(7.4)


where k is the number of clusters, cj is the centroid of cluster j, and W is an N ? k cluster membership matrix (Wij = 1 if data point xi belongs to cluster j or 0 otherwise). After assigning each data point to
its nearest centroid, the centroid location ck can be updated as follows:
?N  Wikxi
	 


N
i=1
points assigned to cluster k.

Wik corresponds to the number of data

The update formula for cluster centroids depends on the choice of ob- jective function. For each scenario described below, derive the update formula for the cluster centroids.

(a) The original k-means clustering algorithm assumes each point is equally important in determining the clusters. However, in reality, some data points could be outliers or noise. One way to address this would be to assign weights to each point, giving lower weights for points that are considered outliers or noise (assume the outliers and noise points can be pre-determined during preprocessing). Let
0 ≤ vi ≤ 1 be the weight for a data point xi. Show how the k- means objective function can be updated to accommodate weights
on each data points. Next, derive a mathematical expression for the centroid update formula (analogous to Equation 7.5). You may assume the weights of the data points are known when applying the “weighted k-means” algorithm.
Answer: The modified objective function is

N	k	N	k
L = ? ? viWij xi — cj 2 = ? ? viWij(xi — cj)T (xi — cj)


Taking its partial derivative with respect to ck and setting it to zero yields


∂L
∂ck



=	viWik
i=1

— 2xi + 2ck  = 0

=? ck =

N
i=1 N

viWikxi viWik

i=1

which is similar to the original k-means approach except the cen- troids are computed based on a weighted average of the data points associated with the cluster.
(b) The original formulation for k-means assumes a squared Euclidean distance function,

d
d(xi, cj) = xi — cj 2 =	(xip — cjp)2.
p=1

Suppose the objective function is modified to 1 - cosine similarity:

d
T
i
p=1

where each data point xi is assumed to have been normalized to unit length, i.e., xT xi = 1. Derive a mathematical expression for the centroid update formula (analogous to Equation 7.5) using the new objective function. Note that you should constrain the centroids to have unit length, i.e., cT ci = 1.
Answer: The Lagrangian formulation for the constrained opti- mization problem can be written as follows

N	k	k
L = ? ? Wij(1 — xT cj) + ? ?j(cT cj — 1)


Taking its partial derivative with respect to cm and setting it to zero yields


∂L
∂cm

N
=	—	Wimxi
i=1


+ 2?mcm = 0

?N  Wimxi
=	c	=   i=1	
2?m
N	N

=? cT cm

= 1 =  1 	W
4?2	im
i=1 j=1

Wjm

xT xj


=? ?m =

1 ,u,?N ?


WimWjmxT xj

i=1 j=1
?N


Wimxi

=? cm = q?N  ?N	T


9. Consider the following set of one-dimensional points: {0.1, 0.25, 0.45, 0.55, 0.8, 0.9}. All the points are located in the range between [0,1].
(a) Suppose we apply kmeans clustering to obtain three clusters, A, B, and C. If the initial centroids are located at {0, 0.4, 1}, respectively, show the cluster assignments and locations of the updated centroids
after the first three iterations by filling out the following table.


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.25
0.45
0.55
0.80
0.90
A
B
C
0
A
B
B
B
C
C
0.00
0.40
1.00
1
A
A
B
B
C
C
0.1
0.42
0.85
2
A
A
B
B
C
C
0.18
0.5
0.85
3
A
A
B
B
C
C
0.18
0.5
0.85

Calculate the overall sum-of-squared errors of the clustering after the third iteration, where:
SSE = ? (xi — µ1)2 + ? (xi — µ2)2 + ? (xi — µ3)2

and µjs are the centroids of the 3 clusters, A, B, and C.


Answer: SSE = 0.0213
(b) For the dataset given in part (a), is it possible to obtain empty clusters? If possible, what are the values of the initial centroids? If not, state why.
Answer: Yes. If two of the centroids are randomly initialized to the left of the leftmost points or to the right of the rightmost points. For example, if the initial centroids are (0, 0.05, 0.8), then the first cluster is empty.
(c) Show the clustering results obtained using bisecting kmeans (with k=3). Start by partitioning the data into 2 clusters, with initial centroids 0 and 1. Then take the cluster with larger SSE and parti- tion it into 2 clusters. Compare the results against k-means. Which method is better to cluster the given dataset?
Answer: After the first binary partition:


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.25
0.45
0.55
0.80
0.90
A
B
0
A
A
A
B
B
B
0.0
1.0
1
A
A
A
B
B
B
0.27
0.75
2
A
A
A
B
B
B
0.27
0.75
3
A
A
A
B
B
B
0.27
0.75

The resulting sum-of-squared errors for the clusters are SSE (cluster A) = 0.0617,	SSE (cluster B) = 0.065
Next, we partition cluster B into 2, denoted as B1 and B2, with initial centroids at 0.55 and 0.80.


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.25
0.45
0.55
0.80
0.90
A
B1
B2
0
A
A
A
B1
B2
B2
-
0.55
0.80
1
A
A
A
B1
B2
B2
-
0.55
0.85
2
A
A
A
B1
B2
B2
-
0.55
0.85
3
A
A
A
B1
B2
B2
-
0.55
0.85

The SSE of the bisecting kmeans is 0.0667, which is larger than the SSE for k-means. Therefore, K-means is more effective on this data set. Intuitively, you can see that bisecting K-means always partition the middle cluster into 2 after the first step. So, the middle cluster (which contains the points 0.45 and 0.55) will always be in separate clusters, which is not the optimal solution.


10. Consider the following set of one-dimensional data points: {0.1, 0.2, 0.42, 0.5, 0.6, 0.8, 0.9}.
(a) Suppose we apply kmeans clustering to obtain three clusters, A, B, and C. If the initial centroids are located at {0, 0.25, 0.6}, respec- tively, show the cluster assignments and locations of the centroids
after the first three iterations by filling out the following table. Calculate the overall sum-of-squared errors of the clustering after the third iteration, where:
SSE = ? (xi — µ1)2 + ? (xi — µ2)2 + ? (xi — µ3)2

and µjs are the centroids of the 3 clusters, A, B, and C.
Answer:


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.20
0.42
0.50
0.60
0.80
0.90
A
B
C
0
-
-
-
-
-
-
-
0.00
0.25
0.60
1
1
2
2
3
3
3
3
0.10
0.31
0.70
2
1
1
2
2
3
3
3
0.15
0.46
0.767
3
1
1
2
2
2
3
3
0.15
0.507
0.85

Total SSE = 0.0263
(b) Apply bisecting k-means (with k=3) on the data. First, apply k- means on the data with k=2 using initial centroids located at {0.1, 0.9}.
Next, compute the SSE for each cluster (make sure you indicate
the SSE values in your answer). Choose the cluster with larger SSE value and split it further into 2 sub-clusters. You can choose the pair of points with the smallest and largest values as your initial centroids. For example, if the cluster to be split contains data points (0.20, 0.40, 0.60, and 0.80), then the centroids should be initialized to 0.20 and 0.80. Show the clustering solution produced obtained applying bisecting k-means.
Answer:


Iter
Cluster assignment of data points
Centroid

0.10
0.20
0.42
0.50
0.60
0.80
0.90
A
B
0
-
-
-
-
-
-
-
0.10
0.90
1
1
1
1
1
2
2
2
0.305
0.767
2
1
1
1
1
2
2
2
0.305
0.767


SSE (A) = 0.104, SSE (B) = 0.047, we split cluster A into two, with initial centroids at 0.10 and 0.50.


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.20
0.42
0.50
0.60
0.80
0.90
A
B
C
0
-
-
-
-
-
-
-
0.10
0.50
0.767
1
1
1
3
3
2
2
2
0.15
0.46
0.767
2
1
1
3
3
2
2
2
0.15
0.46
0.767

Total SSE = 0.0549
(c) Compare the results of k-means clustering against bisecting k-means. Which clustering method is more effective for the given data set? Answer: The k-means clustering is more effective because bisect- ing k-means always split the middle cluster into two parts.

11. Consider the one-dimensional data given in the previous question. Apply fuzzy k-means algorithm with fuzzifier = 2 to obtain three clusters, A, B, and C.

(a) If the initial centroids are located at {0, 0.25, 0.6}, respectively, show the fuzzy cluster assignments and locations of the centroids
after the first three iterations by filling out the following table.
If you assign each data point to the cluster with highest fuzzy score, are the resulting clusters similar to the k-means clustering results? Answer:


Iter
Fuzzy cluster assignment of data points
Centroid Locations

0.10
0.20
0.42
0.50
0.60
0.80
0.90
A
B
C
0
-
-
-
-
-
-
-
0.00
0.25
0.60
1: A B
C
0.674
0.058
0.080
0.033
0
0.052
0.084
0.122
0.259
0.666

0.299
0.928
0.486
0.133
0
0.111
0.161




0.027
0.014
0.434
0.833
1
0.837
0.755



2: A B
C
0.980
0.363
0.169
0.115
0.018
0.036
0.074
0.129
0.332
0.713

0.018
0.627
0.583
0.285
0.035
0.056
0.109




0.001
0.010
0.249
0.600
0.947
0.908
0.817



3: A B
C
0.983
0.764
0.077
0.112
0.047
0.016
0.050
0.143
0.440
0.761

0.015
0.221
0.847
0.548
0.144
0.033
0.093




0.002
0.015
0.076
0.340
0.809
0.951
0.857




The cluster assignment is shown in bold. The answer is not the same as k-means. However, if you continue the clustering process for another iteration, the solution is given below.



Iter
Fuzzy cluster assignment of data points
Centroid Locations

0.10
0.20
0.42
0.50
0.60
0.80
0.90
A
B
C
4: A B
C
0.980
0.939
0.005
0.027
0.058
0.003
0.030
0.149
0.474
0.817

0.016
0.052
0.992
0.924
0.472
0.012
0.081




0.004
0.009
0.003
0.050
0.469
0.985
0.889




which is similar to k-means results.
(b) Repeat the previous analysis with k=2 using initial centroids lo- cated at {0.1, 0.9}. Summarize the results for the first three itera- tions by filling out the following table.
Based on their fuzzy scores, which data point has the most “uni- form” fuzzy score distribution? Such a data point are expected to be located at the boundary between the two clusters.
Answer:


Iter
Fuzzy cluster assignment of data points
Centroid Locations

0.10
0.20
0.42
0.50
0.60
0.80
0.90
A
B
0
-
-
-
-
-
-
-
0.10
0.90
1: A
B
1.000
0.980
0.692
0.500
0.265
0.020
0.000
0.239
0.758

0.000
0.020
0.308
0.500
0.735
0.980
1.000


2: A
B
0.957
0.995
0.778
0.495
0.161
0.006
0.044
0.246
0.750

0.043
0.005
0.222
0.505
0..839
0.994
0.956


3: A
B
0.952
0.993
0.782
0.491
0.152
0.008
0.050
0.248
0.748

0.048
0.007
0.218
0.509
0.848
0.992
0.950



The data point with the most uniform distribution is located at 0.50.

12. Consider the following set of one-dimensional data points: {0.10, 0.15, 0.23, 0.50, 0.60, 0.84, 0.95}.
(a) Suppose we apply kmeans clustering to obtain three clusters, A, B, and C. If the initial centroids are located at {0.10, 0.23, 0.84}, respectively, show the cluster assignments and locations of the cen-
troids after the first three iterations.
Calculate the overall sum-of-squared errors of the clustering after the third iteration, where:
SSE = ? (xi — µ1)2 + ? (xi — µ2)2 + ? (xi — µ3)2

and µjs are the centroids of the 3 clusters, A, B, and C.
Answer:



Iter
Cluster assignment of data points
Centroid Locations

0.10
0.15
0.23
0.50
0.60
0.84
0.95
A
B
C
0
-
-
-
-
-
-
-
0.10
0.23
0.84
1
A
A
B
B
C
C
C
0.125
0.365
0.797
2
A
A
A
B
C
C
C
0.160
0.500
0.797
3
A
A
A
B
B
C
C
0.160
0.550
0.895

SSE = 0.01965
(b) Apply bisecting k-means (with k=3) on the data. First, apply reg- ular k-means with k=2 using the initial centroids located at {0.10, 0.95}.
Next, compute the SSE for each cluster (make sure you indicate
the SSE values in your answer). Choose the cluster with larger SSE value and split it further into 2 sub-clusters by filling out the table below:
Answer:
Results after first split:


Iter
Cluster assignment of data points
Centroid

0.10
0.15
0.23
0.50
0.60
0.84
0.95
A
B
0
-
-
-
-
-
-
-
0.10
0.95
1
A
A
A
A
B
B
B
0.245
0.797
2
A
A
A
A
B
B
B
0.245
0.797
Results after second split:


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.15
0.23
0.50
0.60
0.84
0.95
A
B
C
0
-
-
-
-
-
-
-
0.100
0.500
0.797
1
A
A
A
C
B
B
B
0.160
0.500
0.797
2
A
A
A
C
B
B
B
0.160
0.500
0.797
3
A
A
A
C
B
B
B
0.160
0.500
0.797
SSE = 0.07267
(c) Compare the overall SSE of k-means clustering against bisecting k-means. Which clustering method is more effective for the given data set?
Answer: K-means has lower SSE compared to bisecting k-means on the given dataset.

13. Consider the one-dimensional data given in the previous question. Apply fuzzy k-means algorithm with fuzzifier = 2 to obtain three clusters, A, B, and C. If the initial centroids are located at {0.10, 0.23, 0.84}, respec-


tively, show the fuzzy cluster assignments and locations of the centroids after the first four iterations.
If you assign each data point to the cluster with highest fuzzy score, are the resulting clusters similar to the k-means clustering results?
Answer:


Iter
Fuzzy score of data points in clusters A, B, C
Centroid Locations

0.10
0.15
0.23
0.50
0.60
0.84
0.95
A
B
C
0
-
-
-
-
-
-
-
0.10
0.23
0.84
1: A B
C
1.000
0.716
0.000
0.218
0.140
0.000
0.016
0.134
0.288
0.832

0.000
0.280
1.000
0.479
0.255
0.000
0.022




0.000
0.004
0.000
0.302
0.606
1.000
0.961



2: A B
C
0.965
0.987
0.270
0.193
0.138
0.000
0.020
0.141
0.361
0.843

0.033
0.012
0.723
0.575
0.308
0.000
0.030




0.002
0.001
0.007
0.233
0.553
1.000
0.950



3: A B
C
0.973
0.998
0.675
0.113
0.121
0.000
0.017
0.150
0.493
0.864

0.024
0.002
0.311
0.762
0.448
0.000
0.031




0.003
0.000
0.014
0.124
0.431
1.000
0.952



4: A B
C
0.980
1.000
0.903
0.000
0.046
0.001
0.011
0.157
0.539
0.890

0.016
0.000
0.083
0.999
0.820
0.005
0.034




0.004
0.000
0.014
0.000
0.134
0.994
0.955




If assigned to the highest fuzzy score:
Cluster A: 0.10, 0.15, 0.23
Cluster B: 0.50, 0.60
Cluster C: 0.84, 0.95
The solution is similar to k-means.

14. Consider the following set of one-dimensional data points: {0.1, 0.25, 0.45, 0.55, 0.8, 0.9}. All the points are located in the range between [0,1].

(a) Suppose we apply kmeans clustering to obtain three clusters, A, B, and C. If the initial centroids are located at {0, 0.4, 1}, respectively, show the cluster assignments and locations of the updated centroids
after the first three iterations by filling out the following table.




Iter
Cluster assignment of data points
(A, B, or C)
Centroid Locations

0.10
0.25
0.45
0.55
0.80
0.90
A
B
C
0
-
-
-
-
-
-
0.00
0.40
1.00
1









2









3










Calculate the overall sum-of-squared errors of the clustering after the third iteration, where:
SSE = ? (xi — µ1)2 + ? (xi — µ2)2 + ? (xi — µ3)2

and µjs are the centroids of the 3 clusters, A, B, and C.
Answer:


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.25
0.45
0.55
0.80
0.90
A
B
C
0
-
-
-
-
-
-
0.00
0.40
1.00
1
A
B
B
B
C
C
0.10
0.42
0.85
2
A
A
B
B
C
C
0.18
0.5
0.85
3
A
A
B
B
C
C
0.18
0.5
0.85

SSE = 0.0213
(b) For the dataset given in part (a), is it possible to obtain an empty cluster after the first cluster assignment? If possible, what are the locations of the initial centroids to produce the empty cluster? If not, state why.
Answer: Yes. If two of the centroids are randomly initialized to the left of the leftmost points or to the right of the rightmost points. For example, if the initial centroids are (0, 0.05, 0.8), then the first cluster is empty.
(c) Show the clustering results obtained using bisecting kmeans (with k=3). Start by partitioning the data into 2 clusters, with the initial centroids located at 0.10 and 0.90. Then take the cluster with larger SSE and partition it into 2 smaller clusters (you may set the initial centroids to be the two points that are furthest away from each other in the original cluster). Calculate the SSE of the final 3 clusters and compare the results against the k-means solution.
Answer: After the first binary partition:



Iter
Cluster assignment of data points
Centroid Locations

0.10
0.25
0.45
0.55
0.80
0.90
A
B
0
-
-
-
-
-
-
0.10
0.90
1
A
A
A
B
B
B
0.27
0.75
2
A
A
A
B
B
B
0.27
0.75
3
A
A
A
B
B
B
0.27
0.75

The resulting sum-of-squared errors for the clusters are
SSE (cluster A) = 0.0617,	SSE (cluster B) = 0.0650
Next, we partition cluster B into 2, denoted as B1 and B2, with initial centroids at 0.55 and 0.90.


Iter
Cluster assignment of data points
Centroid Locations

0.10
0.25
0.45
0.55
0.80
0.90
A
B1
B2
0
-
-
-
-
-
-
-
0.55
0.90
1
A
A
A
B1
B2
B2
-
0.55
0.85
2
A
A
A
B1
B2
B2
-
0.55
0.85
3
A
A
A
B1
B2
B2
-
0.55
0.85

The SSE of the bisecting kmeans is 0.0667, which is larger than the SSE for k-means. Therefore, K-means is more effective on this data set. Intuitively, you can see that bisecting K-means always partition the middle cluster into 2 after the first step. So, the middle cluster (which contains the points 0.45 and 0.55) will always be in separate clusters, which is not the optimal solution.

15. K-means is a prototype-based clustering algorithm, in which the choice of the cluster prototype (i.e., representative point) depends on the cluster-
ing objective function. For example, consider a data set ? = {x1, x2, · · · , xN }, where each xi is a d-dimensional feature vector. The standard k-means
algorithm attempts to minimize the following sum-of-square loss func- tion:
N	k
min ? ? Wij xi — µj 2,	(7.7)
{µ},W i=1 j=1
where k is the number of clusters, µj is the prototype vector of cluster j, and W is an N ? k cluster membership matrix (Wij = 1 if data point xi belongs to cluster j or 0 otherwise). It can be shown that the prototype


vector for the squared loss function is given by the cluster centroid
?N  Wikxi
The denominator term ?N  Wik corresponds to the number of data
points assigned to cluster k. Derive the formula for computing the cluster prototype µ for each scenario described below. Assume Wij is known when computing the prototype. Show your steps clearly.
(a) The spherical k-means algorithm is designed to minimize the fol- lowing (1 - cosine similarity) loss function:

N	k
min ? ? Wij(1 — xT µj ),	(7.9)
{µ},W i=1 j=1

where each data point xi is assumed to have been normalized to unit length, i.e., xT xi = 1. The cluster prototypes are also constrained to have unit length, i.e., µT µi = 1. Hint: solve the constrained op- timization problem using the Lagrange multiplier method assuming W is fixed.
Answer: The Lagrangian formulation for the constrained opti- mization problem can be written as follows

N	k	k
L = ? ? Wij(1 — xT µj ) + ? ?j(µT µj — 1)


Taking its partial derivative with respect to µm and setting it to zero yields


 ∂L
∂µm

N
=	—	Wimxi
i=1


+ 2?mµm = 0

?N  Wimxi
=	µ	=   i=1	
2?m
N	N

=? µT µm

= 1 =  1 	W
4?2	im
i=1 j=1

Wjm

xT xj


=? ?m =

1 ,u,?N ?


WimWjmxT xj

i=1 j=1
?N


Wimxi

=? µm = q?N  ?N	T

(b) The standard k-means clustering algorithm assumes each point is equally important in determining the clusters. In practice, some data points may be outliers or noise. One way to address this would be to assign a weight to each point, giving lower weights for points that are considered outliers or noise (assume the outliers and noise points were already pre-determined during preprocessing) and
higher weights to other legitimate points. Let 0 ≤ vi ≤ 1 be the weight for a data point xi. The loss function for the “weighted
k-means” algorithm is given as follows:

N	k

min

? ? viWij xi — µj 2	(7.10)


Answer: Taking the partial derivative of the objective function with respect to µk and setting it to zero yields


∂L
∂µk



=
i=1


viWik

— 2xi + 2µk  = 0

=? µk =

N
i=1 N

viWikxi viWik

i=1


which is similar to the original k-means approach except the cen- troids are computed based on a weighted average of the data points associated with the cluster.

7.2 Cluster Validation
1. Consider the confusion matrices for two clustering solutions as shown below, where the rows correspond to the clusters and the columns corre- spond to the ground truth classes. Note that solution 2 simply partitions the first cluster of solution 1 into two smaller sub-clusters.



Each entry nij in the matrix corresponds to the number of data points assigned to cluster i that belong to class j. Furthermore, let ni+ =

number of data points that belong to class j, and N =	ij nij (i.e.,
the sum of all entries in the table) be the total number of data points.
In this exercise, you will compare the performance of the two clustering solutions using the following measures:

• Entropy, e = ?

ni+ ei, where ei = — ?

nij log nij

is the entropy

• Purity, p = ? ni+ pi, where pi = maxj nij


is the purity of cluster



• Normalized mutual information
2 ?


nij log  nij N 



where H1 = — ? ni+ log ni+ , and H2 = — ?


n+j log n+j .

Answer the following questions:


(a) Compute the values of entropy, purity, and NMI when the clusters are pure (i.e., contains only data points from one class). Assume number of clusters is the same as number of classes (i.e., k = 2).
Answer:

Entropy = 0
Purity = 1
NMI = 1
(b) Compute the entropy for both solutions. Which solution is better?
Answer:

esolution1 = 0.8755
                     esolution2 = 0.8652 Hence, solution 2 is better.
(c) Compute the purity for both solutions. Which solution is better?
Answer:

psolution1 = 0.7
                       psolution2 = 0.7 Both solutions are equivalent good.
(d) Compute the NMI for both solutions. Which solution is better?
Answer:

NMIsolution1 = 0.1263
NMIsolution2 = 0.1142
Solution 1 is better
(e) Based on your answers above, state which supervised measure do you think is better and why?
Answer:
Solution 2 has an extra cluster but the clusters are not much better than those in solution 1. This is because entropy tends to be biased towards larger number of clusters. Thus, NMI is the better measure as it is not biased by the number of clusters.


2. Consider a data set ? = {x1, x2, · · · , xN } that contains N points, where each data point xi is a d-dimensional vector of continuous-valued at-
tributes. Let µ0 denote the global centroid of the data, i.e.,
?N  xi


The total sum-of-squared error assuming there is only 1 cluster is given by

N
TSS =	
i=1

N
2	T
2
i=1

where z 2 = zT z = ?d	z2.
(a) Suppose the data is partitioned into 2 clusters, C1 and C2, using k-means. Let µ1 and µ2 be the centroids of the two clusters found. Furthermore, let n1 and n2 be the number of points in clusters 1 and 2, respectively, where n1 + n2 = N . The relationship between µ0, µ1, and µ2 is given by
µ0 = w1µ1 + w2µ2

Derive the mathematical expressions for w1 and w2 in terms of n1, n2, and N .
Answer:
?N  xi
=	?xi?C1 xi + ?xi?C2 xi

=	n1µ1 + n2µ2
N


since µj

=	xi?Cj xi
nj

=	n1 µ N	1

+ n2 µ
N	2


(b) The sum-of-squared errors for k-means algorithm with 2 clusters,
C1 and C2, is given by


SSE =	
xi?C1

2
2
xi?C2

xi — µ2 2


Show that TSS ≥ SSE, i.e., sum-of-squared errors for 2 clusters is always smaller than or equal to the sum-of-squared errors with only
1 cluster.
Answer:


SSE  =	
xi?C1

2
2
xi?C2

xi — µ2 2

=	
xi?C1

2
2
xi?C2

xi — µ0 + µ0 — µ2 2

=	?  xi — µ0 2 + µ0 — µ1 2 + 2(xi — µ0)T (µ0 — µ1) 
+	?  xi — µ0 2 + µ0 — µ2 2 + 2(xi — µ0)T (µ0 — µ2) 
2	2
xi?C2
N
= ? xi — µ0 2 + n1 µ0 — µ1 2 + n2 µ0 — µ2 2
+ 2 ? (xi — µ0)T (µ0 — µ1) + 2 ? (xi — µ0)T (µ0 — µ2)



Since


we have

(xi — µ0) = nj(µj — µ0) = —nj(µ0 — µj ),
xi?Cj


SSE = ? xi — µ0 2 + n1 µ0 — µ1 2 + n2 µ0 — µ2 2

2
i=1
— 2n1(µ0 — µ1)

2

(µ0 — µ1) — 2n2(µ0 — µ2)

2

(µ0 — µ2)

= TSS — n1 µ0 — µ1 2 — n2 µ0 — µ2 2
2	2

Since  µ0 — µj 2 ≥ 0, therefore SSE ≤ TSS.
3. Consider a data set ? = {x1, x2, · · · , xN } that contains N points, where each data point xi is a p-dimensional vector of continuous-valued at-
tributes. Suppose the N data points are grouped into two clusters, C1 and C2 using k-means clustering. Show that the SSE is non-increasing when the data is split (from 1 cluster containing all N points) into 2 clusters.


Answer: For this proof, we use the following property of a vector:
v 2 = vT v ≥ 0. The SSE for 1 cluster is given by



SSE1	=

?i=1

xi — c 2 =

?i=1

xT xi — 2cT xi + cT c


,	(7.11)



where

1  N
c =	xi or
N
i=1


N
xi
i=1


= N c.

Thus, Equation (7.11) can be simplified as

N	N

SSE1	=

? xT xi — 2N cT c + N cT c = ? xT xi — N cT c (7.12)


Suppose the data points are split into 2 clusters, C1 and C2. The total SSE for the 2 clusters is given by


SSE2	=
xi?C1

xi — c1 2 +
xi?C2

xi — c2 2

=	? xT xi — N1cT c1 + ?

xT xi — N2cT c2

i
xi?C1
N

1	i	2
xi?C2

= ? xT xi — N1cT c1 — N2cT c2	(7.13)

Let

∆ = SSE1 — SSE2 = N1cT c1 + N2cT c2 — N cT c.

Next, we can express c in terms of N1, N2, c1 and c2. Note that


1
c  =
N
i=1


1
i	N
xi?C1


xi +


1
xi = N
xi?C2


N1c1 + N2c2



Thus,

∆  =  N1cT c1 + N2cT c2 —


N
N1c1 + N2c2

 T 



N1c1 + N2c2

=	 1  NN cT c + NN cT c  —  1  N 2cT c

+ N 2cT c + 2N N cT c 

=	 1 N (N — N )cT c + N (N — N )cT c
— 
2N N cT c 

=	 1  N N cT c + N N cT c
— 
2N N cT c 

=	N1N2 N

c1 — c2

≥ 0
since	· 2 is always non-negative. Thus SSE1 ≥ SSE2, which completes the proof.


8




Alternative Clustering

8.1 Type of Clustering
1. State the type of clustering for each method given below. You need to indicate whether it is: (a) partitional (non-hierarchical) or hierarchical,
(b) exclusive (disjoint), overlapping, or fuzzy, (c) partial or complete.
Example: K-means clustering: partitional, exclusive, complete.

(a) Spectral clustering
Answer: partitional, exclusive, complete.
(b) Single link
Answer: hierarchical, exclusive (for each level of the hierarchy), complete.
(c) Group average method
Answer: hierarchical, exclusive (for each level of the hierarchy), complete.
(d) DBScan.
Answer: partitional, exclusive, partial.


8.2 HierarchicalClustering
1. Use the distance matrix shown in the table below to perform single and complete link hierarchical clustering. Show your results by drawing a dendrogram. The dendrogram should clearly show the order in which the points are merged and the y-axis show the distance between pairs of clusters being merged at each iteration.



p1
p2
p3
p4
p5
p1
0
0.3830
0.1474
0.3466
0.1616
p2
0.3830
0
0.4592
0.1252
0.2227
p3
0.1474
0.4592
0
0.3840
0.2643
p4
0.3466
0.1252
0.3840
0
0.2102
p5
0.1616
0.2227
0.2643
0.2102
0
Answer:

Single link

0.21

0.2

0.19

0.18

0.17

0.16

0.15

0.14

0.13

2	4	1	3	5

Figure 8.1. Single link.


2. Consider the five objects shown in Figure 8.3.


Complete link

0.45

0.4

0.35

0.3

0.25

0.2

0.15

2	4	5	1	3

Figure 8.2. Complete link.


2	1.01
1	1
3	1
4	2	2.01
6
2	2
6	2

Figure 8.3. Data set for question 2.

(a) Suppose we apply the single link (MIN) algorithm to cluster the ob- jects. Draw the dendrogram for the clusters assuming the similarity measure is Euclidean distance.
Answer: Euclidean distance matrix


A
B
C
D
E
A
0
3.74
3.16
2.23
1.41
B
3.74
0
2.00
5.91
4.47
C
3.16
2.00
0
5.19
4.00
D
2.23
5.91
5.19
0
1.73
E
1.41
4.46
4.00
1.73
0


The dendrogram is shown in Figure 8.4.


MIN (Single Link)

3.2


3


2.8


2.6


2.4


2.2


2


1.8


1.6


1.4

1	5	4	2	3


Figure 8.4. Dendrogram using single-link with Euclidean distance.


(b) Repeat the question in part (a) assuming that the similarity mea- sure is correlation.
Answer: Correlation matrix


A
B
C
D
E
A
1
1
0.87
0
0
B
1
1
0.87
0
0
C
0.87
0.87
1
-0.50
-0.50
D
0
0
-0.57
1
1
E
0
0
-0.57
1
1
The dendrogram is shown in Figure 8.5.
(c) Suppose we apply the complete link (MAX) algorithm to cluster the objects. Draw the dendrogram for the clusters assuming the similarity measure is Euclidean distance.
Answer:
The dendrogram is shown in Figure 8.6.
(d) Repeat the question in part (c) assuming that the similarity mea- sure is correlation.
Answer:



MIN (Single Link)

0


0.1


0.2


0.3


0.4


0.5


0.6


0.7


0.8


0.9


1
4	5	1	2	3
Figure 8.5. Dendrogram using single-link with correlation.



MAX (Complete Link)

6


5.5


5


4.5


4


3.5


3


2.5


2


1.5


1	5	4	2	3


Figure 8.6. Dendrogram using complete-link with Euclidean distance.

The dendrogram is shown in Figure 8.7.

3. Consider the following two-dimensional data points:



MAX (Complete Link)

?0.5





0





0.5





1
4	5	1	2	3
Figure 8.7. Dendrogram using complete-link with correlation.


Data point
x1
x2
1
0.1
0.2
2
0.2
0.1
3
0.4
0.8
4
0.5
1.0
5
0.7
0.35


The Euclidean distance between the data points is summarized below:
,	?
?.	.?


(a) Apply the single link (MIN) algorithm to cluster the objects. Draw the dendrogram for the clusters assuming the distance measure is Euclidean. Make sure you label the y-axis of the dendrogram care- fully.
Answer: See Figure 8.8.
(b) Suppose we apply the complete link (MAX) algorithm to cluster the objects. Draw the dendrogram for the clusters assuming the



0.55


0.5


0.45


0.4


0.35


0.3


0.25


0.2


0.15

1	2	3	4	5

Figure 8.8. Dendrogram for single link (MIN).

distance measure is Euclidean. Make sure you label the y-axis of the dendrogram carefully.
Answer: See Figure 8.9.



0.9


0.8


0.7


0.6


0.5


0.4


0.3


0.2


1	2	5	3	4

Figure 8.9. Dendrogram for complete link (MAX).


(c) Apply k-means on the data set with k = 2. Compare the cluster- ing result against MIN and MAX (with k = 2). Which methods produce similar clustering results?
Answer: K-means will put data points 1, 2, and 5 in one cluster, and 3 and 4 in another cluster. The results for MAX and k-means are more similar.
(d) Repeat part (c) using 1 - cosine as distance measure (this approach is also known as spherical k-means). Compare the results against k-means with Euclidean distance, MIN, and MAX algorithms. Ex- plain why the results for spherical k-means are different.
Answer: K-means will put data points 1, 3, and 4 in one cluster, and 2 and 5 in another cluster. These results are different than k- means with Euclidean distance, MIN, and MAX algorithms because the it considers the similarity of angles between the data points instead of their Euclidean distance.
(e) Which method (k-means or spherical k-means) do you think is more appropriate for clustering document data? Why?
Answer: Spherical k-means is more appropriate because Euclidean distance (used in regular k-means) is more sensitive to the document length and inappropriate for asymmetric binary data.

4. Use the distance matrix shown in the table below to perform single and complete link hierarchical clustering. Show your results by drawing a dendrogram. The dendrogram should clearly show the order in which the points are merged and the y-axis show the distance between pairs of clusters being merged at each iteration.



p1
p2
p3
p4
p5
p1
0
0.5840
0.1955
0.3815
0.1127
p2
0.5840
0
0.6132
0.4956
0.5733
p3
0.1955
0.6132
0
0.2390
0.3067
p4
0.3815
0.4956
0.2390
0
0.4694
p5
0.1127
0.5733
0.3067
0.4694
0


Answer:
Dendrograms are shown in Figure 8.10 and 8.11.



0.5
0.4956

0.45

0.4

0.35

0.3

0.25
0.2390
0.2
0.1955

0.15
0.1127
0.1
1	5	3	4	2



Figure 8.10. Dendrogram for Single Link

5. Consider the following 2-dimensional data set:

Substance
Mass
Volume
A
1
1
B
10
10
C
2
1
D
8
4
(a) Identify the two clusters obtained by using regular k-means algo- rithm.
Answer: {A,C} and {B,D}
(b) Identify the two clusters obtained by using single-link (MIN) hier-
archical clustering.
Answer: {A,C} and {B,D}
(c) Identify the two clusters obtained by using complete-link (MAX)
hierarchical clustering.
Answer: {A,C} and {B,D}
(d) How would you modify the k-means algorithm (without modifying
the data) so that we obtain two clusters based on the substance density, i.e., we want A and B in one cluster and C and D in another.




0.6132


0.6


0.55



0.4694


0.5

0.45


0.4

0.35

0.3



0.2390


0.25

0.2



0.1127


0.15

0.1





1	5	3	4	2




Figure 8.11. Dendrogram for Complete Link

Answer: Use 1 - cosine similarity as distance measure for k-means.

6. Consider the data set shown in Figure 8.12 along with its corresponding distance matrix.



Distance Matrix


P1
P2
P3
P4
P5
P1
0
0.445
0.577
0.525
0.781
P2
0.445
0
0.812
0.252
0.468
P3
0.577
0.812
0
0.678
0.819
P4
0.525
0.252
0.678
0
0.259
P5
0.781
0.468
0.819
0.259
0.000





Figure 8.12. Data set for hierarchical clustering.


(a) Which of the dendrograms shown in Figure 8.13 corresponds to the clustering solution for single link (MIN) and which corresponds to complete link (MAX)?



Dendrogram 1	Dendrogram 2

Figure 8.13. Dendrograms.

Answer: Dendrogram 1 is for single link (MIN) and dendrogram 2 is for complete link (MAX).
(b) Show the cophenetic distance matrix for single link using the distance matrix given in Figure 8.12.
Answer:

Point
P1
P2
P3
P4
P5
P1
0
0.445
0.577
0.445
0.445
P2
0.445
0
0.577
0.252
0.259
P3
0.577
0.577
0
0.577
0.577
P4
0.445
0.252
0.577
0
0.259
P5
0.445
0.259
0.577
0.259
0
(c) Show the cophenetic distance matrix for complete link using the distance matrix given in Figure 8.12.
Answer:


Point
P1
P2
P3
P4
P5
P1
0
0.812
0.577
0.812
0.812
P2
0.812
0
0.812
0.252
0.468
P3
0.577
0.812
0
0.812
0.812
P4
0.812
0.252
0.812
0
0.468
P5
0.812
0.468
0.812
0.468
0

(d) Compute the cophenetic correlation coefficient for the single link and complete link algorithms. Which method is better according to this measure?
Answer: Single link: 0.8191; Complete link: 0.7774

7. Consider the following set of one-dimensional data points:

0.6, 1.2, 1.8, 2.4, 3.0, 4.2, 4.8

(a) Suppose we apply kmeans clustering to obtain two clusters. If the initial centroids are located at 1.8 and 4.5, show the cluster assign- ments and locations of the centroids after the algorithm converges. Compute the total sum-of-squared errors of the clusters.
Answer: First cluster is 0.6, 1.2, 1.8, 2.4, 3.0.
Error = 3.6
Second cluster is 4.2, 4.8.
Error = 0.18
Total Error = 3.78
(b) Repeat the previous question using 1.5 and 4.0 as the initial cen- troids. Show the cluster assignment and locations of centroids after the algorithm converges. Compute the total sum-of-squared errors of the clusters.
Answer: First cluster is 0.6, 1.2, 1.8, 2.4 .
Error = 1.8
Second cluster is 3.0, 4.2, 4.8.
Error = 1.68
Total Error = 3.48

(c) What are the two clusters produced by single link?
Answer: The two clusters are {0.6, 1.2, 1.8, 2.4, 3.0} and {4.2,
4.8}.


(d) Which technique, K-means or single link, seems to produce the most natural clustering in this situation? (For K-means, choose the clustering result with the lowest squared error).
Answer: MIN (single link) produces the most natural clustering. Although k-means can produce a similar solution, it was not the clustering with lowest squared error.

8. Consider the following two-dimensional data points:


Data point
x1
x2
1
0.1
0.2
2
0.2
0.1
3
0.4
0.8
4
0.5
1.0
5
0.7
0.35

(a) Compute the Euclidean distance between every pair of points. Show your results in a 5 ? 5 distance matrix.
Answer:


0
0.1414
0.6708
0.8944
0.6185
0.1414
0
0.7280
0.9487
0.5590
0.6708
0.7280
0
0.2236
0.5408
0.8944
0.9487
0.2236
0
0.6801
0.6185
0.5590
0.5408
0.6801
0


(b) Apply the single link (MIN) algorithm to cluster the objects. Draw the dendrogram for the clusters assuming the distance measure is Euclidean. Make sure you label the y-axis of the dendrogram care- fully.
Answer: See Figure 8.14(a).
(c) Suppose we apply the complete link (MAX) algorithm to cluster the objects. Draw the dendrogram for the clusters assuming the distance measure is Euclidean. Make sure you label the y-axis of the dendrogram carefully.
Answer: See Figure ??(b).




0.55



0.9


0.5
0.8

0.45
0.7

0.4
0.6

0.35
0.5


0.3


0.25


0.2


0.15



0.4


0.3


0.2


1	2	3	4	5

(a) Single-link clustering


1	2	5	3	4

(b) Complete-link clustering


Figure 8.14. Hierarchical clustering results.

(d) Apply k-means on the data set with k = 2. Compare the cluster- ing result against MIN and MAX (with k = 2). Which methods produce similar clustering results?
Answer: The k-means clustering result is shown in Figure 8.15. The two clusters found are {1,2,5} and {3,4}. This is similar to the results of MAX (complete clustering) with k = 2.



1


0.9


0.8


0.7


0.6


0.5


0.4


0.3


0.2


0.1


0
0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1

Figure 8.15. K-means clustering.


9. Use the distance matrix shown in the table below to perform single and complete link hierarchical clustering. Show your results by drawing a dendrogram. The dendrogram should clearly show the order in which the points are merged and the y-axis show the distance between pairs of clusters being merged at each iteration.



p1
p2
p3
p4
p5
p1
0
0.5840
0.1955
0.3815
0.1127
p2
0.5840
0
0.6132
0.4956
0.5733
p3
0.1955
0.6132
0
0.2390
0.3067
p4
0.3815
0.4956
0.2390
0
0.4694
p5
0.1127
0.5733
0.3067
0.4694
0


Answer: The dendrograms for single link and complete link are shown in Figures ?? and ??, respectively.



Figure 8.16. Dendrogram for Single Link


10. [4 points] Consider the following Euclidean distance matrix for 5 data points:




Figure 8.17. Dendrogram for Complete Link



p1
p2
p3
p4
p5
p1
0
0.4167
0.4974
0.4879
0.3847
p2
0.4167
0
0.7396
0.3413
0.4261
p3
0.4974
0.7396
0
0.5241
0.3439
p4
0.4879
0.3413
0.5241
0
0.1865
p5
0.3847
0.4261
0.3439
0.1865
0


(a) Apply the single link (MIN) algorithm to cluster the objects. Draw the dendrogram for the clusters. Make sure you label the y-axis of the dendrogram carefully.
Answer:
(b) Apply the complete link (MAX) algorithm to cluster the objects. Draw the dendrogram for the clusters. Make sure you label the y-axis of the dendrogram carefully.
Answer:

11. Consider a dataset that has 5 data points, {p1, p2, p3, p4, p5}. Suppose we are interested in applying agglomerative hierarchical clustering to
the dataset. The table below shows the distance between every pair of points:




Figure 8.18. Single link results.



Figure 8.19. Complete link results.



p1
p2
p3
p4
p5
p1
0
0.8147
0.9058
0.1270
0.9134
p2
0.8147
0
0.6324
0.0975
0.2785
p3
0.9058
0.6324
0
0.5469
0.9575
p4
0.1270
0.0972523
0.5469
0
0.9649
p5
0.9134
0.2785
0.9575
0.9649
0


(a) Draw the dendrogram obtained when applying the single link (MIN) clustering method to the dataset. The dendrogram must clearly show the order in which the points are merged. The y-axis of the dendrogram must indicate the distance at which a pair of clusters were merged at each iteration.
Answer: See Figure 8.20.



Figure 8.20. Dendrogram for single link


(b) Repeat part (a) by drawing the dendrogram obtained when apply- ing the complete link (MAX) method.
Answer: See Figure 8.21.




Figure 8.21. Dendrogram for complete link

8.3 Density-based Clustering
1. Consider the data set shown in Figure 8.22. Suppose we apply DBScan algorithm with Eps = 0.15 (in Euclidean distance) and MinPts = 3.

1


0.9


0.8


0.7


0.6


0.5


0.4


0.3


0.2


0.1

0
0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1

Figure 8.22. DBScan clustering.


(a) List all the core points in the diagram (you can use the labels of the data points in the diagram).
Answer:
MinPts > 3 (including central point): a-p, s, u.
(b) List all the border points in the diagram.
Answer:
MinPts > 3 (including central point): q, r, t, v, x
(c) List all the noise points in the diagram.
Answer:
MinPts > 3 (including central point): w, y, z
(d) Based on the DBScan algorithm described in the book, how many clusters are obtained from the data set?
Answer: 2

2. Consider the data set shown in Figure 8.23. Suppose we apply DBScan algorithm with Eps = 0.15 (in Euclidean distance) and MinPts = 3.



1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1

Figure 8.23. DBScan clustering.


(a) List all the core points in the diagram (you can use the labels of the data points in the diagram).


Answer: A point is a core point if the number of points in the Eps neighborhood is more than MinPts (3). So the list of core points are A - P, T, X
(b) List all the border points in the diagram.
Answer: The list of border points are Q, S, U, W, Y, Z
(c) List all the noise points in the diagram.
Answer: The list of noise points are R and V.
(d) Based on the DBScan algorithm, how many clusters are obtained from the data set?
Answer: 3 clusters

3. Consider the data set shown in Figure 8.24. Suppose we apply DBScan algorithm with Eps = 0.15 (in Euclidean distance) and MinPts = 3.


1


0.9


0.8


0.7


0.6


0.5


0.4


0.3


0.2


0.1


0
0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1

Figure 8.24. DBScan clustering.



(a) List all the core points in the diagram (you can use the labels of the data points in the diagram). Note: a point is considered a core point if there are more than MinPts number of points (including the point itself) within a neighborhood of radius Eps.
Answer:
a, b, c, d, e, f, g, h, i, j, k, l, q, r, s, t, x.


(b) List all the border points in the diagram.
Answer:
m, p, u, v, v, w, y, z.
(c) List all the noise points in the diagram.
Answer:
n, o.
(d) Using the DBScan algorithm, how many clusters will be obtained from the data set?
Answer:
3 clusters.

4. Consider the data set shown in Figure ??. Suppose we apply DBScan algorithm with Eps = 0.15 (in Euclidean distance) and MinPts = 3.



Figure 8.25. DBScan clustering.


(a) List all the core points in the diagram (use the labels a-y from the diagram to indicate which data points are core points). Note: a


point is considered a core point if there are more than MinPts number of points (including the point itself) within a neighborhood of radius Eps. Thus, if a data point is in a neighborhood that contains 4 or more points (including itself), it is classified as a core point.
Answer: a-i, l-o, q-x
(b) List all the border points in the diagram.
Answer: j, k, p, y
(c) List all the noise points in the diagram.
Answer: z
(d) Using the DBScan algorithm, what are the clusters obtained from the data set?
Answer: {a - j}, {k - p}, {q - y}.
It is also possible for node p to be assigned to the cluster {q- y}.
5. Consider the data set shown in Figure 8.26. Suppose we apply DBScan algorithm with Eps = 0.15 (in Euclidean distance) and MinPts = 3.

(a) List all the core points in the diagram (you can use the labels of the data points in the diagram). Note: a point is considered a core point if there are more than MinPts number of points (including the point itself) within a neighborhood of radius Eps.
Answer: a-k, m, o-x
(b) List all the border points in the diagram.
Answer: l, n, y
(c) List all the noise points in the diagram.
Answer: z
(d) Using the DBScan algorithm, how many clusters will be obtained from the data set?
Answer: 2

6. Consider the data set shown in Figure 8.27. Suppose we apply DBScan algorithm with Eps = 0.15 (in Euclidean distance) and MinPts = 4.

(a) List all the core points in the diagram (use the labels a-y from the diagram to indicate which data points are core points). Note: a point is considered a core point if there are more than MinPts




Figure 8.26. Data set for DBScan clustering.

number of points (including the point itself) within a neighborhood of radius Eps. Thus, if a data point is in a neighborhood that contains 5 or more points (including itself), it is classified as a core point.
Answer: b, d, e, f, g, h, i, k, l, s, v.
(b) List all the border points in the diagram.
Answer: a, c, j, m, r, u, t, w.
(c) List all the noise points in the diagram.
Answer: n, o, p, q, x, y, z.
(d) Using the DBScan algorithm, what are the clusters obtained from the data set?
There are 2 clusters found: a - m and r - w.





Figure 8.27. DBScan clustering.

8.4 Spectral Clustering
1. Consider the graph data shown in Figure 8.28. Assume the weights for all the links are equal to 1.



Figure 8.28. Graph data


(a) Compute the Laplacian matrix for the graph. Use the node indices shown in Figure 8.28 to order the rows and columns of the matrix. Answer:


, 2	—1 —1	0	0	0	0  ?
—1	2	—1	0	0	0	0
—1 —1	3	—1	0	0	0
.	.
0	0	—1	2	0	—1	0
.	.
0	0	0	0	2	—1 —1
0	0	0	—1 —1	3	—1
0	0	0	0	—1 —1	2
(b) Compute the first three smallest eigenvalues of the graph Laplacian matrix.
Answer: The three smallest eigenvalues are 0, 0.2679 and 1.5858.
(c) Compute the eigenvectors that correspond to the three smallest eigenvalues given in part (b).
Answer: The eigenvectors are as follows:
, 0.3780 —0.4440 —0.2808 ?

0.3780 —0.3251	0.1645
. 0.3780  —0.0000	0.7941 .
0.3780	0.4440	—0.2808
0.3780	0.3251	0.1645
0.3780	0.4440	—0.2808
(d) Apply k-means on the eigenvector matrix to generate 3 clusters. List the three clusters found.
Answer: V1 = {1, 2, 3}, V2 = {4}, V3 = {5, 6, 7}.
(e) Calculate the normalized cut obtained for the 3 clusters found. Let V denote the set of all the nodes in a graph and W = [wij] denote its adjacency matrix. Suppose V is partitioned into 3 disjoint subsets,
V1, V2, and V3, where V1 ? V2 ? V3 = V . The normalized cut for the partitions can be computed as follows:


Ncut(V , V , V ) = ? Cut(Vi, V — Vi)



(8.1)



where



d(Vi) =
k?Vi,j?V
Cut(A, B)  =
i?A,j?B



wij,

wij	(8.2)


Answer: For the solution given in the previous question, Cut(V1, V — V1) = Cut(V3, V — V3) = 1, and Cut(V2, V — V2) = 2. Furthermore, the degree of the nodes can be found from the diagonal elements
of the Laplacian matrix. All the nodes have degree 2 except for data points 3 and 6. Thus, d(V1) = 2 + 2 + 3 = 7, d(V2) = 2, and d(V3) = 3 + 2 + 2 = 7. Hence, the normalized cut for the clusters is

1	2	1	9
Ncut(V1, V2, V3) = 7 + 2 + 7 = 7 .
(f) Suppose the 3 clusters found are as follows:

(1, 2),	(3, 4, 6),	(5, 7)
Compute the normalized cut of the clusters. Is the normalized cut smaller, larger, or equal to the solution found in part (d)?
Answer: For this solution, Cut(V1, V — V1) = Cut(V3, V — V3)
= 2, and Cut(V2, V — V2) = 4. Furthermore, d(V1) = 2 + 2 = 4,
d(V2) = 3 +2 +3 = 8, and d(V3) = 2 +2 = 4. Hence, the normalized
cut for the clusters is

2	4	2	6
Ncut(V1, V2, V3) = 4 + 8 + 4 = 4 .
The normalized cut for this solution is larger than the previous solution.

2. Consider the graph data shown in Figure 8.29. Assume the weights for all the links are equal to 1.

(a) Compute the Laplacian matrix for the graph. Use the node indices shown in Figure 8.29 to order the rows and columns of the matrix. Answer:





Figure 8.29. Graph data


3	—1 —1 —1	0	0	0
—1	2	—1	0	0	0	0
—1 —1	3	—1	0	0	0
L =	—1	0	—1	3	—1	0	0
0	0	0	—1	3	—1 —1
0	0	0	0	—1	2	—1
0	0	0	0	—1 —1	2








(8.3)

(b) Compute the first three smallest eigenvalues of the graph Laplacian matrix.
Answer: 0, 0.3588, 2.2763
(c) Compute the eigenvectors that correspond to the three smallest eigenvalues given in part (b).
Answer:

0.3780 —0.3482 —0.0900
?0.3780  —0.4244	0.6515 ?
0.3780	0.3078	—0.2735
0.3780	0.4801	0.2143
0.3780	0.4801	0.2143
(d) Apply k-means on the eigenvector matrix to generate 3 clusters. List the three clusters found. You may need to run the k-means algorithm multiple times and choose the solution with lowest SSE. For example, you can use Matlab’s k-means implementation to do this:
matlab> clusters = kmeans( eigv, numClusters, ’Replicates’, 200);


where eigv corresponds to the matrix of eigenvectors found in part (c), numClusters is the number of desired clusters, and Replicates is the number of times you repeat k-means with different initialization. Answer: {1,3,4}, {2}, {5,6,7}
(e) Calculate the normalized cut obtained for the 3 clusters found. Let
V denote the set of all the nodes in a graph and W = [wij] denote its adjacency matrix. Suppose V is partitioned into 3 disjoint subsets, V1, V2, and V3, where V1 ? V2 ? V3 = V . The normalized cut for the
partitions can be computed as follows:


Ncut(V , V , V ) = ? Cut(Vi, V — Vi)




(8.4)

where






Answer:



d(Vi) =
k?Vi,j?V
Cut(A, B)  =
i?A,j?B



wij,

wij	(8.5)




Ncut =

2	3	1
+	+
2	9	7


= 1.4762.

3. Consider the graph data shown in Figure 8.30. Assume the weights for all the links are equal to 1.


Figure 8.30. Graph data


(a) Compute the Laplacian matrix for the graph. Use the node indices shown in Figure 8.30 to order the rows and columns of the matrix.


Answer:
2	—1 —1	0	0	0	0
—1	3	—1 —1	0	0	0
—1 —1	3	—1	0	0	0
L =	0	—1 —1	3	—1	0	0
0	0	0	—1	3	—1 —1
0	0	0	0	—1	1	0
0	0	0	0	—1	0	1
(b) Compute the two smallest eigenvalues of the graph Laplacian ma- trix.
Answer: 0, 0.3588
(c) Create a 7 ? 2 matrix corresponding to the eigenvectors associated with the two smallest eigenvalues given in part (b).
Answer:
—0.3780	0.4244
—0.3780	0.3482
—0.3780	0.3482
?—0.3780	0.1471 ?
—0.3780 —0.4801
(d) Apply k-means on the eigenvector matrix to generate 2 clusters. List the two clusters found.
Answer: The two clusters found are {1,2,3,4} and {5,6,7}.
(e) Calculate the normalized cut obtained for the 2 clusters. Let V
denote the set of all the nodes in a graph and W = [wij] denote its adjacency matrix. Suppose V is partitioned into 2 disjoint subsets,
V1 and V2, where V1 ?V2 = V . The normalized cut for the partitions can be computed as follows:


Ncut(V , V ) = Cut(V1, V2) + Cut(V1, V2)


(8.6)


where

1	2	d(V1)

d(Vi) =
k?Vi,j?V

d(V2)


wkj,

Cut(V1, V2) =
i?V1,j?V2

wij	(8.7)



Answer:


Ncut =


1	1
+
11	5


= 0.2909.







































































































































































































